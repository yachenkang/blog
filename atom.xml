<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hamish的科研blog</title>
  <icon>https://www.gravatar.com/avatar/3a8d3023f7286597c0df6b02acb9fa24</icon>
  
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="https://yachenkang.github.io/blog/"/>
  <updated>2020-10-15T08:30:34.080Z</updated>
  <id>https://yachenkang.github.io/blog/</id>
  
  <author>
    <name>Kang Yachen</name>
    <email>kangyachen@westlake.edu.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NIPS2020 Reinforcement Learning Reading List</title>
    <link href="https://yachenkang.github.io/blog/2020/06/15/NIPS2020-Reinforcement-Learning-Reading-List/"/>
    <id>https://yachenkang.github.io/blog/2020/06/15/NIPS2020-Reinforcement-Learning-Reading-List/</id>
    <published>2020-06-15T02:20:40.000Z</published>
    <updated>2020-10-15T08:30:34.080Z</updated>
    
    <content type="html"><![CDATA[<p>NIPS2020论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。</p><a id="more"></a><p>*标注的为值得精读论文</p><ul><li><p><strong>Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement.</strong> [<a href="https://arxiv.org/abs/2002.11089" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Ben Eysenbach (Carnegie Mellon University) · XINYANG GENG (UC Berkeley) · Sergey Levine (UC Berkeley) · Russ Salakhutdinov (Carnegie Mellon University)</li><li>多任务强化学习（RL）旨在学习同时解决许多任务的策略。一些先前的工作发现，使用不同的奖励函数重新标记过去的经验可以提高样本效率。重新标记方法通常会问：事后看来，如果我们假设我们的经验对于某些任务是最佳的，那么对于哪个任务最佳？ 在本文中，我们证明事后重新标记是逆RL，这一发现表明我们可以将逆RL与RL算法串联使用，来有效解决许多任务。我们使用这个想法来泛化目标重新标记技术，从先前的工作到任意类别的任务。我们的实验证实，使用逆RL重新标记数据可加快通用多任务设置中的学习速度，其中包括达成目标，具有离散奖励集的域以及具有线性奖励函数的域。</li></ul></li><li><p><strong>Generalised Bayesian Filtering via Sequential Monte Carlo.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ayman Boustati (University of Warwick) · Omer Deniz Akyildiz (University of Warwick) · Theodoros Damoulas (University of Warwick &amp; The Alan Turing Institute) · Adam Johansen (University of Warwick)</li></ul></li><li><p><strong>Softmax Deep Double Deterministic Policy Gradients.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ling Pan (Tsinghua University) · Qingpeng Cai (Alibaba Group) · Longbo Huang (IIIS, Tsinghua Univeristy)</li></ul></li><li><p><strong>Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Gen Li (Tsinghua University) · Yuting Wei (Carnegie Mellon University) · Yuejie Chi (CMU) · Yuantao Gu (Tsinghua University) · Yuxin Chen (Princeton University)</li></ul></li><li><p><strong>Learning Multi-Agent Coordination for Enhancing Target Coverage in Directional Sensor Networks.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jing Xu (Peking University) · Fangwei Zhong (Peking University) · Yizhou Wang (Peking University)</li></ul></li><li><p><strong>Off-Policy Imitation Learning from Observations.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Zhuangdi Zhu (Michigan State University) · Kaixiang Lin (Michigan State University) · Bo Dai (Google Brain) · Jiayu Zhou (Michigan State University)</li></ul></li><li><p><strong>Can Q-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a SAT Solver?.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Vitaly Kurin (University of Oxford) · Saad Godil (NVIDIA) · Shimon Whiteson (University of Oxford) · Bryan Catanzaro (NVIDIA)</li></ul></li><li><p><strong>DISK: Learning local features with policy gradient.</strong> [<a href>论文链接</a>]</p><ul><li>作者: MichaÅ‚ Tyszkiewicz (EPFL) · Pascal Fua (EPFL, Switzerland) · Eduard Trulls (Google)</li></ul></li><li><p><strong>Learning Individually Inferred Communication for Multi-Agent Cooperation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ziluo Ding (Peking University) · Tiejun Huang (Peking University) · Zongqing Lu (Peking University)</li></ul></li><li><p><strong>Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jorge Mendez (University of Pennsylvania) · Boyu Wang (University of Western Ontario) · Eric Eaton (University of Pennsylvania)</li></ul></li><li><p><strong>Fixed-Support Wasserstein Barycenters: Computational Hardness and Fast Algorithm.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Tianyi Lin (UC Berkeley) · Nhat Ho (University of Texas at Austin) · Xi Chen (New York University) · Marco Cuturi (Google Brain  &amp;  CREST - ENSAE) · Michael Jordan (UC Berkeley)</li></ul></li><li><p><strong>Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yijie Guo (University of Michigan) · Jongwook Choi (University of Michigan) · Marcin Moczulski (Google Brain) · Shengyu Feng (University of Illinois Urbana Champaign) · Samy Bengio (Google Research, Brain Team) · Mohammad Norouzi (Google Brain) · Honglak Lee (Google / U. Michigan)</li></ul></li><li><p><strong>Almost Optimal Model-Free Reinforcement Learningvia Reference-Advantage Decomposition.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Zihan Zhang (Tsinghua University) · Yuan Zhou (UIUC) · Xiangyang Ji (Tsinghua University)</li></ul></li><li><p><strong>Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yujing Hu (NetEase Fuxi AI Lab) · Weixun Wang (Tianjin University) · Hangtian Jia (Netease Fuxi AI Lab) · Yixiang Wang (University of Science and Technology of China) · Yingfeng Chen (NetEase Fuxi AI Lab) · Jianye Hao (Tianjin University) · Feng Wu (University of Science and Technology of China) · Changjie Fan (NetEase Fuxi AI Lab)</li></ul></li><li><p><strong>Effective Diversity in Population Based Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jack Parker-Holder (University of Oxford) · Aldo Pacchiano (UC Berkeley) · Krzysztof M Choromanski (Google Brain Robotics) · Stephen J Roberts (University of Oxford)</li></ul></li><li><p><strong>A Boolean Task Algebra for Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Geraud Nangue Tasse (University of the Witwatersrand) · Steven James (University of the Witwatersrand) · Benjamin Rosman (University of the Witwatersrand / CSIR)</li></ul></li><li><p><strong>A new convergent variant of Q-learning with linear function approximation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Diogo Carvalho (GAIPS, INESC-ID) · Francisco S. Melo (IST/INESC-ID) · Pedro A. Santos (Instituto Superior TÃ©cnico)</li></ul></li><li><p><strong>Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Zhiyuan Xu (Syracuse University) · Kun Wu (Syracuse University) · Zhengping Che (DiDi AI Labs, Didi Chuxing) · Jian Tang (DiDi AI Labs, DiDi Chuxing) · Jieping Ye (Didi Chuxing)</li></ul></li><li><p><strong>Multi-task Batch Reinforcement Learning with Metric Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jiachen Li (University of California, San Diego) · Quan Vuong (University of California San Diego) · Shuang Liu (University of California, San Diego) · Minghua Liu (UCSD) · Kamil Ciosek (Microsoft) · Henrik Christensen (UC San Diego) · Hao Su (UCSD)</li></ul></li><li><p><strong>Demystifying Orthogonal Monte Carlo and Beyond.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Han Lin (Columbia University) · Haoxian Chen (Columbia University) · Krzysztof M Choromanski (Google Brain Robotics) · Tianyi Zhang (Columbia University) · Clement Laroche (Columbia University)</li></ul></li><li><p><strong>On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Bin Hu (University of Illinois at Urbana-Champaign) · Tamer Basar (University of Illinois at Urbana-Champaign)</li></ul></li><li><p><strong>Towards Playing Full MOBA Games with Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Deheng Ye (Tencent) · Guibin Chen (Tencent) · Wen Zhang (Tencent) · chen sheng (qq) · Bo Yuan (Tencent) · Bo Liu (Tencent) · Jia Chen (Tencent) · Hongsheng Yu (Tencent) · Zhao Liu (Tencent) · Fuhao Qiu (Tencent AI Lab) · Liang Wang (Tencent) · Tengfei Shi (Tencent) · Yinyuting Yin (Tencent) · Bei Shi (Tencent AI Lab) · Lanxiao Huang (Tencent) · qiang fu (Tencent AI Lab) · Wei Yang (Tencent AI Lab) · Wei Liu (Tencent AI Lab)</li></ul></li><li><p><strong>How to Learn a Useful Critic? Model-based Action-Gradient-Estimator Policy Optimization.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Pierluca D’Oro (MILA) · Wojciech  JaÅ›kowski (NNAISENSE SA)</li></ul></li><li><p><strong>Reinforcement Learning in Factored MDPs: Oracle-Efficient Algorithms and Tighter Regret Bounds for the Non-Episodic Setting.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ziping Xu (University of Michigan) · Ambuj Tewari (University of Michigan)</li></ul></li><li><p><strong>HiPPO: Recurrent Memory with Optimal Polynomial Projections.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Albert Gu (Stanford) · Tri Dao (Stanford University) · Stefano Ermon (Stanford) · Atri Rudra (University at Buffalo, SUNY) · Christopher RÃ© (Stanford)</li></ul></li><li><p><strong>Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Julien Roy (Mila) · Paul Barde (Quebec AI institute - Ubisoft La Forge) · FÃ©lix G Harvey (Polytechnique MontrÃ©al) · Derek Nowrouzezahrai (McGill University) · Chris Pal (MILA, Polytechnique MontrÃ©al, Element AI)</li></ul></li><li><p><strong>Bias no more: high-probability data-dependent regret bounds for adversarial bandits and MDPs.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Chung-Wei Lee (University of Southern California) · Haipeng Luo (University of Southern California) · Chen-Yu Wei (University of Southern California) · Mengxiao Zhang (University of Southern California)</li></ul></li><li><p><strong>Minimax Confidence Interval for Off-Policy Evaluation and Policy Optimization.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Nan Jiang (University of Illinois at Urbana-Champaign) · Jiawei Huang (University of Illinois at Urbana-Champaign)</li></ul></li><li><p><strong>Confounding-Robust Policy Evaluation in Infinite-Horizon Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Nathan Kallus (Cornell University) · Angela Zhou (Cornell University)</li></ul></li><li><p><strong>Simultaneously Learning Stochastic and Adversarial Episodic MDPs with Known Transition.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Tiancheng Jin (University of Southern California) · Haipeng Luo (University of Southern California)</li></ul></li><li><p><strong>Learning Retrospective Knowledge with Reverse Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Shangtong Zhang (University of Oxford) · Vivek Veeriah (University of Michigan) · Shimon Whiteson (University of Oxford)</li></ul></li><li><p><strong>Combining Deep Reinforcement Learning and Search for Imperfect-Information Games.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Noam Brown (Facebook AI Research) · Anton Bakhtin (Facebook AI Research) · Adam Lerer (Facebook AI Research) · Qucheng Gong (Facebook AI Research)</li></ul></li><li><p><strong>Variance reduction for Langevin Monte Carlo in high dimensional sampling problems.</strong> [<a href>论文链接</a>]</p><ul><li>作者: ZHIYAN DING (University of Wisconsin-Madison) · Qin Li (University of Wisconsin-Madison)</li></ul></li><li><p><strong>POMO: Policy Optimization with Multiple Optima for Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yeong-Dae Kwon (Samsung SDS) · Jinho Choo (Samsung SDS) · Byoungjip Kim (Samsung SDS) · Iljoo Yoon (Samsung SDS) · Youngjune Gwon (Samsung SDS) · Seungjai Min (Samsung SDS)</li></ul></li><li><p><strong>Mixed Hamiltonian Monte Carlo for Mixed Discrete and Continuous Variables.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Guangyao Zhou (Vicarious AI)</li></ul></li><li><p><strong>Self-Paced Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Pascal Klink (TU Darmstadt) · Carlo D’Eramo (TU Darmstadt) · Jan Peters (TU Darmstadt &amp; MPI Intelligent Systems) · Joni Pajarinen (TU Darmstadt)</li></ul></li><li><p><strong>Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Sebastian Curi (ETH ZÃ¼rich) · Felix Berkenkamp (Bosch Center for Artificial Intelligence) · Andreas Krause (ETH Zurich)</li></ul></li><li><p><strong>Doubly Robust Off-Policy Value and Gradient Estimation for Deterministic Policies.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Nathan Kallus (Cornell University) · Masatoshi Uehara (Cornell University)</li></ul></li><li><p><strong>Off-Policy Evaluation and Learning for External Validity under a Covariate Shift.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Masatoshi Uehara (Cornell University) · Masahiro Kato (The University of Tokyo) · Shota Yasui (Cyberagent)</li></ul></li><li><p><strong>Improving Sample Complexity Bounds for (Natural) Actor-Critic Algorithms.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Tengyu Xu (The Ohio State University) · Zhe Wang (Ohio State University) · Yingbin Liang (The Ohio State University)</li></ul></li><li><p><strong>Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jiajin Li (The Chinese University of Hong Kong) · Caihua Chen (Nanjing University) · Anthony Man-Cho So (CUHK)</li></ul></li><li><p><strong>A maximum-entropy approach to off-policy evaluation in average-reward MDPs.</strong> [<a href="https://arxiv.org/abs/2006.12620" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Nevena Lazic (DeepMind) · Dong Yin (DeepMind) · Mehrdad Farajtabar (DeepMind) · Nir Levine (DeepMind) · Dilan Gorur (DeepMind) · Chris Harris (Google) · Dale Schuurmans (Google Brain &amp; University of Alberta)</li><li>这项工作的重点是在无限水平的无折扣马尔可夫决策过程（MDP）中使用函数逼近的off-policy评估（OPE）。对于遍历和线性的MDP（即在某些已知特征中奖励和动态是线性的），我们提供了第一个有限样本OPE误差界限，将现有结果扩展到了偶发和折扣情况之外。在更一般的情况下，当特征动态近似线性且具有任意奖励时，我们提出了一种使用函数逼近来估计平稳分布的新方法。 我们将这个问题公式构造为在经验动态下找到匹配特征期望值的最大熵分布。我们表明，这导致指数族分布，其足够的统计量是特征，与监督学习中的最大熵方法平行。 我们在多种环境中证明了提出的OPE方法的有效性。</li></ul></li><li><p><strong>Off-policy Policy Evaluation For Sequential Decisions Under Unobserved Confounding.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Hongseok Namkoong (Stanford University) · Ramtin Keramati (Stanford University) · Steve Yadlowsky (Stanford University) · Emma Brunskill (Stanford University)</li></ul></li><li><p><strong>Self-Imitation Learning via Generalized Lower Bound Q-learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yunhao Tang (Columbia University)</li></ul></li><li><p><strong>Weakly-Supervised Reinforcement Learning for Controllable Behavior.</strong> [<a href="https://arxiv.org/abs/2004.02860" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Lisa Lee (CMU / Google Brain / Stanford) · Ben Eysenbach (Carnegie Mellon University) · Russ Salakhutdinov (Carnegie Mellon University) · Shixiang (Shane) Gu (Google Brain) · Chelsea Finn (Stanford)</li><li>强化学习（RL）是学习采取动作解决任务的强大框架。但是，在许多设定下，智能体必须将所有可能任务的难以想象的巨大空间缩小到当前被要求解决的单个任务。我们是否可以将任务空间限制为语义上有意义的任务？ 在这项工作中，我们介绍了一个框架，该框架使用弱监督自动将任务的这个语义有意义的子空间与无意义的“chaff”任务的巨大空间自动区分开。我们表明，该学到的子空间能够进行有效的探索，并提供包含状态之间距离信息的表示。在各种具有挑战性的，基于视觉的连续控制问题上，我们的方法可带来可观的性能提升，尤其是随着环境复杂性的提高。</li></ul></li><li><p><strong>An Improved Analysis of  (Variance-Reduced) Policy Gradient and Natural Policy Gradient Methods.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yanli Liu (UCLA) · Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Tamer Basar (University of Illinois at Urbana-Champaign) · Wotao Yin (Alibaba US, DAMO Academy)</li></ul></li><li><p><strong>MOReL: Model-Based Offline Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Rahul Kidambi (Cornell University) · Aravind Rajeswaran (University of Washington) · Praneeth Netrapalli (Microsoft Research) · Thorsten Joachims (Cornell)</li></ul></li><li><p><strong>Zap Q-Learning With Nonlinear Function Approximation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Shuhang Chen (University of Florida) · Adithya M Devraj (University of Florida) · Fan Lu (University of Florida) · Ana Busic (INRIA) · Sean Meyn (University of Florida)</li></ul></li><li><p><strong>Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ruosong Wang (Carnegie Mellon University) · Russ Salakhutdinov (Carnegie Mellon University) · Lin Yang (UCLA)</li></ul></li><li><p><strong>Security Analysis of Safe and Seldonian Reinforcement Learning Algorithms.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Pinar Ozisik (UMass Amherst) · Philip Thomas (University of Massachusetts Amherst)</li></ul></li><li><p><strong>RepPoints v2: Verification Meets Regression for Object Detection.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yihong Chen (Peking University) · Zheng Zhang (MSRA) · Yue Cao (Microsoft Research) · Liwei Wang (Peking University) · Stephen Lin (Microsoft Research) · Han Hu (Microsoft Research Asia)</li></ul></li><li><p><strong>Learning to Communicate in Multi-Agent Systems via Transformer-Guided Program Synthesis.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jeevana Priya Inala (MIT) · Yichen Yang (MIT) · James Paulos (University of Pennsylvania) · Yewen Pu (MIT) · Osbert Bastani (University of Pennysylvania) · Vijay Kumar (University of Pennsylvania) · Martin Rinard (MIT) · Armando Solar-Lezama (MIT)</li></ul></li><li><p><strong>Belief-Dependent Macro-Action Discovery in POMDPs using the Value of Information.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Genevieve E Flaspohler (Massachusetts Institute of Technology) · Nicholas Roy (MIT) · John W Fisher III (MIT)</li></ul></li><li><p><strong>Bayesian Multi-type Mean Field Multi-agent Imitation Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Fan Yang (University at Buffalo) · Alina Vereshchaka (University at Buffalo) · Changyou Chen (University at Buffalo) · Wen Dong (University at Buffalo)</li></ul></li><li><p><strong>Model-based Adversarial Meta-Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Zichuan Lin (Tsinghua University) · Garrett W. Thomas (Stanford University) · Guangwen Yang (Tsinghua University) · Tengyu Ma (Stanford University)</li></ul></li><li><p><strong>Provably Efficient Neural GTD for Off-Policy Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Hoi-To Wai (The Chinese University of Hong Kong) · Zhuoran Yang (Princeton) · Zhaoran Wang (Northwestern University) · Mingyi Hong (University of Minnesota)</li></ul></li><li><p><strong>A Randomized Algorithm to Reduce the Support of Discrete Measures.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Francesco Cosentino (University of Oxford) · Harald Oberhauser (University of Oxford) · Alessandro Abate (University of Oxford)</li></ul></li><li><p><strong>Model Inversion Networks for Model-Based Optimization.</strong> [<a href="https://arxiv.org/abs/1912.13464" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Aviral Kumar (UC Berkeley) · Sergey Levine (UC Berkeley)</li><li>在这项工作中，我们旨在解决数据驱动的优化问题，其中的目标是找到一个输入，该输入可以在访问具有相应分数的输入数据集的情况下最大化未知分数函数。当输入为高维且有效输入构成该空间的一小部分子集（例如，有效的蛋白质序列或有效的自然图像）时，此类基于模型的优化问题将变得异常困难，因为优化器必须避免分布不均和无效的输入。我们建议使用模型反演网络（MINs）解决此类问题，模型反演网络学习从得分到输入的逆映射。MINs可以扩展到高维输入空间，并利用脱机记录的数据来解决上下文优化和非上下文优化问题。MINs还可以处理offline数据源和active数据收集。我们从贝叶斯优化文献，基于高维模型的图像和蛋白质设计优化问题以及从记录数据中进行的上下文老虎机优化等任务来评估MINs。</li></ul></li><li><p><strong>Safe Reinforcement Learning via Curriculum Induction.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Matteo Turchetta (ETH Zurich) · Andrey Kolobov (Microsoft Research) · Shital Shah (Microsoft) · Andreas Krause (ETH Zurich) · Alekh Agarwal (Microsoft Research)</li></ul></li><li><p><strong>Conservative Q-Learning for Offline Reinforcement Learning.</strong> [<a href="https://arxiv.org/abs/2006.04779" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Aviral Kumar (UC Berkeley) · Aurick Zhou (University of California, Berkeley) · George Tucker (Google Brain) · Sergey Levine (UC Berkeley)</li><li>在强化学习（RL）中有效地利用以前收集的大型数据集是大规模实际应用的主要挑战。offline RL算法保证无需进一步交互即可从以前收集的静态数据集中学习有效的策略。但是，在实践中，offline RL提出了一个重大挑战，标准的off-policy RL方法可能会因对价值的高估而失败，这种高估往往由数据集以及学到的策略之间的分布偏移引起，尤其是在训练复杂和多模态数据分布时。在本文中，我们提出了保守Q学习（CQL），其目的是通过学习保守Q函数来解决这些局限性，从而使该Q函数下的策略期望值lower-bounds其真实值。我们从理论上证明CQL对当前策略的价值产生了lower bound，并且可以将其纳入具有理论改进保证的策略学习过程中。在实践中，CQL通过简单的Q值正则化扩展了标准的Bellman误差目标，该Q值正则化可以在现有的深度Q-learning和actor-critic的实现基础上直接实现。在离散和连续控制域上，我们都表明CQL大大优于现有的offline RL方法，经常，学习的策略可以获得更高的2-5倍的最终回报，尤其是从复杂的多模态数据分布中学习时。</li></ul></li><li><p><strong>SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Xiaoya Li (Shannon.AI) · Yuxian Meng (Shannon.AI) · Mingxin Zhou (Shannon.AI) · Qinghong  Han (Shannon.AI) · Fei Wu (Zhejiang University) · Jiwei Li (Shannon.AI)</li></ul></li><li><p><strong>Variational Bayesian Monte Carlo with Noisy Likelihoods.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Luigi Acerbi (University of Helsinki)</li></ul></li><li><p><strong>Munchausen Reinforcement Learning.</strong> [<a href="https://arxiv.org/abs/2007.14430" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Nino Vieillard (Google Brain) · Olivier Pietquin (Google Research Brain Team) · Matthieu Geist (Google Brain)</li><li>自举是强化学习（RL）中的核心机制。 大多数算法都基于temporal differences，以其对当前值的估计来代替过渡状态的真实值。但是，可以利用另一个估算来引导RL：当前策略。我们的核心贡献在于一个非常简单的想法：将放缩的log-policy添加到即时奖励中。我们证明，以这种方式稍加修改Deep Q-Network（DQN）即可提供一种在Atari游戏上与分布式方法匹敌的智能体，而无需利用分布式RL，n步收益或先验重放。为了证明这种想法的通用性，我们还将其与Implicit Quantile Network（IQN）结合使用。最终的智能体在Atari上的表现要优于Rainbow，在不对原始算法进行很少修改的情况下安装了最新的技术。 为了增加此经验研究，我们提供了关于幕后发生的强大理论见解-隐式Kullback-Leibler正则化和行动差距的增加。</li></ul></li><li><p><strong>A Self-Tuning Actor-Critic Algorithm.</strong> [<a href="https://arxiv.org/abs/2002.12928" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Tom Zahavy (Technion) · Zhongwen Xu (DeepMind) · Vivek Veeriah (University of Michigan) · Matteo Hessel (Google DeepMind) · Junhyuk Oh (DeepMind) · Hado van Hasselt (DeepMind) · David Silver (DeepMind) · Satinder Singh (DeepMind)</li><li>强化学习算法对超参数的选择高度敏感，通常需要大量的人工工作才能确定在新域上表现良好的超参数。在本文中，我们迈出了一步，通过meta-gradient descent（Xu等人，2018）使用元梯度在线自动调整超参数。我们应用我们的算法Self-Tuning Actor-Critic（STAC），自调整actor-critic损失函数的所有可微超参数，发现辅助任务，并使用新型的leaky V-trace算子改善off-policy学习。STAC易于使用，高效采样并且不需要显着增加计算量。Ablative studies表明，随着我们适应更多的超参数，STAC的整体性能得到改善。 当应用于Arcade学习环境（Bellemare等人，2012）时，STAC将人类正常化得分的中位数从200％的步长从243％提高到364％。当应用于DM Control套件（Tassa等人，2018）时，STAC在使用特征学习时以3,000万步的​​平均得分从217提高到389，从像素学习时从108改进到202，在真实世界强化学习挑战赛（Dulac-Arnold等人，2020年）中从195改进到295。</li></ul></li><li><p><strong>Non-Crossing Quantile Regression for Distributional Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Fan Zhou (Shanghai University of Finance and Economics) · Jianing Wang (Shanghai University of Finance and Economics) · Xingdong Feng (Shanghai University of Finance and Economics)</li></ul></li><li><p><strong>Learning Implicit Credit Assignment for Multi-Agent Actor-Critic.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Meng Zhou (University of Sydney) · Ziyu Liu (University of Sydney) · Pengwei Sui (University of Sydney) · Yixuan Li (The University of Sydney) · Yuk Ying Chung (The University of Sydney)</li></ul></li><li><p><strong>Online Meta-Critic Learning for Off-Policy Actor-Critic Methods.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Wei Zhou (National University of Defense Technology) · Yiying Li (National University of Defense Technology) · Yongxin Yang (University of Edinburgh ) · Huaimin Wang (National University of Defense Technology) · Timothy Hospedales (University of Edinburgh)</li></ul></li><li><p><strong>Online Decision Based Visual Tracking via Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: ke Song (Shandong university) · Wei Zhang (Shandong University) · Ran Song (School of Control Science and Engineering, Shandong University) · Yibin Li (Shandong University)</li></ul></li><li><p><strong>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Paul Barde (Quebec AI institute - Ubisoft La Forge) · Julien Roy (Mila) · Wonseok Jeon (MILA, McGill University) · Joelle Pineau (McGill University) · Chris Pal (MILA, Polytechnique MontrÃ©al, Element AI) · Derek Nowrouzezahrai (McGill University)</li></ul></li><li><p><strong>Discovering Reinforcement Learning Algorithms.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Junhyuk Oh (DeepMind) · Matteo Hessel (Google DeepMind) · Wojciech Czarnecki (DeepMind) · Zhongwen Xu (DeepMind) · Hado van Hasselt (DeepMind) · Satinder Singh (DeepMind) · David Silver (DeepMind)</li></ul></li><li><p><strong>Model-based Policy Optimization with Unsupervised Model Adaptation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jian Shen (Shanghai Jiao Tong University) · Han Zhao (Carnegie Mellon University) · Weinan Zhang (Shanghai Jiao Tong University) · Yong Yu (Shanghai Jiao Tong Unviersity)</li></ul></li><li><p><strong>Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Filippos Christianos (University of Edinburgh) · Lukas SchÃ¤fer (University of Edinburgh) · Stefano Albrecht (University of Edinburgh)</li></ul></li><li><p><strong>The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Harm Van Seijen (Microsoft Research) · Hadi Nekoei (MILA) · Evan Racah (Mila, UniversitÃ© de MontrÃ©al) · Sarath Chandar (Mila / Ã‰cole Polytechnique de MontrÃ©al)</li></ul></li><li><p><strong>Deep Inverse Q-learning with Constraints.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Gabriel Kalweit (University of Freiburg) · Maria Huegle (University of Freiburg) · Moritz Werling (BMWGroup, Unterschleissheim) · Joschka Boedecker (University of Freiburg)</li></ul></li><li><p><strong>Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Nino Vieillard (Google Brain) · Tadashi Kozuno (Okinawa Institute of Science and Technology) · Bruno Scherrer (INRIA) · Olivier Pietquin (Google Research    Brain Team) · Remi Munos (DeepMind) · Matthieu Geist (Google Brain)</li></ul></li><li><p><strong>Task-agnostic Exploration in Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Xuezhou Zhang (UW-Madison) · Yuzhe Ma (University of Wisconsin-Madison) · Adish Singla (MPI-SWS)</li></ul></li><li><p><strong>Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Tianren Zhang (Tsinghua University) · Shangqi Guo (Tsinghua University) · Tian Tan (Stanford University) · Xiaolin Hu (Tsinghua University) · Feng Chen (Tsinghua University)</li></ul></li><li><p><strong>Reinforcement Learning with Feedback Graphs.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Christoph Dann (Carnegie Mellon University) · Yishay Mansour (Google) · Mehryar Mohri (Courant Inst. of Math. Sciences &amp; Google Research) · Ayush Sekhari (Cornell University) · Karthik Sridharan (Cornell University)</li></ul></li><li><p><strong>Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jianda Chen (Nanyang Technological University) · Shangyu Chen (Nanyang Technological University, Singapore) · Sinno Jialin Pan (Nanyang Technological University, Singapore)</li></ul></li><li><p><strong>Towards Safe Policy Improvement for Non-Stationary MDPs.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yash Chandak (University of Massachusetts Amherst) · Scott Jordan (University of Massachusetts Amherst) · Georgios Theocharous (Adobe Research) · Martha White (University of Alberta) · Philip Thomas (University of Massachusetts Amherst)</li></ul></li><li><p><strong>Multi-Task Reinforcement Learning with Soft Modularization.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ruihan Yang (UC San Diego) · Huazhe Xu (UC Berkeley) · YI WU (UC Berkeley) · Xiaolong Wang (UCSD/UC Berkeley)</li></ul></li><li><p><strong>Weighted QMIX: Improving Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Tabish Rashid (University of Oxford) · Gregory Farquhar (University of Oxford) · Bei Peng (University of Oxford) · Shimon Whiteson (University of Oxford)</li></ul></li><li><p><strong>MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Elise van der Pol (University of Amsterdam) · Daniel Worrall (University of Amsterdam) · Herke van Hoof (University of Amsterdam) · Frans Oliehoek (TU Delft) · Max Welling (University of Amsterdam / Qualcomm AI Research)</li></ul></li><li><p><strong>CoinDICE: Off-Policy Confidence Interval Estimation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Bo Dai (Google Brain) · Ofir Nachum (Google Brain) · Yinlam Chow (Google Research) · Lihong Li (Google Research) · Csaba Szepesvari (DeepMind / University of Alberta) · Dale Schuurmans (Google Brain &amp; University of Alberta)</li></ul></li><li><p><strong>An Operator View of Policy Gradient Methods.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Dibya Ghosh (Google) · Marlos C. Machado (Google Brain) · Nicolas Le Roux (Google Brain)</li></ul></li><li><p><strong>On Efficiency in Hierarchical Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Zheng Wen (DeepMind) · Doina Precup (DeepMind) · Morteza Ibrahimi (DeepMind) · Andre Barreto (DeepMind) · Benjamin Van Roy (Stanford University) · Satinder Singh (DeepMind)</li></ul></li><li><p><strong>Variational Policy Gradient Method for Reinforcement Learning with General Utilities.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Junyu Zhang (Princeton University) · Alec Koppel (U.S. Army Research Laboratory) · Amrit Singh Bedi (US Army Research Laboratory) · Csaba Szepesvari (DeepMind / University of Alberta) · Mengdi Wang (Princeton University)</li></ul></li><li><p><strong>A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yue Wu (University of California, Los Angeles) · Weitong ZHANG (University of California, Los Angeles) · Pan Xu (University of California, Los Angeles) · Quanquan Gu (UCLA)</li></ul></li><li><p><strong>POLY-HOOT: Monte-Carlo Planning in Continuous Space MDPs with Non-Asymptotic Analysis.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Weichao Mao (University of Illinois Urbana-Champaign) · Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Qiaomin Xie (Cornell University) · Tamer Basar (University of Illinois at Urbana-Champaign)</li></ul></li><li><p><strong>Can Temporal-Diï¬€erence and Q-Learning Learn Representation? A Mean-Field Theory.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yufeng Zhang (Northwestern University) · Qi Cai (Northwestern University) · Zhuoran Yang (Princeton) · Yongxin Chen (Georgia Institute of Technology) · Zhaoran Wang (Northwestern University)</li></ul></li><li><p><strong>Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jianzhun Du (Harvard University) · Joseph Futoma (Harvard University) · Finale Doshi-Velez (Harvard)</li></ul></li><li><p><strong>Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Gen Li (Tsinghua University) · Yuting Wei (Carnegie Mellon University) · Yuejie Chi (CMU) · Yuantao Gu (Tsinghua University) · Yuxin Chen (Princeton University)</li></ul></li><li><p><strong>Reinforcement Learning with Augmented Data.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Misha Laskin (UC Berkeley) · Kimin Lee (UC Berkeley) · Adam Stooke (UC Berkeley) · Lerrel Pinto (New York University) · Pieter Abbeel (UC Berkeley &amp; covariant.ai) · Aravind Srinivas (UC Berkeley)</li></ul></li><li><p><strong>Improved Sample Complexity for Incremental Autonomous Exploration in MDPs.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jean Tarbouriech (Facebook AI Research Paris &amp; Inria Lille) · Matteo Pirotta (Facebook AI Research) · Michal Valko (DeepMind Paris and Inria Lille - Nord Europe) · Alessandro Lazaric (Facebook Artificial Intelligence Research)</li></ul></li><li><p><strong>EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jiachen Li (University of California, Berkeley) · Fan Yang (University of California, Berkeley) · Masayoshi Tomizuka (University of California, Berkeley) · Chiho Choi (Honda Research Institute US)</li></ul></li><li><p><strong>Autofocused oracles for model-based design.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Clara Fannjiang (UC Berkeley) · Jennifer Listgarten (UC Berkeley)</li></ul></li><li><p><strong>Off-Policy Evaluation via the Regularized Lagrangian.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Mengjiao Yang (Google) · Ofir Nachum (Google Brain) · Bo Dai (Google Brain) · Lihong Li (Google Research) · Dale Schuurmans (Google Brain &amp; University of Alberta)</li></ul></li><li><p><strong>Reinforcement Learning with Combinatorial Actions: An Application to Vehicle Routing.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Arthur Delarue (MIT) · Ross Anderson (Google Research) · Christian Tjandraatmadja (Google)</li></ul></li><li><p><strong>MOPO: Model-based Offline Policy Optimization.</strong> [<a href="https://arxiv.org/abs/2005.13239" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Tianhe Yu (Stanford University) · Garrett W. Thomas (Stanford University) · Lantao Yu (Stanford University) · Stefano Ermon (Stanford) · James Zou (Stanford University) · Sergey Levine (UC Berkeley) · Chelsea Finn (Stanford) · Tengyu Ma (Stanford University)</li><li><a href="https://github.com/tianheyu927/mopo" target="_blank" rel="noopener">code</a></li><li>offline 强化学习（RL）指的是完全从大量先前收集的数据中学习策略的问题。这个问题设置提供了利用此类数据集获取策略的希望，而无需进行任何昂贵或危险的主动探索。但是，由于脱机训练数据与所学习策略访问的状态之间的分布偏移，这一问题同样具有挑战性。尽管最近取得了重大进展，但最成功的现有方法是无模型的，并且将策略限制在数据的支持上，从而无法将模型推广到未见状态。在本文中，我们首先观察到，与无模型方法相比，现有的基于模型的RL算法在offline环境中已经产生了可观的收益。但是，为在线设置设计的基于标准模型的RL方法没有提供明确的机制来避免offline设置的分布偏移问题。取而代之的是，我们建议修改现有的基于模型的RL方法，方法是由动态的不确定性人为惩罚所获得的奖励。我们从理论上表明，该算法在真实MDP下最大化了策略收益的下限。我们还描述了在离开批数据支持的风险与收益之间的权衡。我们的算法Model-based Offline Policy Optimization（MOPO）在现有的offline RL基准测试和两项具有挑战性的连续控制任务（需要利用新任务收集的数据进行泛化）上均优于标准的基于模型的RL算法和现有的最新无模型offline RL算法。</li></ul></li><li><p><strong>Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Shaocong Ma (University of Utah) · Yi Zhou (University of Utah) · Shaofeng Zou (University at Buffalo, the State University of New York)</li></ul></li><li><p><strong>Dynamic Regret of Policy Optimization in Non-stationary Environments.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yingjie Fei (Cornell University) · Zhuoran Yang (Princeton) · Zhaoran Wang (Northwestern University) · Qiaomin Xie (Cornell University)</li></ul></li><li><p><strong>DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction.</strong> [<a href="https://arxiv.org/abs/2003.07305" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Aviral Kumar (UC Berkeley) · Abhishek Gupta (University of California, Berkeley) · Sergey Levine (UC Berkeley)</li><li><a href="https://bair.berkeley.edu/blog/2020/03/16/discor/" target="_blank" rel="noopener">blog</a></li><li><a href="https://github.com/ku2482/discor.pytorch" target="_blank" rel="noopener">code</a></li><li>深度强化学习可以学习适用于各种任务的有效策略，但众所周知，由于不稳定和对超参数敏感，因此难以使用。其原因尚不清楚。当使用标准的监督方法（例如，针对老虎机问题）时，on-policy的数据收集会提供“hard negatives”，可以在策略可能访问的那些状态和动作中准确地纠正模型。我们称这种现象为“校正反馈”。我们表明，基于自举的Q-learning算法不一定能从此校正反馈中受益，并且对算法收集的经验进行训练不足以校正Q函数中的错误。实际上，Q-learning和相关方法可能会表现出智能体收集的经验分布与该经验的训练所诱导的策略之间病态的相互作用，从而导致潜在的不稳定，局部最优收敛，以及从嘈杂、稀疏或延迟的奖励中学习获得较差的结果。我们从理论和实验上证明了这个问题的存在。然后，我们表明对数据分布的特定修正可以缓解此问题。基于这些观察，我们提出了一种新算法DisCor，该算法可计算出该最佳分布的近似值，并使用它重新加权用于训练的transitions，从而在一系列具有挑战性的RL设置（例如多任务学习和从嘈杂的奖励信号中学习）中得到实质性的改进。</li></ul></li><li><p><strong>FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Alekh Agarwal (Microsoft Research) · Sham Kakade (University of Washington) · Akshay Krishnamurthy (Microsoft) · Wen Sun (Microsoft Research NYC)</li></ul></li><li><p><strong>Neurosymbolic Reinforcement Learning with Formally Verified Exploration.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Greg Anderson (University of Texas at Austin) · Abhinav Verma (Rice University) · Isil Dillig (UT Austin) · Swarat Chaudhuri (The University of Texas at Austin)</li></ul></li><li><p><strong>Generalized Hindsight for Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Alexander Li (UC Berkeley) · Lerrel Pinto (New York University) · Pieter Abbeel (UC Berkeley &amp; covariant.ai)</li></ul></li><li><p><strong>Finite-Time Analysis for Double Q-learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Huaqing Xiong (Ohio State University) · Lin Zhao (National University of Singapore) · Yingbin Liang (The Ohio State University) · Wei  Zhang (Southern University of Science and Technology)</li></ul></li><li><p><strong>Subgroup-based Rank-1 Lattice Quasi-Monte Carlo.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yueming LYU (University of Technology Sydney) · Yuan Yuan (MIT) · Ivor Tsang (University of Technology, Sydney)</li></ul></li><li><p><strong>Meta-Gradient Reinforcement Learning with an Objective Discovered Online.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Zhongwen Xu (DeepMind) · Hado van Hasselt (DeepMind) · Matteo Hessel (Google DeepMind) · Junhyuk Oh (DeepMind) · Satinder Singh (DeepMind) · David Silver (DeepMind)</li></ul></li><li><p><strong>TorsionNet: A Reinforcement Learning Approach to Sequential Conformer Search.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Tarun Gogineni (University of Michigan) · Ziping Xu (University of Michigan) · Exequiel  Punzalan (University of Michigan) · Runxuan Jiang (University of Michigan) · Joshua Kammeraad (University of Michigan) · Ambuj Tewari (University of Michigan) · Paul Zimmerman (University of Michigan)</li></ul></li><li><p><strong>Succinct and Robust Multi-Agent Communication With Temporal Message Control.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Sai Qian Zhang (Harvard University) · Qi  Zhang (Amazon) · Jieyu Lin (University of Toronto)</li></ul></li><li><p><strong>Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Cong Zhang (Nanyang Technological University) · Wen Song (Institute of Marine Scinece and Technology, Shandong University) · Zhiguang Cao (National University of Singapore) · Jie Zhang (Nanyang Technological University) · Puay Siew Tan (SIMTECH) · Xu Chi (Singapore Institute of Manufacturing Technology, A-Star)</li></ul></li><li><p><strong>Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Qiwen Cui (Peking University) · Lin Yang (UCLA)</li></ul></li><li><p><strong>Instance-based Generalization in Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Martin Bertran (Duke University) · Natalia L Martinez (Duke University) · Mariano Phielipp (Intel AI Labs) · Guillermo Sapiro (Duke University)</li></ul></li><li><p><strong>Preference-based Reinforcement Learning with Finite-Time Guarantees.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yichong Xu (Carnegie Mellon University) · Ruosong Wang (Carnegie Mellon University) · Lin Yang (UCLA) · Aarti Singh (CMU) · Artur Dubrawski (Carnegie Mellon University)</li></ul></li><li><p><strong>Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Salman Habib (New Jersey Institute of Tech) · Allison Beemer (New Jersey Institute of Technology) · Joerg Kliewer (New Jersey Institute of Technology)</li></ul></li><li><p><strong>BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Xinyue Chen (NYU Shanghai) · Zijian Zhou (NYU Shanghai) · Zheng Wang (NYU Shanghai) · Che Wang (New York University) · Yanqiu Wu (New York University) · Keith Ross (NYU Shanghai)</li></ul></li><li><p><strong>Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Mengdi Xu (Carnegie Mellon University) · Wenhao Ding (Carnegie Mellon University) · Jiacheng Zhu (Carnegie Mellon University) · ZUXIN LIU (Carnegie Mellon University) · Baiming Chen (Tsinghua University) · Ding Zhao (Carnegie Mellon University)</li></ul></li><li><p><strong>On Reward-Free Reinforcement Learning with Linear Function Approximation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ruosong Wang (Carnegie Mellon University) · Simon Du (Institute for Advanced Study) · Lin Yang (UCLA) · Russ Salakhutdinov (Carnegie Mellon University)</li></ul></li><li><p><strong>Near-Optimal Reinforcement Learning with Self-Play.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yu Bai (Salesforce Research) · Chi Jin (Princeton University) · Tiancheng Yu (MIT )</li></ul></li><li><p><strong>Robust Multi-Agent Reinforcement Learning with Model Uncertainty.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · TAO SUN (Amazon.com) · Yunzhe Tao (Amazon Artificial Intelligence) · Sahika Genc (Amazon Artificial Intelligence) · Sunil Mallya (Amazon AWS) · Tamer Basar (University of Illinois at Urbana-Champaign)</li></ul></li><li><p><strong>Towards Minimax Optimal Reinforcement Learning in Factored Markov Decision Processes.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yi Tian (MIT) · Jian Qian (MIT) · Suvrit Sra (MIT)</li></ul></li><li><p><strong>Scalable Multi-Agent Reinforcement Learning for Networked Systems with Average Reward.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Guannan Qu (California Institute of Technology) · Yiheng Lin (California Institute of Technology) · Adam Wierman (California Institute of Technology) · Na Li (Harvard University)</li></ul></li><li><p><strong>Constrained episodic reinforcement learning in concave-convex and knapsack settings.</strong> [<a href>论文链接</a>]</p><ul><li>作者: KiantÃ© Brantley (The University of Maryland College Park) · Miro Dudik (Microsoft Research) · Thodoris Lykouris (Microsoft Research NYC) · Sobhan Miryoosefi (Princeton University) · Max Simchowitz (Berkeley) · Aleksandrs Slivkins (Microsoft Research) · Wen Sun (Microsoft Research NYC)</li></ul></li><li><p><strong>Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Devavrat Shah (Massachusetts Institute of Technology) · Dogyoon Song (Massachusetts Institute of Technology) · Zhi Xu (MIT) · Yuzhe Yang (MIT)</li></ul></li><li><p><strong>Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Younggyo Seo (KAIST) · Kimin Lee (UC Berkeley) · Ignasi Clavera Gilaberte (UC Berkeley) · Thanard Kurutach (University of California Berkeley) · Jinwoo Shin (KAIST) · Pieter Abbeel (UC Berkeley &amp; covariant.ai)</li></ul></li><li><p><strong>Cooperative Heterogeneous Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Han Zheng (UTS) · Pengfei Wei (National University of Singapore) · Jing Jiang (University of Technology Sydney) · Guodong Long (University of Technology Sydney (UTS)) · Qinghua Lu (Data61, CSIRO) · Chengqi Zhang (University of Technology Sydney)</li></ul></li><li><p><strong>Global Convergence of Natural Primal-Dual Method for Constrained Markov Decision Processes.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Dongsheng Ding (University of Southern California) · Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Mihailo Jovanovic (University of Southern California) · Tamer Basar (University of Illinois at Urbana-Champaign)</li></ul></li><li><p><strong>Implicit Distributional Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yuguang Yue (University of Texas at Austin) · Zhendong Wang (University of Texas, Austin) · Mingyuan Zhou (University of Texas at Austin)</li></ul></li><li><p><strong>Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Sreejith Balakrishnan (National University of Singapore) · Quoc Phong Nguyen (National University of Singapore) · Bryan Kian Hsiang Low (National University of Singapore) · Harold Soh (National University Singapore)</li></ul></li><li><p><strong>EPOC: A Provably Correct Policy Gradient Approach to Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Alekh Agarwal (Microsoft Research) · Mikael Henaff (Microsoft) · Sham Kakade (University of Washington) · Wen Sun (Microsoft Research NYC)</li></ul></li><li><p><strong>Provably Efficient Reinforcement Learning with Kernel and Neural Function Approximations.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Zhuoran Yang (Princeton) · Chi Jin (Princeton University) · Zhaoran Wang (Northwestern University) · Mengdi Wang (Princeton University) · Michael Jordan (UC Berkeley)</li></ul></li><li><p><strong>Decoupled Policy Gradient Methods for Competitive Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Constantinos Daskalakis (MIT) · Dylan Foster (MIT) · Noah Golowich (Massachusetts Institute of Technology)</li></ul></li><li><p><strong>Upper Confidence Primal-Dual Reinforcement Learning for CMDP with Adversarial Loss.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Shuang Qiu (University of Michigan) · Xiaohan Wei (University of Southern California) · Zhuoran Yang (Princeton) · Jieping Ye (University of Michigan) · Zhaoran Wang (Northwestern University)</li></ul></li><li><p><strong>Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Sham Kakade (University of Washington) · Tamer Basar (University of Illinois at Urbana-Champaign) · Lin Yang (UCLA)</li></ul></li><li><p><strong>PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Henry Charlesworth (University of Warwick) · Giovanni Montana (University of Warwick)</li></ul></li><li><p><strong>Improving Generalization in Reinforcement Learning with Mixture Regularization.</strong> [<a href>论文链接</a>]</p><ul><li>作者: KAIXIN WANG (National University of Singapore) · Bingyi Kang (National University of Singapore) · Jie Shao (Fudan University) · Jiashi Feng (National University of Singapore)</li></ul></li><li><p><strong>A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Arnu Pretorius (InstaDeep) · Scott Cameron (Instadeep) · Elan van Biljon (Stellenbosch University) · Thomas Makkink (InstaDeep) · Shahil Mawjee (InstaDeep) · Jeremy du Plessis (University of Cape Town) · Jonathan Shock (University of Cape Town) · Alexandre Laterre (InstaDeep) · Karim Beguir (InstaDeep)</li></ul></li><li><p><strong>Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Georgios Amanatidis (University of Essex) · Federico Fusco (Sapienza University of Rome) · Philip Lazos (Sapienza University of Rome) · Stefano Leonardi (Sapienza University of Rome) · Rebecca ReiffenhÃ¤user (Sapienza University of Rome)</li></ul></li><li><p><strong>Planning in Markov Decision Processes with Gap-Dependent Sample Complexity.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Anders Jonsson (Universitat Pompeu Fabra) · Emilie Kaufmann (CNRS) · Pierre Menard (Inria) · Omar Darwiche Domingues (Inria) · Edouard Leurent (INRIA) · Michal Valko (DeepMind)</li></ul></li><li><p><strong>Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yunqiu Xu (University of Technology Sydney) · Meng Fang (Tencent) · Ling Chen (“ University of Technology, Sydney, Australia”) · Yali Du (University College London) · Joey Tianyi Zhou (IHPC, A*STAR) · Chengqi Zhang (University of Technology Sydney)</li></ul></li><li><p><strong>Robust Reinforcement Learning via Adversarial training with Langevin Dynamics.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Parameswaran Kamalaruban (EPFL) · Yu-Ting Huang (EPFL) · Ya-Ping Hsieh (EPFL) · Paul Rolland (EPFL) · Cheng Shi (Unversity of Basel) · Volkan Cevher (EPFL)</li></ul></li><li><p><strong>Interferobot: aligning an optical interferometer by a reinforcement learning agent.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Dmitry Sorokin (Russian Quantum Center) · Alexander Ulanov (Russian Quantum Center) · Ekaterina Sazhina (Russian Quantum Center) · Alexander Lvovsky (Oxford University)</li></ul></li><li><p><strong>Reinforcement Learning for Control with Multiple Frequencies.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Jongmin Lee (KAIST) · ByungJun Lee (KAIST) · Kee-Eung Kim (KAIST)</li></ul></li><li><p><strong>Learning to Play Sequential Games versus Unknown Opponents.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Pier Giuseppe Sessa (ETH ZÃ¼rich) · Ilija Bogunovic (ETH Zurich) · Maryam Kamgarpour (ETH ZÃ¼rich) · Andreas Krause (ETH Zurich)</li></ul></li><li><p><strong>Contextual Games: Multi-Agent Learning with Side Information.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Pier Giuseppe Sessa (ETH ZÃ¼rich) · Ilija Bogunovic (ETH Zurich) · Andreas Krause (ETH Zurich) · Maryam Kamgarpour (ETH ZÃ¼rich)</li></ul></li><li><p><strong>Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yingjie Fei (Cornell University) · Zhuoran Yang (Princeton) · Yudong Chen (Cornell University) · Zhaoran Wang (Northwestern University) · Qiaomin Xie (Cornell University)</li></ul></li><li><p><strong>Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Aaron Sonabend (Harvard University) · Junwei Lu () · Leo Anthony Celi (Massachusetts Institute of Technology) · Tianxi Cai (Harvard School of Public Health) · Peter Szolovits (MIT)</li></ul></li><li><p><strong>Dynamic allocation of limited memory resources in reinforcement learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Nisheet Patel (University of Geneva) · Luigi Acerbi (University of Helsinki) · Alexandre Pouget (University of Geneva)</li></ul></li><li><p><strong>AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Afshin Oroojlooy (SAS Institute, Inc) · Mohammadreza Nazari (SAS Institute Inc.) · Davood Hajinezhad (SAS Institute Inc.) · Jorge Silva (SAS)</li></ul></li><li><p><strong>Sample-Efficient Reinforcement Learning of Undercomplete POMDPs.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Chi Jin (Princeton University) · Sham Kakade (University of Washington) · Akshay Krishnamurthy (Microsoft) · Qinghua Liu (Princeton University)</li></ul></li><li><p><strong>Learning discrete distributions with infinite support.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Doron Cohen (Ben-Gurion University of the Negev) · Aryeh Kontorovich (Ben Gurion University) · Geoï¬€rey Wolfer (Ben-Gurion University of the Negev)</li></ul></li><li><p><strong>Joint Policy Search for Multi-agent Collaboration with Incomplete Information.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yuandong Tian (Facebook AI Research) · Qucheng Gong (Facebook AI Research) · Yu Jiang (Facebook AI Research)</li></ul></li><li><p><strong>R-learning in actor-critic model offers a biologically relevant mechanism for sequential decision-making.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Sergey Shuvaev (Cold Spring Harbor Laboratory) · Sarah Starosta (Washington University in St. Louis) · Duda Kvitsiani (Aarhus University) · Adam Kepecs (Washington University in St. Louis) · Alexei Koulakov (Cold Spring Harbor Laboratory)</li></ul></li><li><p><strong>Multi-agent active perception with prediction rewards.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Mikko Lauri (University of Hamburg) · Frans Oliehoek (TU Delft)</li></ul></li><li><p><strong>RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ziyu Wang (Deepmind) · Caglar Gulcehre (Deepmind) · Alexander Novikov (DeepMind) · Thomas Paine (DeepMind) · Sergio GÃ³mez (DeepMind) · Konrad Zolna (DeepMind) · Rishabh Agarwal (Google Research, Brain Team) · Josh Merel (DeepMind) · Daniel Mankowitz (DeepMind) · Cosmin Paduraru (DeepMind) · Gabriel Dulac-Arnold (Google Research) · Jerry Li (Google) · Mohammad Norouzi (Google Brain) · Matthew Hoffman (DeepMind) · Nicolas Heess (Google DeepMind) · Nando de Freitas (DeepMind)</li></ul></li><li><p><strong>A local temporal difference code for distributional reinforcement learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Pablo Tano (University of Geneva) · Peter Dayan (Max Planck Institute for Biological Cybernetics) · Alexandre Pouget (University of Geneva)</li></ul></li><li><p><strong>Learning to Play No-Press Diplomacy with Best Response Policy Iteration.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Thomas Anthony (DeepMind) · Tom Eccles (DeepMind) · Andrea Tacchetti (DeepMind) · JÃ¡nos KramÃ¡r (DeepMind) · Ian Gemp (DeepMind) · Thomas Hudson (DeepMind) · Nicolas Porcel (DeepMind) · Marc Lanctot (DeepMind) · Julien Perolat (DeepMind) · Richard Everett (DeepMind) · Satinder Singh (DeepMind) · Thore Graepel (DeepMind) · Yoram Bachrach ()</li></ul></li><li><p><strong>The Value Equivalence Principle for Model-Based Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Christopher Grimm (University of Michigan) · Andre Barreto (DeepMind) · Satinder Singh (DeepMind) · David Silver (DeepMind)</li></ul></li><li><p><strong>Multi-agent Trajectory Prediction with Fuzzy Query Attention.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Nitin Kamra (University of Southern California) · Hao Zhu (Peking University) · Dweep Kumarbhai Trivedi (University of Southern California) · Ming Zhang (Peking University) · Yan Liu (University of Southern California)</li></ul></li><li><p><strong>Trust the Model When It Is Confident: Masked Model-based Actor-Critic.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Feiyang Pan (Institute of Computing Technology, Chinese Academy of Sciences) · Jia He (Huawei) · Dandan Tu (Huawei) · Qing He (Institute of Computing Technology, Chinese Academy of Sciences)</li></ul></li><li><p><strong>POMDPs in Continuous Time and Discrete Spaces.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Bastian Alt (Technische UniversitÃ¤t Darmstadt) · Matthias Schultheis (Technische UniversitÃ¤t Darmstadt) · Heinz Koeppl (Technische UniversitÃ¤t Darmstadt)</li></ul></li><li><p><strong>Steady State Analysis of Episodic Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Huang Bojun (Rakuten Institute of Technology)</li></ul></li><li><p><strong>Learning Multi-Agent Communication through Structured Attentive Reasoning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Murtaza Rangwala (Virginia Tech) · Ryan K Williams (Virginia Tech)</li></ul></li><li><p><strong>Information-theoretic Task Selection for Meta-Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ricardo Luna Gutierrez (University of Leeds) · Matteo Leonetti (University of Leeds)</li></ul></li><li><p><strong>The Mean-Squared Error of Double Q-Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Wentao Weng (Tsinghua University) · Harsh Gupta (University of Illinois at Urbana-Champaign) · Niao He (UIUC) · Lei Ying (University of Michigan) · R. Srikant (University of Illinois at Urbana-Champaign)</li></ul></li><li><p><strong>A Unifying View of Optimism in Episodic Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Gergely Neu (Universitat Pompeu Fabra) · Ciara Pike-Burke (Imperial College London)</li></ul></li><li><p><strong>Accelerating Reinforcement Learning through GPU Atari Emulation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Steven Dalton (Nvidia) · iuri frosio (nvidia)</li></ul></li><li><p><strong>Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Huan Zhang (UCLA) · Hongge Chen (MIT) · Chaowei Xiao (University of Michigan, Ann Arbor) · Bo Li (UIUC) · mingyan liu (university of Michigan, Ann Arbor) · Duane Boning (Massachusetts Institute of Technology) · Cho-Jui Hsieh (UCLA)</li></ul></li><li><p><strong>Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Guangxiang Zhu (Tsinghua university) · Minghao Zhang (Tsinghua University) · Honglak Lee (Google / U. Michigan) · Chongjie Zhang (Tsinghua University)</li></ul></li><li><p><strong>Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Guy Lorberbom (Technion) · Chris J. Maddison (University of Toronto) · Nicolas Heess (Google DeepMind) · Tamir Hazan (Technion) · Daniel Tarlow (Google Brain)</li></ul></li><li><p><strong>Hamiltonian Monte Carlo using an adjoint-differentiated Laplace approximation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Charles Margossian (Columbia) · Aki Vehtari (Aalto University) · Daniel Simpson (University of Toronto) · Raj Agrawal (MIT)</li></ul></li><li><p><strong>A Unified Switching System Perspective and Convergence Analysis of Q-Learning Algorithms.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Niao He (UIUC) · Donghwan Lee (KAIST)</li></ul></li><li><p><strong>Adaptive Discretization for Model-Based Reinforcement Learning.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Sean Sinclair (Cornell University) · Tianyu Wang (Duke University) · Gauri Jain (Cornell University) · Siddhartha Banerjee (Cornell University) · Christina Yu (Cornell University)</li></ul></li><li><p><strong>Stateful Posted Pricing with Vanishing Regret via Dynamic Deterministic Markov Decision Processes.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yuval Emek (Technion - Israel Institute of Technology) · Ron Lavi (Technion) · Rad Niazadeh (Chicago Booth School of Business) · Yangguang Shi (Technion - Israel Institute of Technology)</li></ul></li><li><p><strong>Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Yao Liu (Stanford University) · Adith Swaminathan (Microsoft Research) · Alekh Agarwal (Microsoft Research) · Emma Brunskill (Stanford University)</li></ul></li><li><p><strong>Off-Policy Interval Estimation with Lipschitz Value Iteration.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Ziyang Tang (UT Austin) · Yihao Feng (UT Austin) · Na Zhang (Tsinghua University) · Jian Peng (University of Illinois at Urbana-Champaign) · Qiang Liu (UT Austin)</li></ul></li><li><p><strong>Provably adaptive reinforcement learning in metric spaces.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Tongyi Cao (University of Massachusetts Amherst) · Akshay Krishnamurthy (Microsoft)</li></ul></li><li><p><strong>Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model.</strong> [<a href="https://arxiv.org/abs/1907.00953" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>作者: Alex Lee (UC Berkeley) · Anusha Nagabandi (UC Berkeley) · Pieter Abbeel (UC Berkeley &amp; covariant.ai) · Sergey Levine (UC Berkeley)</li><li><a href="https://alexlee-gk.github.io/slac/" target="_blank" rel="noopener">website</a></li><li><a href="https://github.com/alexlee-gk/slac" target="_blank" rel="noopener">code</a></li><li>深度强化学习（RL）算法可以使用大容量深度网络直接从图像观察中学习。但是，这些高维度的观察空间在实践中提出了许多挑战，因为策略现在必须解决两个问题：表示学习和任务学习。在这项工作中，我们通过显式学习可以加速从图像进行强化学习的潜在表示，分别解决这两个问题。我们提出了stochastic latent actor-critic（SLAC）算法：一种样本有效且高性能的RL算法，用于直接从高维图像输入中学习复杂连续控制任务的策略。SLAC提供了一种新颖且有原则的方法，通过学习紧凑的潜在表示，然后在模型学到的潜在空间中执行RL，将随机顺序模型和RL统一为一个方法。我们的实验评估表明，在一系列困难的基于图像的控制任务上，我们的方法在最终性能和样本效率方面均优于无模型和基于模型的替代方法。</li></ul></li><li><p><strong>Inverse Reinforcement Learning from a Gradient-based Learner.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Giorgia Ramponi (Politecnico di Milano) · Gianluca Drappo (Politecnico di Milano) · Marcello Restelli (Politecnico di Milano)</li></ul></li><li><p><strong>Efficient Planning in Large MDPs with Weak Linear Function Approximation.</strong> [<a href>论文链接</a>]</p><ul><li>作者: Roshan Shariff (University of Alberta) · Csaba Szepesvari (DeepMind / University of Alberta)</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NIPS2020论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading List" scheme="https://yachenkang.github.io/blog/categories/Reading-List/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>NIPS2019 Reinforcement Learning Reading List</title>
    <link href="https://yachenkang.github.io/blog/2020/06/15/NIPS2019-Reinforcement-Learning-Reading-List/"/>
    <id>https://yachenkang.github.io/blog/2020/06/15/NIPS2019-Reinforcement-Learning-Reading-List/</id>
    <published>2020-06-15T02:20:40.000Z</published>
    <updated>2020-06-24T09:16:13.561Z</updated>
    
    <content type="html"><![CDATA[<p>NIPS2019论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。</p><a id="more"></a><p>*标注的为值得精读论文</p><h2 id="Oral"><a href="#Oral" class="headerlink" title="Oral"></a>Oral</h2><ul><li><p><strong>A neurally plausible model learns successor representations in partially observable environments.</strong> Vértes, E., &amp; Sahani, M. (2019). [<a href="https://papers.nips.cc/paper/9522-a-neurally-plausible-model-learns-successor-representations-in-partially-observable-environments" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>动物需要根据传入的嘈杂的感官观察，在与环境互动时设计出最大化returns的策略。与任务相关的状态，例如环境中智能体的位置或掠食者的存在，通常不能直接观察到，而必须使用可用的感官信息来推断。已经提出的successor representations（SR）处在model-based和model-free的强化学习策略的中间，可以快速进行价值计算并快速适应奖励函数或目标位置的变化。实际上，最近的研究表明神经反应的特征与SR框架一致。但是，尚不清楚如何在部分观察到的嘈杂环境中学习和计算此类表示。在这里，我们介绍了一个使用<strong>distribution successor features</strong>的神经上合理的模型，该模型建立在分布式分布代码上，用于表示和计算不确定性，并允许通过successor representation在部分可观测的环境中进行有效的值函数计算。我们表明，分布式successor features可以在嘈杂的环境中支持强化学习，而在这种环境中，直接学习成功的策略是不可行的。</li></ul></li><li><p><strong>Batched Multi-armed Bandits Problem.</strong> Gao, Z., Han, Y., Ren, Z., &amp; Zhou, Z. (2019). [<a href="https://papers.nips.cc/paper/8341-batched-multi-armed-bandits-problem" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在本文中，我们研究了分批设置的多武装匪徒问题，在这种情况下，所采用的策略必须将数据分成少量批处理。 虽然\ cite {perchet2016batched}已完全体现了对两臂随机土匪的最小最大遗憾，但对于多臂案件来说，武器数量对后悔的影响仍未解决。 此外，是否自适应选择批次大小是否有助于减少后悔的问题也仍未得到探讨。 在本文中，我们提出了BaSE（分批逐次淘汰）策略，以实现批处理多臂匪的速率最优后悔（在对数因子之内），即使以自适应方式确定批大小，其下界也匹配。</li></ul></li><li><p><strong>Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning.</strong> Van Seijen, H., Fatemi, M., &amp; Tavakoli, A. (2019). [<a href="https://papers.nips.cc/paper/9560-using-a-logarithmic-mapping-to-enable-lower-discount-factors-in-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>为了更好地理解折现因子影响强化学习中优化过程的不同方式，我们设计了一组实验来单独研究每种效果。 我们的分析表明，普遍的看法是，低折扣因素的不良表现是由（过小的）行动间隙引起的。 我们提出了另一种假设，该假设将跨越状态空间的行动差距的大小差异确定为主要原因。 然后，我们引入一种新方法，该方法通过将值估计值映射到对数空间来实现更均一的动作间隙。 我们在标准假设下证明了该方法的收敛性，并通过经验证明了它确实为近似的强化学习方法提供了较低的折现因子。 反过来，这可以解决一类强化学习问题，这些问题是传统方法难以解决的。</li></ul></li></ul><h2 id="Poster"><a href="#Poster" class="headerlink" title="Poster"></a>Poster</h2><h3 id="reward分解"><a href="#reward分解" class="headerlink" title="reward分解"></a>reward分解</h3><ul><li><p><strong>Distributional Reward Decomposition for Reinforcement Learning.</strong> Lin, Z., Zhao, L., Yang, D., Qin, T., Yang, G., &amp; Liu, Y. (2019). [<a href="https://papers.nips.cc/paper/8852-distributional-reward-decomposition-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>许多强化学习（RL）任务具有特定的属性，可以利用这些属性来修改现有的RL算法以适应这些任务并进一步提高性能，此类属性的一般类别是多重奖励channel。在那些环境中，可以将全部奖励分解为从不同channel获得的子奖励。现有的关于奖励分解的工作或者需要环境先验知识才能分解全部奖励，或者在没有先验知识的情况下分解奖励但会降低算法性能。在本文中，我们提出了Distributional Reward Decomposition for Reinforcement Learning（DRDRL），这是一种新颖的奖励分解算法，可以捕获分布式设置下的多个奖励channel结构。根据经验，我们的方法捕获了多通道结构并发现了有意义的奖励分解，而无需任何先验知识。因此，在具有多个奖励渠道的环境中，我们的代理比现有方法具有更好的性能。</li></ul></li><li><p><strong>Learning Reward Machines for Partially Observable Reinforcement Learning.</strong> Toro Icarte, R., Waldie, E., Klassen, T. Q., Valenzano, R., Castro, M. P., &amp; McIlraith, S. A. (2019). [<a href="https://papers.nips.cc/paper/9685-learning-reward-machines-for-partially-observable-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>最初用于指定强化学习（RL）中的问题的奖励机（RMs）提供了一种基于自动机的奖励函数的结构化表示，该表示使智能体可以将问题分解为子问题，可以使用off-policy学习有效地学习这些问题。这里，我们表明RMs可以从经验中学习，而不是由用户指定，并且所产生的问题分解可以用于有效解决部分可观察RL问题。我们将学习RMs的任务定位为离散的优化问题，其目标是找到将问题分解为一组子问题的RM，以使其最优无记忆策略的组合成为原始问题的最优策略。 我们在三个部分可观察domain上展示该方法的有效性（显着胜于A3C，PPO和ACER），并讨论其优势，局限性和广阔的潜力。</li></ul></li><li><p><strong>LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning.</strong> Du, Y., Han, L., Fang, M., Dai, T., Liu, J., &amp; Tao, D. (2019). [<a href="https://papers.nips.cc/paper/8691-liir-learning-individual-intrinsic-reward-in-multi-agent-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>合作分布式多智能体强化学习（MARL）的一项巨大挑战是，当仅获得团队奖励时，每个个体都会产生多样化的行为。先前的研究在奖励shaping或设计可以区别对待智能体的中心化critic方面付出了很多努力。在本文中，我们建议合并两个方向，并向每个智能体学习内在的奖励函数，该函数在每个时间步均会刺激智能体。具体而言，特定智能体的内在奖励将涉及为智能体计算不同的智能体critic，以指导其个体政策的更新。同时，将对参数化的内在奖励函数进行更新，以最大程度地提高环境中的预期累积团队奖励，从而使目标与原始MARL问题相符。所提出的方法被称为MARL中的learning individual intrinsic reward（LIIR）。我们将LIIR与星际争霸II战斗游戏中的许多最先进的MARL方法进行了比较。结果证明了LIIR的有效性，并且我们证明LIIR可以在每个时间步长上为每个个体分配有洞察力的内在奖励。</li></ul></li><li><p><strong>RUDDER: Return Decomposition for Delayed Rewards.</strong> Arjona-Medina, J. A., Gillhofer, M., Widrich, M., Unterthiner, T., Brandstetter, J., &amp; Hochreiter, S. (2019). [<a href="https://papers.nips.cc/paper/9509-rudder-return-decomposition-for-delayed-rewards" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出了RUDDER，这是一种新颖的强化学习方法，用于有限马尔科夫决策过程（MDP）中的延迟奖励。在MDP中，Q值等于预期的即时奖励加上预期的未来奖励。后者与时差（TD）学习中的偏差问题以及蒙特卡洛（MC）学习中的高方差问题有关。当奖励延迟时，这两个问题都更加严重。 RUDDER旨在使预期的未来回报为零，从而简化了Q值估算，以计算即时奖励的均值。我们提出以下两个新概念，以将预期的未来奖励推向零。（i）奖励重新分配，它导致具有与最优策略相同的等价收益决策过程，并且在最优时，预期的未来回报为零。（ii）通过贡献分析进行收益分解，将强化学习任务转换为深度学习擅长的回归任务。在具有延迟奖励的人工任务上，RUDDER比MC快得多，并且比蒙特卡洛树搜索（MCTS），TD（λ）和奖励shaping方法快得多。在Atari游戏中，位于Proximal Policy Optimization（PPO）baseline之上的RUDDER可以提高得分，这在具有延迟奖励的游戏中最为突出。视频链接如下<a href="https://goo.gl/EQerZV" target="_blank" rel="noopener">https://goo.gl/EQerZV</a></li></ul></li></ul><h3 id="model-based方法"><a href="#model-based方法" class="headerlink" title="model-based方法"></a>model-based方法</h3><ul><li><strong>Learning to Predict Without Looking Ahead: World Models Without Forward Prediction.</strong> Freeman, C. D., Metz, L., &amp; Google Brain, D. H. (2019). [<a href="https://papers.nips.cc/paper/8778-learning-to-predict-without-looking-ahead-world-models-without-forward-prediction" target="_blank" rel="noopener">论文链接</a>]<ul><li>许多基于模型的强化学习涉及学习智能体世界的模型，并训练智能体以利用该模型更有效地执行任务。尽管这些模型对智能体非常有用，但我们知道的每个自然存在的模型（例如大脑）都是由竞争性生存进化压力产生的副产品，而不是通过梯度下降最小化有监督的前向预测loss而产生的。有用的模型可能来自混乱而缓慢的演化优化过程，这表明前向预测建模可以作为在适当情况下优化的副作用而出现。至关重要的是，此优化过程不必明确是前向预测loss。在这项工作中，我们对传统的强化学习进行了改进，我们将其称为observational dropout，限制了智能体在每个时间步观察真实环境的能力。这样做，我们可以强迫智能体学习一种世界模型，以填补强化学习过程中的观察gap。我们表明，这样产生的世界模型虽然未经过显式训练以预测未来，但可以帮助智能体学习在其环境中表现良好所需的关键技能。我们的结果视频可在<a href="https://learningtopredict.github.io/" target="_blank" rel="noopener">https://learningtopredict.github.io/</a>获得</li></ul></li></ul><h3 id="imitation-learning"><a href="#imitation-learning" class="headerlink" title="imitation learning"></a>imitation learning</h3><ul><li><strong>Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller.</strong> Sharma, P., Pathak, D., &amp; Gupta, A. (2019). [<a href="https://papers.nips.cc/paper/8528-third-person-visual-imitation-learning-via-decoupled-hierarchical-controller" target="_blank" rel="noopener">论文链接</a>]<ul><li>我们研究了一种通用的设置，用于从演示中学习，以构建一个智能体，该智能体可以通过仅从第三人称视角观看人类演示的单个视频来在看不见的场景中操纵新对象。为了实现此目标，我们的智能体不仅应学会了解所展示的第三人称视频在上下文中的意图，而且应在其环境配置中执行预期的任务。我们的主要见解是在学习过程中明确地实施这种结构，通过将要达成的目标（预期任务）与如何达到目标（控制器）分离开来。我们提出了一种分层设置，其中高级模块学习以第三人称视频演示为条件的一系列第一人称子目标，而底层控制器则预测实现这些子目标的动作。我们的智能体根据原始图像观察结果进行操作，而无需访问完整的状态信息。我们在使用Baxter的真实机器人平台上展示结果，完成将对象倒入以及放入盒子中的操作任务。 可在<a href="https://pathak22.github.io/hierarchical-imitation/" target="_blank" rel="noopener">https://pathak22.github.io/hierarchical-imitation/</a>上找到项目视频。</li></ul></li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li><p><strong>Control What You Can: Intrinsically Motivated Task-Planning Agent.</strong> Blaes, S., Vlastelica, M., Poganči´c, P., Zhu, J.-J., &amp; Martius, G. (2019). [<a href="https://s-bl.github.io/cwyc/" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出了一种新型的内在驱动力智能体，通过优化学习过程来学习如何以样本有效的方式来控制环境，即通过尽可能少的环境交互来控制环境。它使用surprise-based动机来学习哪些东西是可以控制的，如何分配时间和注意力以及对象之间的关系。与内在驱动力，非分层以及最新的分层baseline相比，我们方法的有效性在合成和机器人操作环境中得到了证明，可显着提高性能，并减少样本的复杂性。简而言之，我们的工作结合了多种任务级计划智能体结构（在任务图上回溯搜索，概率路线图，搜索工作分配）以及从头开始学习的内在驱动力。</li></ul></li><li><p><strong>Generalization of Reinforcement Learners with Working and Episodic Memory.</strong> Fortunato, M., Tan, M., Faulkner, R., Hansen, S., Adrià, ?, Badia, P., … Deepmind, C. B. (2019). [<a href="https://papers.nips.cc/paper/9411-generalization-of-reinforcement-learners-with-working-and-episodic-memory" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>记忆是智能的重要方面，并在许多深度强化学习模型中发挥作用。但是，在了解特定的记忆系统何时比其他系统更有用以及它们的泛化性如何方面进展甚微。该领域还没有一种普遍的，一致且严格的方法来评估智能体在保留数据上的性能。在本文中，我们旨在开发一种全面的方法来测试智能体中不同类型的记忆，并评估智能体可以如何将其在训练中学到的东西应用到与训练集在我们建议的相关维度上不同的集合上，用于评估特定于记忆的泛化。为此，我们首先构建了一组多样化的记忆任务，这些任务使我们能够评估多个维度上的测试时泛化。其次，我们在结合了多个记忆系统的智能体架构上开发并执行多种消融，观察其baseline模型，并针对任务套件调查其性能。</li></ul></li><li><p><strong>Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity.</strong> Pathak, D., Lu, C., Darrell, T., Isola, P., &amp; Efros, A. A. (2019). [<a href="https://pathak22.github.io/modular-assemblies/" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>当前的感觉运动学习方法通​​常从他们学习控制的现有复杂主体（例如，机械臂）开始。相比之下，本文研究了一种模块化的协同进化策略：一组原语智能体学习动态地自我组装成复合机体，同时学习协调其行为以控制这些机体。每个原语智能体都包括一个肢体，一端附有一个电机。肢体可以选择连接起来以形成集体。当一个肢体启动链接动作并且附近有另一个肢体时，后者与“父母”肢体的电机磁连接。这形成了新的单体智能体，可以进一步与其他智能体链接。这样，可以通过策略控制复杂的形态，这种策略的架构与形态显式对应。我们评估这些动态和模块化智能体在模拟环境中的性能。与静态基线和单体baseline相比，我们展示了其对环境以及智能体结构的测试时变化有更好的泛化性。补充材料中提供了项目视频和源代码。</li></ul></li><li><p><strong>A Composable Specification Language for Reinforcement Learning Tasks.</strong> Jothimurugan, K., Alur, R., &amp; Bastani, O. (2019). [<a href="https://papers.nips.cc/paper/9462-a-composable-specification-language-for-reinforcement-learning-tasks" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>强化学习是一种学习机器人任务控制策略的有前途的方法。 然而，指定复杂的任务（例如，具有多个目标和安全约束）可能是具有挑战性的，因为用户必须设计对整个任务进行编码的奖励函数。 此外，用户经常需要手动调整奖励以确保学习算法的收敛。 我们提出了一种用于指定复杂控制任务的语言，以及一种将我们的语言规范编译为奖励功能并自动执行奖励整形的算法。 我们在一个名为SPECTRL的工具中实现了我们的方法，并表明它优于几个最新的基准。</li></ul></li><li><p><strong>A Family of Robust Stochastic Operators for Reinforcement Learning.</strong> Lu, Y., Squillante, M. S., &amp; Wah Wu, C. (2019). [<a href="https://papers.nips.cc/paper/9696-a-family-of-robust-stochastic-operators-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们考虑了一个新的随机算子家族，用于强化学习，其目的是减轻负面影响，并变得对近似误差或估计误差更加稳健。 建立了各种理论结果，包括表明我们的操作员家族在随机意义上保留了最优性并增加了行动差距。 我们的经验结果说明了我们强大的随机算子的强大优势，其性能明显优于传统的Bellman算子和最近提出的算子。</li></ul></li><li><p><strong>A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation.</strong> Yang, R., Sun, X., &amp; Narasimhan, K. (2019). Retrieved from <a href="https://github.com/RunzheYang/MORL" target="_blank" rel="noopener">https://github.com/RunzheYang/MORL</a></p><ul><li>我们引入了一种具有线性偏好的多目标强化学习（MORL）的新算法，其目标是能够对新任务进行少量调整。 在MORL中，目的是学习多个竞争目标的策略，这些目标的相对重要性（首选项）对于代理人是未知的。 虽然这减轻了对标量奖励设计的依赖，但是策略的预期收益会随着偏好的变化而发生显着变化，这使得学习单一模型以在不同的偏好条件下产生最优策略具有挑战性。 我们提出Bellman方程的广义形式，以学习在所有可能的偏好范围内获得最优政策的单个参数表示。 在初始学习阶段之后，我们的代理可以在任何给定的首选项下执行最佳策略，或者自动根据很少的样本来推断基本的首选项。 在四个不同领域的实验证明了我们方法的有效性。</li></ul></li><li><p><strong>A Geometric Perspective on Optimal Representations for Reinforcement Learning.</strong> Bellemare, M. G., Dabney, W., Dadashi, R., Taiga, A. A., Castro, P. S., Le Roux, N., … Lyle, C. (2019). [<a href="https://papers.nips.cc/paper/8687-a-geometric-perspective-on-optimal-representations-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出了一种基于价值函数空间的几何特性的强化学习中表示学习的新视角。 从那里，我们提供了关于价值函数在强化学习中作为辅助任务的有用性的正式证据。 我们的公式考虑对表示进行调整，以使给定环境中所有固定政策的价值函数的（线性）近似最小化。 我们表明，这种优化简化为对一类特殊的价值函数（称为对抗性价值函数（AVF））做出准确的预测。 我们证明使用价值函数作为辅助任务对应于我们公式的预期误差松弛，AVF是自然的候选者，并确定与原型价值函数的密切关系（Mahadevan，2005）。 我们在四房域的一系列实验中重点介绍了AVF的特性及其作为辅助任务的有用性。</li></ul></li><li><p><strong>A Kernel Loss for Solving the Bellman Equation.</strong> Feng, Y., Li, L., Research, G., &amp; Liu, Q. (2019). [<a href="https://papers.nips.cc/paper/9679-a-kernel-loss-for-solving-the-bellman-equation" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>值函数学习在许多最新的强化学习算法中发挥着核心作用。 许多流行的算法（例如Q学习）并没有优化任何目标函数，而是Bellman运算符某些变体的定点迭代，不一定是收缩。 结果，如在实践中所观察到的，它们可能容易失去收敛保证。 在本文中，我们提出了一种新颖的损失函数，可以使用基于梯度的标准方法来保证收敛性，从而对其进行优化。 关键优势在于，可以使用采样的跃迁轻松估算其梯度，而无需像残留梯度之类的现有算法需要双重采样。 我们的方法可以使用策略内数据或策略外数据与通用功能类（例如神经网络）结合使用，并且在几种基准中被证明可以可靠，有效地工作，其中包括已知标准算法存在差异的经典问题。</li></ul></li><li><p><strong>A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning.</strong> Garcia, F. M., &amp; Thomas, P. S. (2019). Retrieved from <a href="https://github.com/fmaxgarcia/Meta-MDP" target="_blank" rel="noopener">https://github.com/fmaxgarcia/Meta-MDP</a></p><ul><li>在本文中，我们考虑一个问题，即负责解决一系列强化学习问题（一系列马尔科夫决策过程）的强化学习代理如何利用生命周期早期获得的知识来提高其解决新问题的能力。 我们认为，以前有类似问题的经验可以为代理商提供有关在面对新的但相关的问题时应如何探索的信息。 我们表明，寻找最佳探索策略的过程可以被公式化为强化学习问题本身，并证明这种策略可以利用在相关问题的结构中发现的模式。 我们以实验结束，这些实验显示了使用我们提出的框架优化勘探策略的好处。</li></ul></li><li><p><strong>A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning.</strong> Li, X., Yang, W., &amp; Zhang, Z. (2019). [<a href="https://papers.nips.cc/paper/8828-a-regularized-approach-to-sparse-optimal-policy-in-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出并研究了常规Markov决策过程（MDP）的通用框架，该框架的目标是找到使预期折扣总奖励加政策正则项最大化的最优政策。 可以将现存的熵规范化MDP转换为我们的框架。 此外，在我们的框架下，许多正则化术语会带来多种模式和稀疏性，这在强化学习中可能很有用。 特别是，我们提出了诱导稀疏最优政策的充分和必要条件。 我们还对建议的正则化MDP进行了完整的数学分析，包括最优性条件，性能误差和稀疏控制。 我们提供了一种通用方法来设计正则化形式，并在复杂的环境设置中提出非政策参与者评论家算法。 我们根据经验分析最优策略的数值属性，并比较离散和连续环境中不同稀疏正则化形式的性能。</li></ul></li><li><p><strong>A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning.</strong> Carion Facebook, N., Lamsade, P., Paris Dauphine, U., Synnaeve Facebook, G., Lazaric Facebook, A., &amp; Usunier Facebook, N. (2019). [<a href="https://papers.nips.cc/paper/9024-a-structured-prediction-approach-for-generalization-in-cooperative-multi-agent-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>有效的协调对于解决多代理协作（MAC）问题至关重要。 尽管集中式强化学习方法可以最佳地解决小型MAC实例，但它们无法解决较大的问题，并且无法推广到与培训期间所看到的场景不同的场景。 在本文中，我们考虑了具有一些固有的局部性（例如地理邻近性）概念的MAC问题，从而使代理与任务之间的交互受到局部限制。 通过利用此属性，我们引入了一种新颖的结构化预测方法来将代理分配给任务。 在每个步骤中，通过解决集中式优化问题（推理过程）来获得分配，该问题的目标函数由学习的评分模型进行参数化。 我们提出了推理过程和评分模型的不同组合，它们能够代表日益复杂的协调模式。 可以在小问题实例上有效地学习生成的分配策略，并可以在具有更多代理和任务的问题中轻松重用所得到的分配策略（即零击概括）。 我们报告了有关玩具搜索和救援问题以及《星际争霸：巢穴之战》中几个目标选择场景的实验结果，在这些案例中，我们的模型在具有比代理商训练期间看到的代理和任务多5倍的实例和任务的实例上，大大优于基于规则的强大基准。</li></ul></li><li><p><strong>A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment.</strong> Leibfried, F., Pascual-Díaz, S., &amp; Grau-Moya, J. (2019). [<a href="https://papers.nips.cc/paper/9001-a-unified-bellman-optimality-principle-combining-reward-maximization-and-empowerment" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>授权是一种信息理论方法，可用于内在地激励学习代理。 它尝试通过鼓励访问具有大量可到达的下一个州的州来最大化代理对环境的控制。 研究表明，授权学习可以导致复杂的行为，而无需明确的奖励信号。 在本文中，我们研究了在外部奖励信号存在下授权的使用。 我们假设授权可以通过鼓励高度授权的国家来引导强化学习（RL）代理找到良好的早期行为解决方案。 我们提出了统一的Bellman最优性原则，以实现授权的奖励最大化。 我们授权的报酬最大化方法概括了Bellman的最优性原则以及最近对其进行的信息理论扩展。 我们证明了赋权价值的独特性，并表明了向最佳解决方案的融合。 然后，我们将这个想法应用到开发非政策参与者批评RL算法，并在高维连续机器人领域（MuJoCo）中进行验证。 与无模型的最新技术相比，我们的方法证明了改进的初始和竞争性最终性能。</li></ul></li><li><p><strong>Adaptive Auxiliary Task Weighting for Reinforcement Learning.</strong> Lin, X., Singh Baweja, H., Kantor, G., &amp; Held, D. (2019). [<a href="https://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>众所周知，强化学习的样本效率很低，因此无法将其应用于许多实际问题，尤其是在像图像这样的高维度观察中。 从其他辅助任务中转移知识是提高学习效率的强大工具。 但是，由于难以选择和组合不同的辅助任务，到目前为止，辅助任务的使用受到限制。 在这项工作中，我们提出了一种有原则的在线学习算法，该算法动态地组合了不同的辅助任务，以加快强化学习的训练速度。 我们的方法基于以下想法：辅助任务应提供渐变方向，从长远来看，这有助于减少主要任务的损失。 与以前的适应辅助任务权重的启发式方法相比，我们在各种环境中证明了我们的算法可以有效地组合各种不同的辅助任务并实现显着的加速。</li></ul></li><li><p><strong>Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates.</strong> Penedones, H., Deepmind, ⇤, Riquelme, C., Google, ⇤, Damien, B., Google, V., … Neu, B. G. (2019). [<a href="https://papers.nips.cc/paper/9359-adaptive-temporal-difference-learning-for-policy-evaluation-with-per-state-uncertainty-estimates" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们从一批轨迹数据中考虑了基于策略价值函数逼近的核心强化学习问题，并将重点放在时间差异（TD）学习和蒙特卡洛（MC）政策评估的各种问题上。 已知这两种方法可实现互补的偏差-方差权衡特性，而TD往往会实现较小的方差，但有可能实现较高的偏差。 在本文中，我们认为TD的较大偏差可能是局部逼近误差放大的结果。 我们通过提出一种在每种状态下在TD和MC之间自适应切换的算法来解决此问题，从而减轻错误的传播。 我们的方法基于检测到TD估计值偏差的学习置信区间。 我们在各种策略评估任务中证明了这种简单的自适应算法在事后观察中具有最佳方法的竞争能力，这表明学习的置信区间是一种强大的技术，可通过数据驱动的方式使策略评估适应使用TD或MC收益。</li></ul></li><li><p><strong>Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model.</strong> Zanette, A., Kochenderfer, M. J., &amp; Brunskill, E. (2019). [<a href="https://papers.nips.cc/paper/8800-almost-horizon-free-structure-aware-best-policy-identification-with-a-generative-model" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>本文关注的是在折扣马尔可夫决策过程（MDP）中计算\最优策略的问题，前提是我们可以通过生成模型访问奖励和转移函数。 我们提出了一种算法，该算法最初与MDP无关，但可以利用特定的MDP结构，以奖励和下一状态值函数的方差以及最佳操作值函数的差异表示，以减少所需的样本复杂性 找到一个好的政策，恰好突出了每个状态对对最终样本复杂度的贡献。 我们分析的一个关键特征是，它消除了次优操作的样本复杂性中的所有视域依赖性，除了价值函数的内在标度和恒定的加性项之外。</li></ul></li><li><p><strong>Better Exploration with Optimistic Actor-Critic.</strong> Ciosek, K., Vuong, Q., Loftin, R., &amp; Hofmann, K. (2019). [<a href="https://papers.nips.cc/paper/8455-better-exploration-with-optimistic-actor-critic" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>演员批评方法是一种无模型的强化学习方法，已成功应用于连续控制中的挑战性任务，通常可以实现最先进的性能。 但是，由于采样效率低，很难在现实世界中广泛采用这些方法。 我们在理论上和经验上都解决了这个问题。 从理论上讲，我们确定了两种现象，这些现象阻碍了诸如Soft Actor Critic之类的现有最新算法的有效探索。 首先，将贪婪的演员更新与批评家的悲观估计结合起来，可以避免主体不知道的行动，这种现象我们称为悲观的探索不足。 其次，当前算法是无方向的，从当前平均值开始以相反的概率对动作进行采样。 这很浪费，因为我们通常需要沿着某些方向采取的行动比其他方向多得多。 为了解决这两种现象，我们引入了一种新算法Optimistic Actor Critic，该算法近似估计状态作用值函数的上下置信度边界。 这使我们能够在不确定性的情况下应用乐观原则，以上限进行定向探索，同时仍然使用下限以避免高估。 我们在一些挑战性的连续控制任务中评估OAC，以实现最先进的样品效率。</li></ul></li><li><p><strong>Biases for Emergent Communication in Multi-agent Reinforcement Learning.</strong> Eccles, T., Bachrach, Y., Lever, G., &amp; Lazaridou, A. (2019). [<a href="https://papers.nips.cc/paper/9470-biases-for-emergent-communication-in-multi-agent-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们研究紧急交流的问题，其中出现语言是因为说话者和听众必须交流信息才能解决任务。 在时间扩展的强化学习领域，事实证明，如果没有集中的代理培训，很难学习这种交流，部分原因是联合勘探存在困难。 我们为积极的信号传递和积极的倾听引入归纳偏置，从而缓解了这一问题。 在一个简单的一步式环境中，我们演示了这些偏见如何缓解学习问题。 我们还将我们的方法应用于更扩展的环境，这表明具有这些归纳偏差的代理可以实现更好的性能，并分析由此产生的通信协议。</li></ul></li><li><p><strong>Budgeted Reinforcement Learning in Continuous State Space.</strong> Carrara, N., Leurent, E., Laroche, R., Urvoy, T., &amp; Pietquin, O. (2019). Retrieved from <a href="https://budgeted-rl.github.io/" target="_blank" rel="noopener">https://budgeted-rl.github.io/</a>.</p><ul><li>预算马尔可夫决策过程（BMDP）是马尔可夫决策过程对要求安全性约束的关键应用程序的扩展。 它依赖于在约束违例信号上以上限形式实施的风险概念，重要的是可以实时修改。 到目前为止，只有在具有已知动力学的有限状态空间的情况下才能求解BMDP。 这项工作将最新技术扩展到连续空间环境和未知的动力学。 我们表明，BMDP的解决方案是新颖的Budgeted Bellman最优性算子的不动点。 这一观察结果使我们能够引入深度强化学习算法的自然扩展，以解决大规模BMDP问题。 我们在两个模拟应用程序上验证了我们的方法：语音对话和自动驾驶。</li></ul></li><li><p><strong>Causal Confusion in Imitation Learning.</strong> De Haan, P., Jayaraman, D., &amp; Levine, S. (2019). [<a href="https://papers.nips.cc/paper/9343-causal-confusion-in-imitation-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>行为克隆通过训练判别模型来预测观察到的专家行为，从而将策略学习从监督学习减少为监督学习。 这种判别模型不是因果关系的：培训过程并不了解专家与环境之间相互作用的因果结构。 我们指出，由于模仿学习中的分布变化，忽略因果关系尤其有害。 特别是，这会导致违反直觉的“因果识别错误”现象：访问更多信息可能会导致性能下降。 我们调查了此问题的产生方式，并提出了一种解决方案，可通过有针对性的干预措施（环境互动或专家查询）来解决，以确定正确的因果模型。 我们证明了因果错误识别会在几个基准控制域以及实际驾驶设置中发生，并针对DAgger和其他基准线和烧蚀验证我们的解决方案。</li></ul></li><li><p><strong>Constrained Reinforcement Learning Has Zero Duality Gap.</strong> Paternain, S., O Chamon, L. F., Calvo-Fullana, M., &amp; Ribeiro, A. (2019). [<a href="https://papers.nips.cc/paper/8973-constrained-reinforcement-learning-has-zero-duality-gap" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>自治代理必须经常处理相互矛盾的要求，例如使用最少的时间/精力完成任务，学习多个任务或与多个对手打交道。 在强化学习（RL）的背景下，这些问题可以通过（i）设计同时描述所有需求的奖励函数或（ii）组合分别对它们进行编码的模块化值函数来解决。 尽管有效，但是这些方法具有严重的缺点。 设计平衡不同目标的良好奖励功能非常具有挑战性，尤其是随着目标数量的增长。 此外，目标之间的隐式干扰可能会导致绩效陷入停滞，因为它们在争夺资源时尤其是在进行政策培训时。 类似地，考虑到参数值对整体政策的影响并不直接，选择用于组合值函数的参数至少与设计全面的奖励一样困难。 通常通过将冲突的需求公式化为受约束的RL问题来解决后者，并使用Primal-Dual方法解决。 由于问题不是凸的，因此通常不能保证这些算法收敛到最优解。 通过确定该问题尽管不具有凸性，但仍具有零对偶间隙，即可以在凸的对偶域中精确地解决该问题，因此为这些方法提供了理论支持。 最后，我们证明了如果通过良好的参数化描述（例如，神经网络）描述了该策略，则该结果基本上成立，并将该结果与文献中存在的原始对偶算法联系起来，并建立了对最优解的收敛性。</li></ul></li><li><p><strong>Convergent Policy Optimization for Safe Reinforcement Learning.</strong> Yu, M., Yang, Z., Kolar, M., &amp; Wang, Z. (2019). [<a href="https://papers.nips.cc/paper/8576-convergent-policy-optimization-for-safe-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们研究了具有非线性函数逼近的安全加固学习问题，其中将策略优化公式化为约束优化问题，其目标和约束均为非凸函数。 对于此类问题，我们通过用从策略梯度估计器获得的凸二次函数来局部替换非凸函数，从而构造了一系列替代凸约束优化问题。 我们证明了这些替代问题的解决方案收敛到原始非凸问题的固定点。 此外，为了扩展我们的理论结果，我们将我们的算法应用于具有安全约束的最优控制和多主体强化学习的示例。</li></ul></li><li><p><strong>Correlation Priors for Reinforcement Learning.</strong> Alt, B., Adrian, ⇤, Šoši´c, Š., &amp; Koeppl, H. (2019). Retrieved from <a href="https://git.rwth-aachen.de/bcs/correlation_priors_for_rl" target="_blank" rel="noopener">https://git.rwth-aachen.de/bcs/correlation_priors_for_rl</a></p><ul><li>许多决策问题自然会展现出继承自底层环境特征的明显结构。 例如，在马尔可夫决策过程模型中，两个不同的状态可以具有固有相关的语义或对类似于物理状态配置的编码。 这通常意味着状态之间局部相关的过渡动态。 为了在这样的环境中完成某些任务，操作代理通常需要执行一系列在时间和空间上相关的动作。 尽管存在各种各样的方法来捕获连续状态作用域中的这些相关性，但是缺少用于离散环境的原则性解决方案。 在这项工作中，我们提出了一种基于Pólya-Gamma增强的贝叶斯学习框架，该框架在这种情况下可以进行类似的推理。 我们演示了许多与决策相关的常见问题的框架，例如模仿学习，子目标提取，系统识别和贝叶斯强化学习。 通过对这些问题的潜在相关结构进行显式建模，与相关不可知模型相比，即使在规模较小的数据集上进行训练时，所提出的方法也能提供优于预测的性能。</li></ul></li><li><p><strong>Curriculum-guided Hindsight Experience Replay.</strong> Fang, M., Zhou, T., Du, Y., Han, L., Zhang, Z., Robotics, T., &amp; Allen, P. G. (2019). Retrieved from <a href="https://github.com/mengf1/CHER" target="_blank" rel="noopener">https://github.com/mengf1/CHER</a>.</p><ul><li>在非政策性深度强化学习中，通常很难收集到足够的成功经验以及稀疏的奖励以供学习。 后验经验重播（HER）使代理能够通过将失败经验的实现状态视为伪目标来从失败中学习。 但是，并非所有失败的经历对于不同的学习阶段都同样有用，因此重播所有失败或统一样本的效率不高。 在本文中，我们建议（1）根据与真实目标的接近程度和对各种伪目标的探索好奇心，自适应地选择失败的重播经验，以及2）逐渐改变目标临近度和多样性的比例， 选择标准中的好奇心：我们采用类似于人的学习策略，在早期阶段会提高好奇心，而在后来又转向更大目标。 这种以目标和好奇心为导向的课程学习’’引出了课程指导的HER（CHER）’’，该课程在学习过程中通过事后视察经验选择来自适应，动态地控制探索与开发之间的权衡。 我们证明，在具有挑战性的机器人环境中，CHER可以改善现有技术。</li></ul></li><li><p><strong>DAC: The Double Actor-Critic Architecture for Learning Options.</strong> Zhang, S., &amp; Whiteson, S. (2019). [<a href="https://papers.nips.cc/paper/8475-dac-the-double-actor-critic-architecture-for-learning-options" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们将期权框架重新制定为两个并行的增强型MDP。 在这种新颖的表述下，所有现成的策略优化算法都可用于学习期权内策略，期权终止条件以及期权的主策略。 我们在每个增强的MDP上应用一个actor-critic算法，从而产生Double Actor-Critic（DAC）体系结构。 此外，我们表明，当将状态值函数用作批注者时，一个批注者可以用另一种来表示，因此仅需要一个批注者。 我们对具有挑战性的机器人仿真任务进行了实证研究。 在转移学习设置中，DAC的性能优于无层次的对应方法和以前的基于梯度的选项学习算法。</li></ul></li><li><p><strong>Discovery of Useful Questions as Auxiliary Tasks.</strong> Veeriah, V., Hessel, M., Xu, Z., Lewis, R., Rajendran, J., Oh, J., … Singh, S. (2016). [<a href="https://papers.nips.cc/paper/9129-discovery-of-useful-questions-as-auxiliary-tasks" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>可以说，聪明的代理人应该能够发现自己的问题，以便在为他们学习答案时学习出乎意料的有用知识和技能。 这与许多机器学习的重点不同，代理人学习的是外部定义的问题的答案。 我们提出一种用于强化学习（RL）代理的新颖方法，以发现公式化为一般价值函数或GVF（一种相当丰富的知识表示形式）的问题。 具体来说，我们的方法使用非近视元梯度来学习GVF问题，以便学习对它们的答案作为辅助任务，可以为RL代理面临的主要任务提供有用的表示。 我们证明，基于发现的GVF的辅助任务本身足以构建支持主要任务学习的表示形式，并且它们比文献中流行的手工设计辅助任务要好。 此外，我们展示了在Atari2600电子游戏的背景下，与主任务一起元学习的辅助任务如何提高演员批评代理的数据效率。</li></ul></li><li><p><strong>Distributional Policy Optimization: An Alternative Approach for Continuous Control.</strong> Tessler, C., Tennenholtz, G., &amp; Mannor, S. (2019). [<a href="https://papers.nips.cc/paper/8416-distributional-policy-optimization-an-alternative-approach-for-continuous-control" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们在连续控制中基于策略梯度的方法中确定了一个基本问题。 由于策略梯度方法需要代理的基础概率分布，因此将策略表示限制为参数分布类。 我们表明，对此类集合进行优化会导致动作空间中的局部运动，从而收敛到次优解决方案。 我们建议一种新颖的分布框架，该框架能够表示连续动作空间上的任意分布函数。 使用此框架，我们构建了一种生成方案，并使用非策略性行为者批评范式进行了训练，我们称其为“生成行为者评论家”（GAC）。 与策略梯度方法相比，GAC不需要了解潜在的概率分布，从而克服了这些限制。 实证评估表明，我们的方法具有可比性，并且经常在连续领域中超过当前的最新基准。</li></ul></li><li><p><strong>Divergence-Augmented Policy Optimization.</strong> Wang, Q., Li, Y., Xiong, J., &amp; Zhang, T. (2019). [<a href="https://papers.nips.cc/paper/8842-divergence-augmented-policy-optimization" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在深度强化学习中，策略优化方法需要处理诸如函数逼近和非策略数据重用之类的问题。 标准策略梯度方法不能很好地处理非策略数据，从而导致过早收敛和不稳定。 本文介绍了一种在重用非策略数据时稳定策略优化的方法。 想法是在生成数据的行为策略与当前策略之间包括Bregman分歧，以确保使用策略外的数据进行小而安全的策略更新。  Bregman差异是在两个策略的状态分布之间计算的，而不是仅根据动作概率来计算，从而导致差异增大的公式。  Atari游戏的经验实验表明，在数据稀缺的情况下，需要重用非策略数据，我们的方法可以比其他最新的深度强化学习算法获得更好的性能。</li></ul></li><li><p><strong>Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control.</strong> Zhang, S. Q., Zhang, Q., &amp; Lin, J. (2019). Retrieved from <a href="https://github.com/saizhang0218/VBC" target="_blank" rel="noopener">https://github.com/saizhang0218/VBC</a>.</p><ul><li></li></ul></li><li><p><strong>Experience Replay for Continual Learning.</strong> Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., &amp; Wayne, G. (2019). [<a href="https://papers.nips.cc/paper/8327-experience-replay-for-continual-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>与复杂世界互动需要不断学习，其中任务和数据分布会随着时间而变化。 持续学习系统应同时展示可塑性（获取新知识）和稳定性（保留旧知识）。 灾难性的遗忘是稳定性的失败，其中新的经验会覆盖以前的经验。 在大脑中，人们普遍认为重播过去的经验可以减少遗忘，但是在深度强化学习中，遗忘作为解决遗忘的一种方法在很大程度上被忽视了。 在这里，我们介绍CLEAR，这是一种基于重播的方法，可以大大减少多任务强化学习中的灾难性遗忘。  CLEAR利用脱离重播的非策略学习和行为克隆来增强稳定性，以及基于策略的学习以保持可塑性。 我们显示，CLEAR在缓解遗忘方面比最新的深度学习技术要好，尽管复杂程度大大降低，并且不需要了解所学习的各个任务。</li></ul></li><li><p><strong>Explicit Explore-Exploit Algorithms in Continuous State Spaces.</strong> Henaff, M. (2019). [<a href="https://papers.nips.cc/paper/9135-explicit-explore-exploit-algorithms-in-continuous-state-spaces" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出了一种新的基于模型的强化学习（RL）算法，该算法包括显式探索和开发阶段，适用于大型或无限状态空间。 该算法维护了一组与当前经验相符的动力学模型，并通过找到在其状态预测之间引起高度不一致的策略进行了探索。 然后，它使用在探索过程中收集的完善的模型集或经验进行开发。 我们表明，在可实现性和最佳计划假设下，我们的算法可证明找到了具有多个样本的近似最优策略，该样本在结构复杂性测度中是多项式的，在某些自然环境中我们发现该策略较低。 然后，我们使用神经网络进行实际逼近，并在实践中证明其性能和样品效率。</li></ul></li><li><p><strong>Explicit Planning for Efficient Exploration in Reinforcement Learning.</strong> Zhang, L., Tang, K., &amp; Yao, X. (2019). [<a href="https://papers.nips.cc/paper/8967-explicit-planning-for-efficient-exploration-in-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>有效的探索对于在强化学习中取得良好的表现至关重要。 尽管从理论上有希望，但现有的系统探索策略（R-MAX，MBIE，UCRL等）本质上是遵循某些预定义启发式方法的贪婪策略。 当启发式方法与Markov决策过程（MDP）的动态性不完全匹配时，在经过已探索状态的过程中会浪费大量时间，从而降低了整体效率。 我们认为明确的勘探计划可以帮助缓解这一问题，并提出了勘探成本价值迭代（VIEC）算法，该算法通过求解增强型MDP来计算最佳勘探方案。 然后，我们对一些流行策略的勘探行为进行详细分析，显示这些策略如何失败并花费O（n ^ 2 md）或O（n ^ 2 m + nmd）步骤来收集一些塔状的足够数据 MDP虽然可以通过VIEC获得最佳的探索方案，但只需要O（nmd），其中n，m是状态和动作的数量，d是数据需求。 该分析不仅指出了现有基于启发式策略的弱点，而且还暗示了在明确的勘探计划中的巨大潜力。</li></ul></li><li><p><strong>Exploration via Hindsight Goal Generation.</strong> Ren, Z., Dong, K., Zhou, Y., Liu, Q., &amp; Peng, J. (2019). [<a href="https://papers.nips.cc/paper/9502-exploration-via-hindsight-goal-generation" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>面向目标的强化学习最近已成为机器人操纵任务的实用框架，在该框架中，要求代理程序达到由状态空间上的功能定义的某个目标。 然而，这种奖励定义的稀疏性使得传统的强化学习算法非常低效。  Hindsight Experience Replay（HER）是最近的一项进步，它极大地提高了样品效率，并大大提高了此类问题的实用性。 它通过以一种简单的启发式方式构造假想目标来利用以前的重放，就像隐式课程一样，以减轻稀疏奖励信号的挑战。 在本文中，我们介绍了Hindsight目标生成（HGG），这是一种新颖的算法框架，可生成有价值的事后观察目标，这些目标很容易使代理在短期内实现，并且也有可能指导代理长期实现实际目标 术语。 我们已经在许多机器人操纵任务上广泛评估了目标生成算法，并在采样效率方面证明了与原始HER相比的实质性改进。</li></ul></li><li><p><strong>Fast Efficient Hyperparameter Tuning for Policy Gradient Methods.</strong> Paul, S., Kurin, V., &amp; Whiteson, S. (2019). [<a href="https://papers.nips.cc/paper/8710-fast-efficient-hyperparameter-tuning-for-policy-gradient-methods" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>策略渐变方法的性能对必须针对任何新应用程序进行调整的超参数设置敏感。 用于调整超参数的广泛使用的网格搜索方法效率低下，并且计算量大。 学习基于超参数而不是固定设置的最佳计划的基于人口的训练等更高级的方法可以产生更好的结果，但是样本效率低下，计算量大。 在本文中，我们提出了动态超参数优化（HOOF），这是一种无梯度算法，只需要进行一次训练即可自动适应通过梯度直接影响策略更新的超参数。 主要思想是使用通过策略梯度方法采样的现有轨迹来优化单步改进目标，从而产生易于实现的示例和计算有效的算法。 我们在多个领域和算法上的实验结果表明，使用HOOF来学习这些超参数进度表可以更快地学习并提高性能。</li></ul></li><li><p><strong>Finding Friend and Foe in Multi-Agent Games.</strong> Serrino, J., Kleiman-Weiner, M., Harvard, ⇤, Parkes, D. C., &amp; Tenenbaum, J. B. (2019). Retrieved from <a href="https://github.com/Detry322/DeepRole" target="_blank" rel="noopener">https://github.com/Detry322/DeepRole</a>.</p><ul><li></li></ul></li><li><p><strong>Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning.</strong> Gupta ECE, H., Srikant ECE, R., &amp; Ying, L. (2019). [<a href="https://papers.nips.cc/paper/8718-finite-time-performance-bounds-and-adaptive-learning-rate-selection-for-two-time-scale-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们研究了两种时标线性随机逼近算法，可用于对诸如GTD，GTD2和TDC等著名的强化学习算法进行建模。 对于学习率固定的情况，我们给出了有限时间的性能范围。 获得这些边界的关键思想是对线性微分方程使用奇异摄动理论的Lyapunov函数。 我们使用边界设计了一种自适应学习速率方案，该方案在我们的实验中大大提高了已知最优多项式衰减规则的收敛速率，并且可用于潜在地提高任何其他在学习前更改学习速率的时间表的性能。 确定的时刻。</li></ul></li><li><p><strong>Fully Parameterized Quantile Function for Distributional Reinforcement Learning.</strong> Yang, D., Zhao, L., Lin, Z., Qin, T., Bian, J., &amp; Liu, T. (2019). [<a href="https://papers.nips.cc/paper/8850-fully-parameterized-quantile-function-for-distributional-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>分布强化学习（RL）与传统RL的不同之处在于，它可以估计分布并获得Atari Games的最新性能，而不是期望总回报。 实际的分布式RL算法的关键挑战在于如何对估计的分布进行参数化处理，以便更好地逼近真实的连续分布。 现有的分布RL算法可对分布函数的概率侧或返回值侧进行参数化，而另一侧如C51，QR-DQN中那样统一固定，或如IQN中那样随机采样。 在本文中，我们提出了完全参数化的分位数功能，该功能可以对分布RL的分位数分数轴（即x轴）和值轴（即y轴）进行参数化。 我们的算法包含一个分数提案网络，该提议网络生成一个离散的分位数分数集合，以及一个分数值网络，该网络给出相应的分位数。 共同训练这两个网络以找到真实分布的最佳近似值。 在55个Atari Games上进行的实验表明，我们的算法明显优于现有的分布式RL算法，并为非分布式代理的Atari学习环境创造了新记录。</li></ul></li><li><p><strong>Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck.</strong> Igl, M., Ciosek, K., Research, M., Li, Y., Tschiatschek, S., Zhang, C., … Hofmann, K. (2019). [<a href="https://papers.nips.cc/paper/9546-generalization-in-reinforcement-learning-with-selective-noise-injection-and-information-bottleneck" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>策略能够推广到新环境的能力是RL代理广泛应用的关键。 防止座席政策过分适应有限的培训环境的一种有前途的方法是应用最初为监督学习而开发的正则化技术。 但是，监督学习和RL之间存在明显差异。 我们讨论了这些差异，并提出了对现有正则化技术的修改，以使其更好地适应RL。 特别是，我们专注于依靠将噪声注入学习函数的正则化技术，该函数包括一些最广泛使用的方法，例如Dropout和Batch Normalization。 为了使它们适应RL，我们提出了选择性噪声注入（SNI），它可以保持注入噪声的正则化效果，同时减轻其对梯度质量的不利影响。 此外，我们证明了信息瓶颈（IB）是特别适合RL的正则化技术，因为它在早期训练RL代理时遇到的低数据情况下很有效。 将IB与SNI结合使用，我们的性能明显优于当前的最新结果，包括最近提出的通用基准Coinrun。</li></ul></li><li><p><strong>Generalized Off-Policy Actor-Critic.</strong> Zhang, S., Boehmer, W., &amp; Whiteson, S. (2019). [<a href="https://papers.nips.cc/paper/8474-generalized-off-policy-actor-critic" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出了一个新的目标，即反事实目标，在持续强化学习（RL）设置中统一了非政策性政策梯度算法的现有目标。 与通常使用的游览目标可能会误导目标策略在部署时的性能相比，我们的新目标可以更好地预测这种性能。 我们证明了广义非策略策略梯度定理来计算反事实目标的策略梯度，并使用强调方法从该策略梯度中获得无偏样本，从而产生了广义非策略参与者关键算法（Geoff-PAC）算法。 我们证明了Geoff-PAC在Mujoco机器人仿真任务中优于现有算法的优点，这是在深层RL基准测试中，强调算法的第一个经验性成功。</li></ul></li><li><p><strong>Goal-conditioned Imitation Learning.</strong> Ding, Y., Florensa, C., Phielipp, M., &amp; Abbeel, P. (2019). Retrieved from <a href="https://sites.google.com/view/goalconditioned-il/" target="_blank" rel="noopener">https://sites.google.com/view/goalconditioned-il/</a></p><ul><li>设计强化学习（RL）的奖励是具有挑战性的，因为它需要传达所需的任务，有效地进行优化并易于计算。 当将RL应用于机器人技术时，后者尤其成问题，因为要检测是否达到所需的配置可能需要大量的监督和检测。 此外，我们通常对能够实现多种配置感兴趣，因此每次设置不同的奖励可能都不切实际。 像Hindsight Experience Replay（HER）之类的方法最近显示出可以学习无需达成奖励即可实现许多目标的策略的承诺。 不幸的是，如果没有诸如沿轨迹重置点之类的技巧，HER可能需要很长时间才能发现如何到达状态空间的某些区域。 在这项工作中，我们研究了结合演示的各种方法，以大幅度加快向能够实现任何目标的策略的收敛速度，也超越了使用其他模仿学习算法训练的代理的性能。 此外，当只有没有专家动作的轨迹可用时，可以使用我们的方法，这可以利用运动学或第三人称演示。</li></ul></li><li><p><strong>Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning.</strong> Assran, M., Romoff, J., Ballas, N., Pineau, J., &amp; Rabbat, M. (2019). Retrieved from <a href="https://github.com/facebookresearch/gala" target="_blank" rel="noopener">https://github.com/facebookresearch/gala</a>.</p><ul><li></li></ul></li><li><p><strong>Guided Meta-Policy Search.</strong> Mendonca, R., Gupta, A., Kralev, R., Abbeel, P., Levine, S., &amp; Finn, C. (2019). [<a href="https://papers.nips.cc/paper/9160-guided-meta-policy-search" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>强化学习（RL）算法在复杂任务上显示出令人鼓舞的结果，但由于需要从头开始学习，因此经常需要不切实际的样本数量。  Meta-RL旨在通过利用先前任务的经验来应对这一挑战，从而更快地解决新任务。 但是，实际上，这些算法通常在\ emph {meta-training}过程中也需要大量的策略经验，这使得它们在许多问题中不切实际。 为此，我们建议以联合方式学习强化学习程序，其中，个别非政策学习者可以解决个别元训练任务，然后将这些解决方案合并为一个元学习者。 由于中央元学习者是通过模仿单个任务的解决方案来学习的，因此它可以适应标准的meta-RL问题设置，也可以容纳其中提供了部分或全部任务并带有示例演示的混合设置。 前者导致一种方法，该方法可以利用在元训练期间不需要大量策略数据的情况下就先前任务学习的策略，而后者在人员易于提供演示的情况下特别有用。 在许多连续控制的meta-RL问题中，我们证明了与以前的工作相比，meta-RL样本效率有了显着提高，并且可以通过视觉观察扩展到域。</li></ul></li><li><p><strong>Hierarchical Decision Making by Generating and Following Natural Language Instructions.</strong> Hu, H., Yarats, D., Gong, Q., Tian, Y., &amp; Lewis, M. (2019). Retrieved from www.minirts.net</p><ul><li></li></ul></li><li><p><strong>Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards.</strong> Li, S., Wang, R., Tang, M., &amp; Zhang, C. (2019). Retrieved from <a href="http://bit.ly/2JxA0eN" target="_blank" rel="noopener">http://bit.ly/2JxA0eN</a></p><ul><li></li></ul></li><li><p><strong>Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement.</strong> Yang, C., Ma, X., Huang, W., Sun, F., Liu, H., Huang, J., &amp; Gan, C. (2019). [<a href="https://papers.nips.cc/paper/8317-imitation-learning-from-observations-by-minimizing-inverse-dynamics-disagreement" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>本文研究了从观察中学习（LfO）的模仿学习，并可以访问仅州示范。 与涉及行动和国家监督的示威学习（LfD）相比，LfO在利用以前不适用的资源（例如视频）方面更为实用，但由于专家指导不完整，因此更具挑战性。 在本文中，我们从理论和实践角度研究LfO及其与LfD​​的区别。 我们首先证明，如果遵循GAIL的建模方法，则LfD和LfO之间的差距实际上在于模仿者和专家之间的逆动力学模型不一致。 更重要的是，该间隙的上限由负因果熵揭示，该因果熵可以以无模型的方式最小化。 我们称我们的方法为逆动态最小化（IDDM），它通过进一步缩小与LfD的差距来增强传统的LfO方法。 具有挑战性的基准的大量经验结果表明，与其他LfO同行相比，我们的方法获得了持续的改进。</li></ul></li><li><p><strong>Imitation-Projected Programmatic Reinforcement Learning.</strong> Verma, A., Le, H. M., Caltech, Y. Y., &amp; Chaudhuri, S. (2019). [<a href="https://papers.nips.cc/paper/9705-imitation-projected-programmatic-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li></li></ul></li><li><p><strong>Importance Resampling for Off-policy Prediction.</strong> Schlegel, M., Chung, W., Graves Huawei, D., Qian, J., &amp; White, M. (2019). [<a href="https://papers.nips.cc/paper/8456-importance-resampling-for-off-policy-prediction" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们研究程序强化学习的问题，其中策略以符号语言表示为短期程序。 程序性策略比神经性策略更易于解释，推广和接受形式验证。 但是，为此类政策设计严格的学习方法仍然是一个挑战。 我们针对这一挑战的方法-一种称为PROPEL的元算法-基于三种见解。 首先，我们将学习任务视为策略空间中的优化，对所需策略具有程序表示形式的约束进行模运算，并使用镜像下降的形式来解决此优化问题，该方法采取梯度步骤进入不受约束的策略空间，然后进行投影 进入受限的空间。 其次，我们将不受约束的策略空间视为混合了神经表示形式和程序表示形式，从而可以采用最新的深度策略梯度方法。 第三，我们通过模仿学习将投影步骤投射为程序综合，并针对该任务利用当代的组合方法。 我们介绍了PROPEL的理论收敛结果，并在三个连续的控制域中对方法进行了经验评估。 实验表明，PROPEL可以大大优于学习编程政策的最新方法。</li></ul></li><li><p><strong>Information-Theoretic Confidence Bounds for Reinforcement Learning.</strong> Lu, X., &amp; Roy, B. Van. (2019). [<a href="https://papers.nips.cc/paper/8516-information-theoretic-confidence-bounds-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们将信息理论概念整合到乐观算法和汤普森采样的设计和分析中。 通过在信息理论量和置信区间之间建立联系，我们获得了将代理的定期性能与其有关环境的信息收益相关联的结果，从而明确表征了勘探与开发的权衡。 由此产生的累积后悔界限取决于代理商在整个环境中的不确定性，并量化先验信息的价值。 我们展示了这种方法在几种环境中的适用性，包括线性强盗，表格MDP和分解MDP。 这些示例说明了用于增强学习算法的设计和分析的通用信息理论方法的潜力。</li></ul></li><li><p><strong>Interval timing in deep reinforcement learning agents.</strong> Deverett DeepMind, B., Faulkner DeepMind, R., Fortunato DeepMind, M., Wayne DeepMind, G., &amp; Leibo DeepMind, J. Z. (2019). [<a href="https://papers.nips.cc/paper/8894-interval-timing-in-deep-reinforcement-learning-agents" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>时间的测量对于智能行为至关重要。 我们知道动物和人工代理都可以成功地使用时间依赖性来选择动作。 在人工代理中，很少有工作直接解决（1）成功开发此功能所需的架构组件；（2）如何在代理的单元和动作中表示此计时功能；以及（3） 系统产生的行为收敛于类似于生物学的解决方案。 在这里，我们研究了深度强化学习代理中的间隔计时能力，这些技巧是在间隔再现范式上进行端到端训练的，间隔再现范式受时序机制实验文献的启发。 我们描述了由递归和前馈因子开发的策略的特征，这两种策略都可以使用独特的机制在时间复制上成功，其中一些机制与生物系统具有特定而有趣的相似性。 这些发现提高了我们对特工如何代表时间的理解，并突出了实验启发性方法表征特工能力的价值。</li></ul></li><li><p><strong>Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning.</strong> Kallus, N., &amp; Uehara, M. (2019). [<a href="https://papers.nips.cc/paper/8594-intrinsically-efficient-stable-and-bounded-off-policy-evaluation-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>上下文强盗和强化学习中的非政策评估（OPE）使得人们无需进行探索就可以评估新颖的决策政策，而探索通常是昂贵的或不可行的。 该问题的重要性吸引了许多提议的解决方案，包括重要性抽样（IS），自归一化IS（SNIS）和双稳健（DR）估计。 如果对Q函数进行了明确规定，则DR及其变体可确保半参数局部效率，但如果不是，则它们可能比IS和SNIS都差。 它还不享受SNIS固有的稳定性和局限性。 我们根据经验可能性为OPE提出新的估算器，这些估算器总是比IS，SNIS和DR更有效，并且满足与SNIS相同的稳定性和有界性。 在此过程中，我们对各种属性进行分类，并根据它们对现有估计量进行分类。 除了理论上的保证外，实证研究还表明，新的估计量具有优势。</li></ul></li><li><p><strong>Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards.</strong> Trott, A., Research, S., Zheng, S., Xiong, C., &amp; Socher, R. (2019). [<a href="https://papers.nips.cc/paper/9225-keeping-your-distance-solving-sparse-reward-tasks-using-self-balancing-shaped-rewards" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>尽管在解决稀疏奖励任务时使用定形奖励可能会有所帮助，但成功地应用它们通常需要仔细的工程，并且要针对具体问题。 例如，在代理必须达到某个目标状态的任务中，简单的目标距离奖励成形通常会失败，因为它使学习容易受到局部最优的影响。 我们引入一种简单有效的无模型方法，以学习对成功取决于达到目标状态的任务所形成的目标距离奖励。 我们的方法引入了基于成对的辅助基于距离的奖励，以鼓励多样化的探索。 这种方法有效地防止了学习动态因幼稚的距离目标奖励塑造而稳定在局部最优值附近，并使策略能够有效解决稀疏的奖励任务。 随着代理商学习解决任务，我们的增强目标不需要任何额外的奖励工程或领域专业知识即可实施并收敛到原始稀疏目标。 我们证明了我们的方法成功解决了各种艰苦的探索任务（包括在Minecraft环境中进行迷宫导航和3D构造），其中基于天真的距离的奖励塑造否则会失败，并且固有的好奇心和奖励重新标记策略表现不佳。</li></ul></li><li><p><strong>Language as an Abstraction for Hierarchical Deep Reinforcement Learning.</strong> Jiang, Y., Gu, S., Murphy, K., Finn, C., &amp; Research, G. (2019). Retrieved from <a href="https://sites.google.com/view/hal-demo" target="_blank" rel="noopener">https://sites.google.com/view/hal-demo</a></p><ul><li>分层强化学习为跨越很长一段时间的学习策略提供了一个有希望的框架，但是设计高级策略和低级策略之间的抽象是一项挑战。 语言是一种人类可以解释和灵活的组成表示形式，使其适合于编码各种行为。<br>我们建议使用语言作为高级策略和低级策略之间的抽象，并证明所产生的策略可以成功地解决带有稀疏奖励的长期任务，并且即使在挑战性高维度时也可以使用语言的组合性很好地概括 观察和行动空间。 首先，我们通过各种消融和与不同HRL方法的比较来证明我们的方法在低维观测空间中的优势，然后将我们的方法扩展到挑战基线无法取得进展的像素观测空间。</li></ul></li><li><p><strong>Large Scale Markov Decision Processes with Changing Rewards.</strong> Rivera Cardoso, A., Wang, H., &amp; Xu, H. (2019). [<a href="https://papers.nips.cc/paper/8505-large-scale-markov-decision-processes-with-changing-rewards" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>-我们考虑马尔可夫决策过程（MDP），其奖励是未知的，并且可能以对抗性方式改变。 我们提供了一种算法，可以实现O（\ sqrt {\ tau（\ ln | S | + \ ln | A |）T} \ ln（T））的后悔界限，其中S是状态空间，A是动作 \ tau是MDP的混合时间，T是周期数。 该算法的计算复杂度是| S |的多项式。 和| A |。 然后，我们考虑实际中经常遇到的设置，其中MDP的状态空间太大，无法提供精确的解决方案。 通过用维数为d \ ll | S |的线性体系近似状态动作占用度量，我们提出了一种改进的算法，该算法具有d的计算复杂度多项式，并且与| S |无关。 我们还证明了这种修改后的算法的遗憾，据我们所知，这是大规模MDP设置中第一个\ tilde {O}（\ sqrt {T}）遗憾，其对抗性不断变化。</li></ul></li><li><p><strong>Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints.</strong> Tschiatschek, S., Ghosh, A., Haug, L., Zurich, E., Devidze, R., &amp; Singla, A. (2019). [<a href="https://papers.nips.cc/paper/8668-learner-aware-teaching-inverse-reinforcement-learning-with-preferences-and-constraints" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>逆向强化学习（IRL）使代理可以通过观察（接近）最佳策略中的演示来学习复杂的行为。 通常的假设是，学习者的目标是与老师表现出的行为相匹配。 在本文中，我们考虑了学习者有其自身偏好的设置，另外还考虑了这种设置。 这些偏好可以例如捕获行为偏见，不匹配的世界观或身体限制。 我们研究了两种教学方法：与学习者无关的教学（教师在其中提供了一种忽略学习者偏好的最佳策略的演示）和对学习者的了解的教学（教师在其中考虑了学习者的偏好）。 我们设计了学习者感知的教学算法，并表明与学习者无关的教学可以显着提高性能。</li></ul></li><li><p><strong>Learning Fairness in Multi-Agent Systems.</strong> Jiang, J., &amp; Lu, Z. (2019). [<a href="https://papers.nips.cc/paper/9537-learning-fairness-in-multi-agent-systems" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>公平对人类社会至关重要，有助于稳定和提高生产力。 同样，公平也是许多多主体系统的关键。 公平对待多主体学习可以帮助多主体系统变得高效和稳定。 但是，学习效率和公平性同时是一个复杂的，多目标的联合政策优化。 为了解决这些困难，我们提出了FEN，这是一种新颖的分层强化学习模型。 我们首先分解每个代理商的公平性，然后提出公平有效的回报，让每个代理商学习自己的优化策略。 为避免多目标冲突，我们设计了一个由控制器和几个子策略组成的层次结构，其中控制器通过在提供不同行为以与环境交互的子策略之间进行切换来最大化公平效益回报。  FEN可以通过完全分散的方式进行培训，从而易于在实际应用中进行部署。 从经验上讲，我们表明FEN可以轻松学习公平性和效率，并且在各种多主体场景中均明显优于基准。</li></ul></li><li><p><strong>Learning from Trajectories via Subgoal Discovery.</strong> Paul, S., Van Baar, J., &amp; Roy-Chowdhury, A. K. (2019). [<a href="https://papers.nips.cc/paper/9049-learning-from-trajectories-via-subgoal-discovery" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>学会用稀疏的仅终端奖励来解决复杂的，面向目标的任务通常需要大量样本。 在这种情况下，使用一组专家轨迹可能有助于更快地学习。 但是，通过对这些轨迹进行有监督的预训练进行的模仿学习（IL）可能效果不佳，并且通常需要使用“环路专家”进行额外的微调。 在本文中，我们提出一种使用专家轨迹的方法，并学习将复杂的主要任务分解为较小的子目标。 我们学习了一个将状态空间划分为子目标的函数，然后可以将其用于设计外部奖励函数。 我们遵循的策略是，代理首先使用IL从轨迹中学习，然后使用已识别的子目标切换到强化学习（RL），以减轻IL步骤中的错误。 为了处理由轨迹集表示的状态，我们还学习了一种功能来调节子目标的预测。 我们证明了我们的方法能够解决面向目标的复杂任务，而其他RL，IL或它们在文献中的组合则无法解决。</li></ul></li><li><p><strong>Learning Robust Options by Conditional Value at Risk Optimization.</strong> Hiraoka, T., Imagawa, T., Mori, T., Onishi, T., &amp; Tsuruoka, Y. (2019). [<a href="https://papers.nips.cc/paper/8530-learning-robust-options-by-conditional-value-at-risk-optimization" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>通常通过使用不准确的环境模型（或模拟器）来学习选项，该模型包含不确定的模型参数。 尽管有几种学习选项的方法可以抵抗模型参数的不确定性，但是这些方法仅考虑学习选项的最坏情况或平均情况。 对案例的这种有限考虑通常会产生在未考虑的案例中效果不佳的选择。 在本文中，我们提出了一种基于条件风险值（CVaR）的方法，以学习在平均情况和最坏情况下均能正常工作的选择。 我们扩展了Chow和Ghavamzadeh（2014）提出的基于CVaR的策略梯度方法，以处理鲁棒的马尔可夫决策过程，然后将扩展的方法应用于学习鲁棒选项。 我们进行实验以评估我们在多关节机器人控制任务（HopperIceBlock，Half-Cheetah和Walker2D）中的方法。 实验结果表明，我们的方法产生的选择具有以下优势：1）比仅使平均损失最小化而学习的选项具有更好的最坏情况性能； 2）与仅使最坏情况减少至最小的选择所带来的更好的平均情况性能 失利。</li></ul></li><li><p><strong>Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Estimators for Reinforcement Learning.</strong> Farquhar, G., Whiteson, S., &amp; Foerster, J. (2019). [<a href="https://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在具有未知或棘手动态的随机环境中优化目标的基于梯度的方法，需要估算导数。 我们得出一个目标，即在自动微分下，以任意阶数产生低方差的无偏导数估计量。 我们的目标与任意优势估计量兼容，后者在使用函数逼近时允许控制任意阶导数的偏差和方差。 此外，我们提出了一种通过折衷更远的因果相关性的影响来权衡高阶导数的偏差和方差的方法。 我们证明了我们的估算器在易于分析的MDP和用于连续控制的元强化学习中的正确性和实用性。</li></ul></li><li><p><strong>Mapping State Space using Landmarks for Universal Goal Reaching.</strong> Huang, Z., Liu, F., &amp; Su, H. (2019). [<a href="https://papers.nips.cc/paper/8469-mapping-state-space-using-landmarks-for-universal-goal-reaching" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>充分了解环境的代理商应能够将其技能应用于任何给定的目标，从而导致学习通用价值函数近似器（UVFA）的根本问题。  UVFA学会预测所有州与目标之间的累积奖励。 但是，从经验上讲，长期目标的价值函数始终很难估算，因此可能导致政策失败。 这给学习过程和神经网络的能力提出了挑战。 我们提出了一种在奖励稀疏的大型MDP中解决此问题的方法，其中跨远程状态的探索和路由都极具挑战性。 我们的方法使用分层的方式显式地对环境进行建模，其中包括基于高层动态地标的地图，该地图抽象了访问的状态空间，而基于底层的价值网络则可以得出精确的本地决策。 我们使用最远点采样从过去的经验中选择地标状态，与简单的统一采样相比，它改善了探索性。 实验表明，我们的方法使代理能够在早期训练阶段达到远距离目标，并且在许多挑战性任务中比标准RL算法具有更好的性能。</li></ul></li><li><p><strong>MAVEN: Multi-Agent Variational Exploration.</strong> Mahajan, A., Rashid, T., Samvelyan, M., &amp; Whiteson, S. (2019). [<a href="https://papers.nips.cc/paper/8978-maven-multi-agent-variational-exploration" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>由于执行过程中的通信限制和训练中的计算可处理性，具有分散执行力的集中式训练是协作深度多主体强化学习的重要设置。 在本文中，我们将分析基于值的方法，这些方法在复杂的环境中具有出色的性能。 我们特别专注于QMIX，这是该领域的最新技术。 我们表明，对QMIX和类似方法引入的联合操作值的表示约束导致可证明的较差的探索和次优性。 此外，我们提出了一种称为MAVEN的新方法，该方法通过引入用于分层控制的潜在空间来混合基于价值和基于策略的方法。 基于值的代理根据分层策略控制的共享潜在变量来限制其行为。 这使MAVEN可以实现有针对性的，时间扩展的探索，这对于解决复杂的多主体任务至关重要。 我们的实验结果表明，MAVEN在具有挑战性的SMAC域上实现了显着的性能提升。</li></ul></li><li><p><strong>MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies.</strong> Peng, X. Bin, Chang, M., Zhang, G., Abbeel, P., &amp; Levine, S. (2019). [<a href="https://papers.nips.cc/paper/8626-mcp-learning-composable-hierarchical-control-with-multiplicative-compositional-policies" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>人类可以利用从先前的经验中学到的技能来执行各种复杂的任务。 为了使自治代理具备此功能，他们必须能够从过去的经验中提取可重用的技能，并以新的方式将其重新组合以用于后续任务。 此外，在控制复杂的高维形态（例如人形物体）时，任务通常需要同时协调多种技能。 快速学习每种技能组合的离散原语变得令人望而却步。 可以重组以创建多种行为的可组合基元可能更适合于对该组合爆炸建模。 在这项工作中，我们提出了乘法合成策略（MCP），这是一种学习可重复使用的运动技能的方法，可以用来产生一系列复杂的行为。 我们的方法将代理的技能分解为一组原始元素，其中可以通过乘法合成同时激活多个原始元素。 这种灵活性使原语可以被转移和重组，以引发新任务所必需的新行为。 我们证明了MCP能够从预训练任务（例如动作模仿）中提取高度复杂的模拟角色的组合技能，然后再利用这些技能来解决具有挑战性的连续控制任务，例如将足球运到球门并进行接球 放置物体并将其运输到目标位置。</li></ul></li><li><p><strong>Meta-Inverse Reinforcement Learning with Probabilistic Context Variables.</strong> Yu, L., Yu, T., Finn, C., &amp; Ermon, S. (2019). [<a href="https://papers.nips.cc/paper/9348-meta-inverse-reinforcement-learning-with-probabilistic-context-variables" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>强化学习需要奖励功能，这在现实应用中通常很难提供或设计。 逆向强化学习（IRL）有望从示威中自动学习奖励功能，但仍存在一些主要挑战。 首先，现有的IRL方法从零开始学习奖励功能，需要大量的演示才能正确推断代理可能需要执行的每个任务的奖励。 其次，更微妙的是，现有方法通常假设一个孤立的行为或任务的演示，而在实践中，提供异类行为的数据集则更加自然和可扩展。 为此，我们提出了一个深层的潜在变量模型，该模型能够从非结构化的多任务演示数据中学习奖励，并且至关重要的是，可以利用这一经验从单个演示中推断出对新的，结构相似的任务的可靠奖励。 与多项最新的模仿和逆强化学习方法相比，我们在多个连续控制任务上的实验证明了我们方法的有效性。</li></ul></li><li><p><strong>Mo’ States Mo’ Problems: Emergency Stop Mechanisms from Observation.</strong> Ainsworth, S., Barnes, M., &amp; Srinivasa, S. (2019). [<a href="https://papers.nips.cc/paper/9654-mo-states-mo-problems-emergency-stop-mechanisms-from-observation" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在许多环境中，只需要完成状态空间的一个相对较小的子集即可完成给定任务。 我们开发了一种使用紧急停车（e-stop）的简单技术来利用这种现象。 使用e-stops可以减少所需探索的次数，从而显着提高了样本的复杂性，同时保留了一个性能边界，可以以较小的渐近次优差距有效地折衷收敛速度。 我们分析了紧急停车的遗憾行为，并在离散和连续设置中提供了经验结果，表明我们的重置机制可以在现有强化学习方法的基础上提供数量级的加速。</li></ul></li><li><p><strong>Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach.</strong> Hu, S., Leung, C.-W., &amp; Leung, H.-F. (2019). [<a href="https://papers.nips.cc/paper/9380-modelling-the-dynamics-of-multiagent-q-learning-in-repeated-symmetric-games-a-mean-field-theoretic-approach" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>长期以来，对多主体学习的动力学进行建模一直是重要的研究课题，但是所有先前的工作都集中在两主体设置上，并且大多使用进化博弈论方法。 在本文中，我们研究了一个n趋于无穷大的n-agent设置，这样，agent与其他一些agent一起通过重复的对称双矩阵博弈同时学习其策略。 使用平均场理论，我们通过平均效应近似其他代理对单个代理的影响。 得出了一个Fokker-Planck方程，该方程描述了Agent群体中Q值的概率分布的演变。 据我们所知，这是第一次显示仅由三个方程组成的系统即可描述n代理设置下的Q学习动力学。 我们通过与典型的对称双矩阵博弈和不同的Q值初始设置下基于代理的仿真进行比较来验证模型。</li></ul></li><li><p><strong>Multi-Agent Common Knowledge Reinforcement Learning.</strong> Schroeder de Witt, C. A., Foerster, J. N., Farquhar, G., S Torr, P. H., Böhmer, W., &amp; Whiteson, S. (2019). [<a href="https://papers.nips.cc/paper/9184-multi-agent-common-knowledge-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li></li></ul></li><li><p><strong>Multiagent Evaluation under Incomplete Information.</strong> Rowland, M., Omidshafiei, S., Tuyls, K., Pérolat, J., Valko, M., Piliouras, G., &amp; Munos, R. (2019). [<a href="https://papers.nips.cc/paper/9395-multiagent-evaluation-under-incomplete-information" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>协作式多主体强化学习通常需要分散的政策，这严重限制了主体协调其行为的能力。 在本文中，我们表明代理之间的常识允许复杂的分散式协调。 在许多分散的合作多主体任务中，例如，当主体可以重构彼此观察的一部分时，常识自然就会产生。 由于代理可以独立地就他们的常识达成共识，因此他们可以执行复杂的协调策略，以完全分散的方式对此知识加以限制。 我们提出了多智能体共同知识强化学习（MACKRL），这是一种新颖的随机行为者评论算法，用于学习分层策略树。 层次结构中的较高级别通过以代理的常识为条件来协调代理的组，或委托具有较小子组但可能具有更丰富的常识的较低代理。 整个策略树可以以完全分散的方式执行。 由于最低的策略树级别由每个代理的独立策略组成，因此，MACKRL简化为独立学习的分散策略，这是特例。 我们证明了我们的方法可以利用公知知识在复杂的分散式协调任务（包括随机矩阵博弈和《星际争霸II》单元微管理中的挑战性问题）上表现出色。</li></ul></li><li><p><strong>Multi-View Reinforcement Learning.</strong> Li, M., Wu, L., Bou Ammar, H., &amp; Wang, J. (2019). [<a href="https://papers.nips.cc/paper/8422-multi-view-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>本文涉及多视图强化学习（MVRL），当代理共享共同的动力学但遵循不同的观察模型时，它可以进行决策。 我们通过扩展部分可观察的马尔可夫决策过程（POMDP）以支持不止一种观察模型来定义MVRL框架，并通过观察增强和交叉视图策略转移提出两种解决方法。 我们根据经验评估我们的方法，并证明其在各种环境中的有效性。 具体而言，我们显示出减少了获取用于处理多视图环境的策略的样本复杂度和计算时间。</li></ul></li><li><p><strong>Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes.</strong> Zhang, J., &amp; Bareinboim, E. (2019). [<a href="https://papers.nips.cc/paper/9496-near-optimal-reinforcement-learning-in-dynamic-treatment-regimes" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>动态治疗方案（DTR）由一系列决策规则组成，每个干预阶段一个决策规则，指示如何根据不断发展的治疗方法和协变量历史确定如何为患者分配治疗方案。 这些制度对于控制慢性疾病特别有效，并且可以说是朝着更加个性化的决策方向发展的关键方面之一。 在本文中，我们将调查在线强化学习（RL）问题，以选择最佳的DTR，前提是可以提供观测数据。 我们开发了第一个自适应算法，该算法在在线设置下无需任何历史数据访问即可在DTR中实现近乎最佳的遗憾。 我们进一步从混杂的观测数据中得出有关DTR的系统动力学的信息范围。 最后，我们结合这些结果并开发出一种新颖的RL算法，该算法可有效利用最佳的DTR，同时利用大量但不完善的混杂观测值。</li></ul></li><li><p><strong>Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy.</strong> Liu, B., Cai, Q., Yang, Z., &amp; Wang, Z. (2019). [<a href="https://papers.nips.cc/paper/9242-neural-trust-regionproximal-policy-optimization-attains-globally-optimal-policy" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>通过神经网络对参与者和评论者进行参数化设置的近距离策略优化和信任区域策略优化（PPO和TRPO）在深度强化学习中取得了重大的经验成功。 但是，由于不凸性，PPO和TRPO的全局收敛性仍然鲜为人知，这使理论与实践脱节。 在本文中，我们证明了带有超参数化神经网络的PPO和TRPO的变体以亚线性速率收敛于全局最优策略。 我们分析的关键是在单点单调性概念下无穷维镜下降的全局收敛性，其中梯度和迭代由神经网络实例化。 特别地，由这种神经网络的过度参数化引起的期望表示能力和最优化几何形状使它们能够精确地逼近无限维梯度并进行迭代。</li></ul></li><li><p><strong>No-Press Diplomacy: Modeling Multi-Agent Gameplay.</strong> Paquette, P., Lu, Y., Bocco, S., Smith, M. O., Ortiz-Gagné, S., Kummerfeld, J. K., … Courville, A. (2019). Retrieved from <a href="https://github.com/diplomacy/research" target="_blank" rel="noopener">https://github.com/diplomacy/research</a></p><ul><li></li></ul></li><li><p><strong>Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs.</strong> Simchowitz, M., &amp; Jamieson, K. (2019). [<a href="https://papers.nips.cc/paper/8399-non-asymptotic-gap-dependent-regret-bounds-for-tabular-mdps" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>本文建立了乐观算法获得了间歇式MDP的间隙依赖和非渐近对数后悔。 与先前的工作相比，我们的边界不依赖于直径样的数量或遍历，而是在依赖于间隙的对数后悔和\ widetilde {\ mathcal {O}}（\ sqrt {HSAT}）之间平滑地插值 -minimax率。 我们分析中的关键技术是一种新颖的剪裁后悔分解法，该分解法适用于一系列最新的情节式MDP乐观算法。</li></ul></li><li><p><strong>Non-Cooperative Inverse Reinforcement Learning.</strong> Zhang, X., Zhang, K., Tamer, E. M., &amp; Bas¸ar, B. (2019). [<a href="https://papers.nips.cc/paper/9145-non-cooperative-inverse-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在有战略对手的情况下做出决定时，需要考虑到对手主动掩盖其预期目标的能力。 为了描述这种战略情况，我们介绍了非合作逆向强化学习（N-CIRL）形式主义。  N-CIRL形式主义由两个目标完全错位的主体组成，其中只有一个主体知道真正的目标功能。 形式上，我们将N-CIRL形式主义建模为具有单面不完整信息的零和马尔可夫博弈。 通过与知识渊博的玩家互动，知识渊博的玩家会尝试推断和优化真实目标功能。 由于单方面信息不完整，可以将多阶段博弈分解为由递归公式表示的一系列单阶段博弈。 解决此递归公式可产生N-CIRL博弈的价值以及更明智的玩家的均衡策略。 通过形成辅助游戏而构造的另一种递归公式，称为对偶游戏，产生了知情程度较低的玩家策略。 在这两个递归公式的基础上，我们开发了一种易于计算的算法来近似求解平衡策略。 最后，我们通过在新型网络安全环境中进行广泛的数值模拟，证明了N-CIRL形式主义优于现有的多代理IRL形式主义的好处。</li></ul></li><li><p><strong>Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning.</strong> Lecarpentier, E., &amp; Rachelson, E. (2019). [<a href="https://papers.nips.cc/paper/8942-non-stationary-markov-decision-processes-a-worst-case-approach-using-model-based-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>这项工作解决了非平稳随机环境中健壮的零击计划的问题。 我们研究随着时间发展的马尔可夫决策过程（MDP），并在这种情况下考虑基于模型的强化学习算法。 我们有两个假设：1）环境以有限的演化速率连续演化；  2）在每个决策时期都知道当前模型，但不知道其演变过程。 我们的贡献可以分为四个方面。  1）我们定义一类特定的MDP，我们称其为非平稳MDP（NSMDP）。 我们通过对过渡和奖励函数w.r.t做出Lipschitz-Continuity的假设来介绍正则演化的概念。 时间;  2）我们考虑使用当前环境模型但不了解其未来发展的规划代理。 这导致我们考虑一种最坏情况的方法，即将环境视为对抗性媒介。  3）遵循这种方法，我们提出了风险厌恶树搜索（RATS）算法，这是一种类似于Minimax搜索的基于零触发的基于模型的方法；  4）我们从经验上说明了RATS带来的好处，并将其性能与参考基于模型的算法进行了比较。</li></ul></li><li><p><strong>No-Regret Learning in Unknown Games with Correlated Payoffs.</strong> Sessa, P. G., Zürich, E., Bogunovic, I., Kamgarpour, M., &amp; Krause, A. (2019). [<a href="https://papers.nips.cc/paper/9514-no-regret-learning-in-unknown-games-with-correlated-payoffs" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们考虑学习玩具有未知奖励功能的重复多主体游戏的问题。 当提供完整的信息反馈时，单人在线学习算法会遇到很大的遗憾，但不幸的是，在许多现实情况下，这是不可用的。 单独的强盗反馈，即仅观察所选择动作的结果，会导致性能大大降低。 在本文中，我们考虑一个自然模型，在该模型中，除了对获得的奖励进行嘈杂的测量外，玩家还可以观察对手的行为。 这种反馈模型，加上对奖励函数的规律性假设，使我们能够利用高斯过程（GP）来利用不同游戏结果之间的相关性。 我们提出了一种基于置信度的新型强盗算法GP-MW，该算法利用GP模型获得奖励功能并运行乘权（MW）方法。 我们获得了新颖的依赖于内核的后悔界限，可以与完整信息环境中的已知界限相提并论，同时大大改善了现有的强盗结果。 我们通过实验证明了GP-MW在随机矩阵游戏中的有效性，以及现实世界中的交通路线和电影推荐问题。 在我们的实验中，GP-MW始终优于几个基准，而其性能通常可与可获取全部信息反馈的方法相媲美。</li></ul></li><li><p><strong>Off-Policy Evaluation via Off-Policy Classification.</strong> Irpan, A., Rao, K., Bousmalis, K., Harris, C., Ibarz, J., &amp; Levine, S. (2019). [<a href="https://papers.nips.cc/paper/8783-off-policy-evaluation-via-off-policy-classification" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在这项工作中，我们考虑了在现实环境中用于深度强化学习（RL）的模型选择问题。 通常，深度RL算法的性能是通过与目标环境的策略上交互来评估的。 但是，为了尽早停止或进行超参数调整而在实际环境中比较模型的成本很高，而且通常实际上是不可行的。 这使我们在这种情况下研究了非政策性政策评估（OPE）。 我们专注于基于价值的方法的OPE，这在诸如机器人技术之类的深度RL中特别受关注，其中基于Q函数估计的非策略算法通常可以比直接策略优化获得更好的样本复杂性。 此外，现有的OPE度量标准要么依赖于环境模型，要么依赖于重要性采样（IS）来纠正不符合政策要求的数据。 但是，对于高维度的观测（例如图像），环境模型可能难以拟合，基于价值的方法可能会使IS难以使用，甚至变得条件恶劣，尤其是在处理连续动作空间时。 在本文中，我们关注具有连续动作空间和稀疏二进制奖励的MDP的特殊情况，它代表了许多重要的实际应用。 通过将OPE定义为正无标签（PU）分类问题，我们提出了一种既不依赖模型也不依赖IS的替代度量。 我们通过实验表明，该指标在许多任务上均优于基线。 最重要的是，它可以在多种泛化方案中可靠地预测不同策略的相对性能，包括将针对基于图像的机器人操纵任务进行模拟训练的策略转移到现实世界中。</li></ul></li><li><p><strong>On the Correctness and Sample Complexity of Inverse Reinforcement Learning.</strong> Komanduru, A., &amp; Honorio, J. (2019). [<a href="https://papers.nips.cc/paper/8933-on-the-correctness-and-sample-complexity-of-inverse-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>逆向强化学习（IRL）是找到奖励函数的问题，该奖励函数针对给定的马尔可夫决策过程生成给定的最佳策略。 本文着眼于具有有限状态和作用的IRL问题的算法无关的几何分析。 然后考虑到逆强化问题的基本目标，提出了由几何分析引起的IRL问题的L1正则化支持向量机公式：找到产生指定最优策略的奖励函数。 本文进一步分析了提出的具有n个状态和k个动作的逆强化学习的公式，并给出了每行最多d个非零的转移概率矩阵的O（d ^ 2 \ log（nk））的样本复杂度， 恢复奖励函数，该奖励函数生成的策略在真实转移概率方面满足Bellman的最优性条件。</li></ul></li><li><p><strong>Planning with Goal-Conditioned Policies.</strong> Nasiriany, S., Pong, V. H., Lin, S., &amp; Levine, S. (2019). [<a href="https://papers.nips.cc/paper/9623-planning-with-goal-conditioned-policies" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>计划方法可以通过构成简单行为来解决临时扩展的顺序决策问题。 但是，规划需要对状态和转换进行适当的抽象，而这些抽象通常需要手动设计。 相比之下，强化学习（RL）可以直接从低级输入中获取行为，但在时间扩展任务上却遇到困难。 我们是否可以利用强化学习来自动形成计划所需的抽象，从而获得两种方法中的最佳方法？ 我们表明，通过RL学习的目标条件策略可以纳入计划中，以便计划者可以专注于要达到的状态，而不是如何达到这些状态。 但是，对于复杂的状态观察（例如图像），并非所有输入都代表有效状态。 因此，我们还建议使用潜在变量模型来紧凑地表示计划者的有效状态集，以便策略提供动作的抽象，而潜在变量模型提供状态的抽象。 我们将我们的方法与基于计划的方法和无模型的方法进行了比较，发现在对需要非贪婪，多阶段行为的基于图像的任务进行评估时，我们的方法明显优于先前的工作。</li></ul></li><li><p><strong>Policy Poisoning in Batch Reinforcement Learning and Control.</strong> Ma, Y., Zhang, X., Sun, W., &amp; Zhu, X. (2019). [<a href="https://papers.nips.cc/paper/9599-policy-poisoning-in-batch-reinforcement-learning-and-control" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们研究了对批量强化学习和控制的安全威胁，攻击者旨在毒化已学习的策略。 受害者是一名强化学习者/控制者，他首先从批处理数据集中估算动力和报酬，然后针对估算估算最佳策略。 攻击者可以在学习发生之前稍稍修改数据集，并希望迫使学习者学习攻击者选择的目标策略。 我们提供了一个解决批处理策略中毒攻击的统一框架，并实例化了对两个标准受害者的攻击：强化学习中的表格确定性等效学习器和控制中的线性二次调节器。 我们表明，这两种实例化都导致凸优化问题，在该问题上可以保证全局最优，并提供了攻击可行性和攻击成本的分析。 实验证明了策略中毒攻击的有效性。</li></ul></li><li><p><strong>Privacy-preserving Q-Learning with Functional Noise in Continuous Spaces.</strong> Wang, B., Hegde, N., &amp; Ai, B. (2019). [<a href="https://papers.nips.cc/paper/9310-privacy-preserving-q-learning-with-functional-noise-in-continuous-spaces" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们考虑了在连续空间中进行强化学习的差分专用算法，因此相邻的奖励函数无法区分。 这可以防止奖励信息被逆强化学习之类的方法利用。 现有的保证差异隐私的研究无法扩展到无限状态空间，因为确保隐私的噪声水平将相应地扩展到无穷大。 我们的目的是保护值函数逼近器，而不必考虑对该函数查询的状态数。 通过在训练中反复将功能噪声添加到值函数中来实现。 通过对噪声空间的内核，此类噪声样本的概率范围以及迭代过程的组成进行一系列分析，我们显示出严格的隐私保证。 通过证明状态空间为离散状态时算法的近似最优性，我们可以深入了解效用分析。 实验证实了我们的理论发现，并显示了对现有方法的改进。</li></ul></li><li><p><strong>Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters.</strong> Metelli, A. M., Likmeta, A., &amp; Restelli, M. (2019). Retrieved from <a href="https://github.com/albertometelli/wql" target="_blank" rel="noopener">https://github.com/albertometelli/wql</a>.</p><ul><li></li></ul></li><li><p><strong>Provably Efficient Q-Learning with Low Switching Cost.</strong> Bai, Y., Xie, T., Jiang, N., Wang, Y.-X., &amp; Barbara, S. (2019). [<a href="https://papers.nips.cc/paper/9013-provably-efficient-q-learning-with-low-switching-cost" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们采取初始步骤来研究适应性有限的PAC-MDP算法，即在后悔最小化期间​​尽可能不频繁更改其探索策略的算法。 这是由于在现实世界的应用程序（例如医学领域）中运行完全自适应算法的困难而引起的，我们建议使用\ emph {local switch cost}概念来量化自适应性。 我们的主要贡献（Q学习与UCB2探索）是一种用于H步情节MDP的无模型算法，该算法可实现亚线性遗憾，其在K个情节中的局部切换成本为O（H ^ 3SA \ log K），我们提供了一个较低的 对于任何无悔算法，\ Omega（HSA）都会限制本地交换成本。 我们的算法可以自然地适应并发设置\ citep {guo2015concurrent}，该设置会产生不平凡的结果，在某些方面，这些结果会比以前的工作有所改善。</li></ul></li><li><p><strong>Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost.</strong> Yang, Z., Chen, Y., Hong, M., &amp; Wang, Z. (2019). [<a href="https://papers.nips.cc/paper/9044-provably-global-convergence-of-actor-critic-a-case-for-linear-quadratic-regulator-with-ergodic-cost" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>尽管actor-critic算法在经验上取得了成功，但其理论理解仍然落后。 在更广泛的范围内，参与者批评者可以被视为用于双级优化的在线交替更新算法，已知其收敛是脆弱的。 为了了解行为批评家的不稳定性，我们将重点放在线性二次调节器上，这是强化学习的一种简单而基本的设置。 我们在这种情况下建立了演员评论家的非渐近收敛分析。 特别是，我们证明了行为批评者以线性收敛速度找到了行为者（政策）和批评者（行动价值函数）的全局最优对。 我们的分析可能是迈向具有非凸子问题的双级优化的完整理论理解的第一步，这在最坏的情况下是NP-困难的，通常使用启发式方法解决。</li></ul></li><li><p><strong>Real-Time Reinforcement Learning.</strong> Ramstedt Mila, S., &amp; Pal, C. (2019). [<a href="https://papers.nips.cc/paper/8571-real-time-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Markov决策过程（MDP）是强化学习（RL）中大多数算法的基础数学框架，通常以错误地假设代理人的环境状态在选择操作过程中不会改变的方式使用。 随着基于MDP的RL系统开始在现实世界中的安全关键情况下找到应用，传统MDP所基于的假设与实时计算的现实之间的这种不匹配可能导致不良结果。 在本文中，我们介绍了一个新的框架，其中状态和动作同时演化，并显示了它与经典MDP公式的关系。 我们在新的实时公式下分析了现有算法，并说明了为什么在实时使用时它们不是最优的。 然后，我们利用这些洞察力来创建新的实时演员评论员（RTAC）算法，该算法在实时和非实时设置方面均优于现有的最新连续控制算法“软件演员批评家”。</li></ul></li><li><p><strong>Reconciling λ-Returns with Experience Replay.</strong> Daley, B., &amp; Amato, C. (2019). [<a href="https://papers.nips.cc/paper/8397-reconciling-returns-with-experience-replay" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>现代的深度强化学习方法已经脱离了资格跟踪所需的增量学习，这使得在这种情况下难以实现λ返回。 特别是，利用经验重播的非政策方法仍然存在问题，因为它们对小批产品的随机抽样不利于λ回报的有效计算。 然而，基于重播的方法通常是最有效的采样方法，将λ-returns纳入其中是实现新的最新性能的可行方法。 为此，我们提出了第一种方法，可以在不依赖于其他形式的去相关（例如异步梯度更新）的情况下，在基于任意重放的方法中实际使用λ返回。 通过将过去过渡的短序列提升为重播内存中的小缓存，可以通过共享Q值来有效地预先计算相邻的λ返回。 计算不会浪费在从未采样过的经验上，并且存储的λ返回值表现为替代目标网络的稳定的时差（TD）目标。 此外，我们的方法赋予了在采样之前观察TD错误的独特能力； 第一次，可以根据转换的真实重要性而不是通过代理对其进行优先级转换。 此外，我们提出了TD误差的新颖用法，以动态选择有助于更快学习的λ值。 我们证明，即使在部分可观察性的前提下，这些创新也可以在玩Atari 2600游戏时提高DQN的性能。 虽然我们的工作专门针对λ返回，但这些思想适用于任何多步返回估计。</li></ul></li><li><p><strong>Regret Bounds for Learning State Representations in Reinforcement Learning.</strong> Ortner, R., Pirotta, M., Fruit, R., Lazaric, A., &amp; Maillard, O.-A. (2019). [<a href="https://papers.nips.cc/paper/9435-regret-bounds-for-learning-state-representations-in-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>当学习者可以使用几种状态表示形式（将历史映射到离散状态空间）时，我们考虑在线强化学习的问题。 假定这些表示中的至少一个诱发了马尔可夫决策过程（MDP），并且根据针对该MDP表示中给出最高平均奖励的最优策略的累积后悔来衡量代理的绩效。 我们提出了一种在通信马尔可夫决策过程中具有O（sqrt（T））后悔的算法（UCB-MS）。 后悔的界限表明，UCB-MS自动适应了马尔可夫模型。 这优于文献中目前已知的最佳结果，后者给出了O（T ^（2/3））阶的遗憾界限。</li></ul></li><li><p><strong>Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function.</strong> Zhang, Z., &amp; Ji, X. (2019). [<a href="https://papers.nips.cc/paper/8549-regret-minimization-for-reinforcement-learning-by-evaluating-the-optimal-bias-function" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出一种基于\ emph {不确定性中的乐观主义}（OFU）原理的算法，该算法能够有效地学习具有有限状态作用空间的Markov决策过程（MDP）建模的强化学习（RL）。 通过评估最佳偏置函数h ^ {<em>}的状态对差异，该算法实现了\ tilde {O}（\ sqrt {SATH}）\ footnote {后缀\ tilde {O}表示O 忽略对数因子。  }对于具有S状态和A动作的MDP，在h ^ {</em>}跨度的上限H，即sp（h ^ {*}）已知的情况下。 此结果优于\ tilde {O}（HS \ sqrt {AT}）\ cite {bartlett2009regal}的最佳后悔范围，是\ sqrt {SH}的一个因素。 此外，此后悔界限与\ Omega（\ sqrt {SATH}）\ cite {jaksch2010near}的下限匹配为对数因子。 结果，我们证明，对于具有有限直径D的MDP，与\ Omega（\ sqrt {DSAT}）\ cite的下限相比，存在\ tilde {O}（\ sqrt {DSAT}）的接近最佳后悔边界 {jaksch2010near}。</li></ul></li><li><p><strong>Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives.</strong> Chi Cheung, W. (2019). [<a href="https://papers.nips.cc/paper/8361-regret-minimization-for-reinforcement-learning-with-vectorial-feedback-and-complex-objectives" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们考虑一个参与在线马尔可夫决策过程的代理商，并且每轮都会收到结果向量。 该代理旨在同时优化与多维结果相关的多个目标。 由于状态转换，为实现接近最佳状态而平衡矢量结果具有挑战性。 特别是，与单一目标情况相反，固定政策通常不是最优的。 我们提出了一种基于Frank-Wolfe算法（Frank和Wolfe 1956），UCRL2（Jaksch等人2010）以及一种至关重要的新颖梯度阈值程序的无悔算法。 该过程涉及仔细延迟梯度更新，并返回非平稳策略，该策略会使结果多样化以优化目标。</li></ul></li><li><p><strong>Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning.</strong> Shi, W., Song, S., Wu, H., Hsu, Y.-C., Wu, C., &amp; Huang, G. (2019). Retrieved from <a href="https://github.com/shiwj16/raa-drl" target="_blank" rel="noopener">https://github.com/shiwj16/raa-drl</a>.</p><ul><li></li></ul></li><li><p><strong>Reinforcement Learning with Convex Constraints.</strong> Miryoosefi, S., Brantley, K., Iii, H. D., Dudík, M., &amp; Schapire, R. E. (2019). [<a href="https://papers.nips.cc/paper/9556-reinforcement-learning-with-convex-constraints" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在标准强化学习（RL）中，学习代理会寻求优化总体奖励。 但是，期望行为的许多关键方面更自然地表示为约束。 例如，设计人员可能希望限制不安全动作的使用，增加轨迹的多样性以进行探索，或者在奖励稀少时近似专家轨迹。 在本文中，我们提出了一种算法方案，可以处理RL任务中的多种约束：具体而言，任何需要某些向量测量值（例如使用动作）的期望值的约束都位于凸集中。 这捕获了先前研究过的约束（例如安全性和与专家的接近度），但也启用了新的约束类别（例如多样性）。 我们的方法具有严格的理论保证，并且仅依赖于大约解决标准RL任务的能力。 因此，它可以轻松适应任何无模型或基于模型的RL。 在我们的实验中，我们证明它与以前的通过约束强制执行安全性的算法相匹配，但是还可以强制执行这些算法未包含的新属性，例如多样性。</li></ul></li><li><p><strong>Robust exploration in linear quadratic reinforcement learning.</strong> Umenberger, J., Ferizbegovic, M., Schön, T. B., &amp; Hjalmarsson, H. (2019). [<a href="https://papers.nips.cc/paper/9668-robust-exploration-in-linear-quadratic-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>学会在不确定和动态的环境中进行决策是许多领域的基本绩效任务。 本文涉及一个未知线性动力学系统的学习控制策略问题，以最小化二次成本函数。 我们提出了一种基于凸优化的方法，该方法可以“稳健”地完成此任务，即，考虑到给定的观测数据，将考虑系统不确定性的最坏情况下的成本降至最低。 该方法平衡了开发和探索之间的联系，从而以一种使系统兴奋的方式减少了最坏情况下的成本最敏感的模型参数的不确定性。 数值模拟和在硬件在环伺服机构中的应用被用来演示这种方法，与在这两种方法中都观察到的替代方法相比，它具有明显的性能和鲁棒性。</li></ul></li><li><p><strong>Robust Multi-agent Counterfactual Prediction.</strong> Peysakhovich, A., Kroer, C., &amp; Lerer, A. (2019). [<a href="https://papers.nips.cc/paper/8572-robust-multi-agent-counterfactual-prediction" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们考虑了使用记录的数据做出预测的问题，如果我们在多代理系统中更改“游戏规则”会发生什么。 这项任务很困难，因为在许多情况下，我们观察到的是个人所采取的行动，而不是个人信息或充分的奖励职能。 此外，代理商具有战略意义，因此，当规则更改时，他们也将更改其行动。 现有方法（例如，结构估计，逆强化学习）假设代理的行为来自优化某种效用或系统处于平衡状态。 他们通过使用观察到的动作来学习潜在的效用函数（也称为类型），然后求解反事实环境的平衡，从而做出反事实预测。 这种方法强加了严格的假设，例如观察到的代理商的合理性以及环境和代理商的效用函数的正确模型。 我们提出了一种方法来分析反事实结论对违反这些假设的敏感性，我们称其为健壮的多主体反事实预测（RMAC）。 我们提供了一种用于计算RMAC范围的一阶方法。 我们将RMAC应用于市场设计中的经典环境：拍卖，学校选择和社会选择。</li></ul></li><li><p><strong>Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update.</strong> Lee, S. Y., Choi, S., &amp; Chung, S.-Y. (2019). [<a href="https://papers.nips.cc/paper/8484-sample-efficient-deep-reinforcement-learning-via-episodic-backward-update" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提出了情节向后更新（EBU）–一种具有直接值传播的新颖的深度强化学习算法。 与常规使用统一随机抽样的经验重播相比，我们的代理对整个情节进行抽样，并将状态值连续传播到其先前状态。 我们的计算有效的递归算法允许稀疏和延迟的奖励直接在采样情节的所有过渡中传播。 我们从理论上证明了EBU方法的收敛性，并通过实验证明了其在确定性和随机环境中的性能。 尤其是在Atari 2600域的49个游戏中，EBU分别仅使用5％和10％的样本即可达到DQN的均值和中值人类标准化性能。</li></ul></li><li><p><strong>Search on the Replay Buffer: Bridging Planning and Reinforcement Learning.</strong> Eysenbach, B., Salakhutdinov, R., Levine Φψ Θ Cmu, S., &amp; Brain, G. (2019). Retrieved from <a href="http://bit.ly/rl_search" target="_blank" rel="noopener">http://bit.ly/rl_search</a></p><ul><li></li></ul></li><li><p><strong>Semi-Parametric Efficient Policy Learning with Continuous Actions.</strong> Demirer, M., Syrgkanis, V., Lewis, G., &amp; Chernozhukov, V. (2019). [<a href="https://papers.nips.cc/paper/9643-semi-parametric-efficient-policy-learning-with-continuous-actions" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们考虑具有连续动作空间的非策略评估和优化。 我们专注于观测数据，其中数据收集策略未知，需要根据数据进行估算。 我们采用半参数方法，其中值函数在处理中采用已知的参数形式，但我们对它如何依赖于所观察的上下文是不可知的。 我们为此设置提出了双重鲁棒的非策略估计，并表明基于该双重鲁棒的估计的非策略优化对于估计策略函数或回归模型的误差具有鲁棒性。 我们还表明，我们的非政策估算的方差达到了半参数效率界限。 如果模型不满足我们的半参数形式，而是根据真实值函数对该函数空间的最佳投影来衡量遗憾，则我们的结果也适用。 我们的工作从仅考虑离散操作的观察数据扩展了策略优化的先前方法。 我们在以最佳个性化定价为动力的综合数据示例中对我们的方法进行了实验评估。</li></ul></li><li><p><strong>Shaping Belief States with Generative Environment Models for RL.</strong> Gregor, K., Rezende, D. J., Besse, F., Wu, Y., &amp; Merzic, H. (2019). [<a href="https://papers.nips.cc/paper/9503-shaping-belief-states-with-generative-environment-models-for-rl" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>当代理与复杂的环境交互时，他们必须形成并保持对那个环境相关方面的信念。 我们提出了一种在复杂环境中有效训练表达生成模型的方法。 我们表明，具有表达生成模型的预测算法可以在视觉丰富和动态3D环境中形成稳定的置信状态。 更准确地说，我们表明，学习的表示形式可以捕获环境的布局以及代理的位置和方向。 我们的实验表明，与强大的无模型基准代理相比，该模型大大提高了许多强化学习（RL）任务的数据效率。 我们发现，预测未来的多个步骤（超调），再结合具有表现力的生成模型，对于稳定表示形式的出现至关重要。 在实践中，在RL中使用表达生成模型在计算上是昂贵的，我们提出了一种减轻这种计算负担的方案，使我们能够构建与无模型基线竞争的代理。</li></ul></li><li><p><strong>Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction.</strong> Kumar, A., Fu, J., Google Brain, G. T., &amp; Levine, S. (2019). [<a href="https://papers.nips.cc/paper/9349-stabilizing-off-policy-q-learning-via-bootstrapping-error-reduction" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>非政策强化学习旨在利用从先前政策中收集到的经验进行高效样本学习。 但是，在实践中，基于Q学习和参与者批评方法的常用的非策略近似动态编程方法对数据分布高度敏感，并且在不收集其他策略上数据的情况下只能取得有限的进展。 作为朝着更强大的政策外算法迈出的一步，我们研究了政策外经验是固定的且与环境没有进一步交互的环境。 我们将\ emph {bootstrapping error}确定为当前方法中不稳定的关键来源。 引导错误是由于来自训练数据分发之外的动作导致的引导错误，并且通过Bellman备份操作员进行累积。 我们从理论上分析引导错误，并演示了如何在备份中严格限制操作选择可以减轻这种错误。 根据我们的分析，我们提出了一种实用的算法，即引导误差累积减少（BEAR）。 我们证明，BEAR能够从一系列不连续的控制任务中，从不同的非政策性分布（包括随机数据和次优演示）中稳健地学习。</li></ul></li><li><p><strong>The Option Keyboard Combining Skills in Reinforcement Learning.</strong> Barreto, A., Borsa, D., Hou, S., Comanici, G., Aygün, E., Hamel, P., … Precup, D. (2019). [<a href="https://papers.nips.cc/paper/9463-the-option-keyboard-combining-skills-in-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>结合已知技能以创建新技能的能力对于解决复杂的强化学习问题（可能会持续很长时间）至关重要。 我们认为，一种强大的技能组合方式是在伪奖励（或“累积量”）的空间中定义和操纵它们。 在此前提下，我们提出了使用期权的形式主义来结合技能的框架。 我们表明，每个确定性选项都可以明确表示为扩展域中定义的累积量。 在此见解和先前对转移学习的结果的基础上，我们将展示如何近似其累积量是已知期权累积量的线性组合的期权。 这意味着，一旦我们学习了与一组累积量相关的选项，我们就可以立即合成由它们的任何线性组合引起的选项，而无需进行任何学习。 我们描述了此框架如何为环境提供层次结构接口，该环境的抽象操作对应于基本技能的组合。 我们在资源管理问题和涉及四足机器人的导航任务中展示了我们方法的实际好处。</li></ul></li><li><p><strong>Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies.</strong> Efroni, Y., Merlis, N., Ghavamzadeh, M., &amp; Mannor, S. (2019). [<a href="https://papers.nips.cc/paper/9389-tight-regret-bounds-for-model-based-reinforcement-learning-with-greedy-policies" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>最先进的基于模型的高效强化学习（RL）算法通常通过迭代求解经验模型来发挥作用，即对收集的经验建立的马尔可夫决策过程（MDP）进行全面规划。 在本文中，我们将重点放在有限状态有限水平MDP设置中的基于模型的RL上，并建立以贪婪策略进行探索-通过一步计划进行操作-可以在遗憾，O（  \ sqrt {HSAT}）。 因此，可以完全避免基于模型的RL的全面规划而不会导致任何性能下降，并且这样做可以使计算复杂度降低S倍。结果基于对实时动态规划的新颖分析， 然后扩展到基于模型的RL。 具体来说，我们将执行全面计划的现有算法归纳为一步计划。 对于这些概括，我们证明了遗憾的界限与它们的全面规划的比率相同。</li></ul></li><li><p><strong>Towards Interpretable Reinforcement Learning Using Attention Augmented Agents.</strong> Mott, A., Zoran, D., Chrzanowski, M., Wierstra, D., &amp; Rezende, D. J. (2019). [<a href="https://papers.nips.cc/paper/9400-towards-interpretable-reinforcement-learning-using-attention-augmented-agents" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>受近期用于图像字幕和问答的注意力模型工作的启发，我们提出了针对强化学习领域的软注意力模型。 该模型通过一种柔软的，自上而下的注意力机制来阻塞代理的视图，从而通过顺序查询其环境视图来迫使代理专注于与任务相关的信息。 注意机制的输出允许直接观察代理程序用来选择其动作的信息，从而比传统模型更容易解释该模型。 我们分析了代理商学习的不同策略，并发现少数策略在不同游戏中反复出现。 我们还表明，该模型学会了分别查询空间和内容（在“哪里”与“什么”）。 我们证明，使用此机制的代理可以在ATARI任务上与最新模型竞争，同时仍可解释。</li></ul></li><li><p><strong>Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling.</strong> Xie, T., Ma, Y., &amp; Wang, Y.-X. (2019). [<a href="https://papers.nips.cc/paper/9161-towards-optimal-off-policy-evaluation-for-reinforcement-learning-with-marginalized-importance-sampling" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>出于现实世界中需要安全策略迭代的强化学习（RL）的许多应用的考虑，我们考虑了非策略评估（OPE）的问题—-使用通过不同方法获得的历史数据来评估新策略的问题 行为策略—-具有较长的视野和较大的动作空间的非平稳间歇性马尔可夫决策过程（MDP）模型。 现有的重要性抽样（IS）方法通常遭受较大的方差，而该方差与RL范围H呈指数关系。为解决此问题，我们考虑采用边际重要性抽样（MIS）估算器，该估算器递归地估算目标政策在每个步骤的状态边际分布 。  MIS实现\ frac {1} {n} \ sum_ {t = 1} ^ H \ mathbb {E} _ {\ mu} \ left [\ frac {d_t ^ \ pi（s_t）^ 2）的均方误差 } {d_t ^ \ mu（s_t）^ 2} \ Var _ {\ mu} \ left [\ frac {\ pi_t（a_t | s_t）} {\ mu_t（a_t | s_t）} \ big（V_ {t + 1}  ^ \ pi（s_ {t + 1}）+ r_t \ big）\中间|  s_t \ right] \ right] + \ tilde {O}（n ^ {-1.5}）其中\ mu和\ pi是日志记录和目标策略，d_t ^ {\ mu}（s_t）和d_t ^ {\ pi}  （s_t）是第t步状态的边际分布，H是水平，n是样本大小，V_ {t + 1} ^ \ pi是\ pi下MDP的值函数。 结果与[Jiang and Li，2016]中的Cramer-Rao下界匹配为H的乘数。据我们所知，这是第一个与H多项式相关的OPE估计误差。除了理论， 我们展示了我们的方法在时变，部分可观察和长期水平的RL环境中的经验优势。</li></ul></li><li><p><strong>Trust Region-Guided Proximal Policy Optimization.</strong> Wang, Y., He, H., Tan, X., &amp; Gan, Y. (2019). Retrieved from <a href="https://github.com/wangyuhuix/TRGPPO" target="_blank" rel="noopener">https://github.com/wangyuhuix/TRGPPO</a>.</p><ul><li></li></ul></li><li><p><strong>Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples.</strong> Xu, T., Zou, S., &amp; Liang, Y. (2019). [<a href="https://papers.nips.cc/paper/9248-two-time-scale-off-policy-td-learning-non-asymptotic-analysis-over-markovian-samples" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>基于梯度的时差（GTD）算法广泛用于非政策学习场景。 其中，已证明具有梯度校正（TDC）算法的两个时标TD具有卓越的性能。 与以前的研究仅以相同且独立分布的（iid）数据样本为特征的TDC的非渐近收敛速度相反，我们提供了在非iid \ Markovian样本下两个时间尺度TDC的第一个非渐近收敛分析。 路径和线性函数近似。 我们证明了两个时标TDC在递减步长下收敛速度可以与O（log t / t ^（2/3））一样快，并且在恒定步距下可以指数快收敛，但代价是不消失 。 我们进一步提出了一种逐步减小步长的TDC算法，并证明了它以逐段线性收敛速率渐近收敛并具有任意小的误差。 我们的实验表明，在恒定步长的情况下，这种算法的收敛速度与TDC一样快，而在逐渐减小的步长下，仍具有与TDC相当的精度。</li></ul></li><li><p><strong>Unsupervised Curricula for Visual Meta-Reinforcement Learning.</strong> Jabri, A., Hsu, K., Eysenbach, B., Gupta, A., Levine, S., &amp; Finn, C. (2019). [<a href="https://papers.nips.cc/paper/9238-unsupervised-curricula-for-visual-meta-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>原则上，元强化学习算法利用许多任务的经验来学习快速有效的强化学习（RL）策略。 但是，当前的meta-RL方法依赖于手动定义的训练任务分布，而手工制作这些任务分布可能具有挑战性且耗时。 是否可以无人监督的方式发现有用的’’培训前任务’’？ 我们通过在可视化环境中对无监督的交互进行建模，开发了一种无监督的算法来诱导自适应元训练任务分布，即自动课程。 任务分布由元学习者的轨迹分布的参数密度模型来支撑。 我们将无监督的meta-RL公式化为潜在任务变量和元学习者的数据分布之间的信息最大化，并描述了一种实用的实例化方法，它在将最新经验集成到任务分布和更新任务的元学习之间进行切换。 重复此过程将导致迭代重组，以使课程随着元学习者数据分布的变化而适应。 此外，我们展示了用于视觉表示的判别性聚类框架如何通过像素观测来支持轨迹级任务在领域中的获取和探索，从而避免了替代方案的陷阱。 在基于视觉的导航和操纵域的实验中，我们表明该算法允许无监督的元学习，既可以转移到手工制作的奖励函数指定的下游任务，又可以作为预训练，以更有效地进行测试任务的元学习 分布。</li></ul></li><li><p><strong>Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning.</strong> Qu, C., Mannor, S., Xu, H., Qi, Y., Song, L., &amp; Xiong, J. (2019). [<a href="https://papers.nips.cc/paper/8402-value-propagation-for-decentralized-networked-deep-multi-agent-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们在完全分散的环境中考虑网络化多主体强化学习（MARL）问题，在该环境中，主体学习协调以取得共同成功。 在交通控制，分布式控制和智能电网等许多领域中普遍遇到此问题。 我们假设每个代理都位于通信网络的节点上，并且只能与其邻居交换信息。 使用softmax时间一致性，我们推导了原始对偶分散优化方法，并获得了一种原理性且数据有效的迭代算法，称为{\ em值传播}。 利用非线性函数逼近，证明了\ mathcal {O}（1 / T）的非渐近收敛速度。 据我们所知，它是第一个在控制，非策略，非线性函数逼近，完全分散设置中具有收敛性保证的MARL算法。</li></ul></li><li><p><strong>Variance Reduced Policy Evaluation with Smooth Function Approximation.</strong> Wai, H.-T., Hong, M., Yang, Z., Wang, Z., &amp; Tang, K. (2019). [<a href="https://papers.nips.cc/paper/8814-variance-reduced-policy-evaluation-with-smooth-function-approximation" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>具有平滑和非线性函数逼近的策略评估显示了增强学习的巨大潜力。 与线性函数逼近相比，它允许使用更丰富的逼近函数，例如神经网络。 传统算法基于两个时间尺度随机逼近，其收敛速度通常很慢。 本文着重于离线环境，其中观察到m个状态-动作对的轨迹。 我们将策略评估问题表述为一个非凸的原始对偶有限和优化问题，该问题的原始子问题是非凸的，而对偶子问题是强凹的。 我们建议使用方差减少的单时标原始对偶梯度算法，并证明它使用O（m / \ epsilon）调用（期望）到梯度预言子收敛到\ epsilon平稳点。</li></ul></li><li><p><strong>VIREL: A Variational Inference Framework for Reinforcement Learning.</strong> Fellows, M., Mahajan, A., Rudner, T. G. J., &amp; Whiteson, S. (2019). [<a href="https://papers.nips.cc/paper/8934-virel-a-variational-inference-framework-for-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>将概率模型应用于强化学习（RL）可以使用功能强大的优化工具，例如RL中的变异推理。 然而，现有的推理框架及其算法对学习最佳策略构成了重大挑战，例如，在伪似然方法中缺少模式捕获行为，在基于最大熵RL的方法中难以学习确定性策略，以及在使用函数逼近器时缺乏分析能力 用过的。 我们提出VIREL，这是RL的理论基础概率推断框架，该框架利用参数化的动作值函数来总结潜在MDP的未来动态，从而归纳现有方法。  VIREL还受益于KL散度的一种模式寻求形式，从推理中自然学习确定性最优策略的能力以及在单独的迭代步骤中优化价值功能和政策的能力。 因此，在将变异期望最大化应用于VIREL时，我们证明了行动者批评算法可以简化为期望最大化，其策略改进等效于E步，策略评估等同于M步。 然后，我们从VIREL派生出一系列演员批评方法，包括一种自适应探索方案。 最后，我们证明了该家族的演员批评算法在几个领域中都优于基于软值函数的最新方法。</li></ul></li><li><p><strong>When to Trust Your Model: Model-Based Policy Optimization.</strong> Janner, M., Fu, J., Zhang, M., &amp; Levine, S. (2019). [<a href="https://papers.nips.cc/paper/9416-when-to-trust-your-model-model-based-policy-optimization" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>设计有效的基于模型的强化学习算法非常困难，因为必须权衡数据生成的难易程度与模型生成的数据的偏差。 在本文中，我们从理论上和经验上都研究了模型使用在政策优化中的作用。 我们首先制定并分析基于模型的强化学习算法，并保证每一步的单调改进。 在实践中，这种分析过于悲观，表明实际的非政策数据总是比模型生成的政策数据更可取，但是我们表明，可以将模型概括性的经验估计合并到此类分析中以证明模型的使用是正确的。 受此分析的启发，我们然后证明了使用短模型生成的，从真实数据分支的卷展栏的简单过程，具有基于模型的算法更复杂而没有通常的陷阱的好处。 特别地，这种方法超越了先前基于模型的方法的样本效率，匹配了最佳的无模型算法的渐近性能，并扩展到导致其他基于模型的方法完全失败的地域。</li></ul></li><li><p><strong>When to use parametric models in reinforcement learning?</strong> Van Hasselt, H., London, D., Hessel, M., &amp; Aslanides, J. (2019). [<a href="https://papers.nips.cc/paper/9579-when-to-use-parametric-models-in-reinforcement-learning" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们研究了何时以及如何在强化学习中参数模型最有用的问题。 特别是，我们着眼于参数模型和体验重放之间的共性和差异。 基于重放的学习算法与基于模型的方法具有重要的特征，包括计划能力：使用更多的计算而无需其他数据来改善预测和行为。 我们讨论什么时候可以期望从这两种方法中受益，并在这种情况下解释先前的工作。 我们假设，在适当的条件下，如果仅将模型用于从观察状态生成虚假的过渡，而基于更新的算法则无需模型，则基于重放的算法应比基于模型的算法更具竞争力或更好。 我们在Atari 2600电子游戏上验证了这一假设。 基于重放的算法获得了最先进的数据效率，与参数模型的先前结果相比有所改善。 此外，我们讨论了使用模型的不同方法。 我们表明，使用模型执行信用分配（例如，直接学习价值或政策）时，向后计划比向前计划要好，即使后者似乎更常见。 最后，我们争论并证明，为即时行为而不是信用分配计划是有益的。</li></ul></li><li><p><strong>Worst-Case Regret Bounds for Exploration via Randomized Value Functions.</strong> Russo, D. (2019). [<a href="https://papers.nips.cc/paper/9587-worst-case-regret-bounds-for-exploration-via-randomized-value-functions" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>本文研究了最近的一项建议，即使用随机值函数来驱动强化学习中的探索。 这些随机值函数是通过将随机噪声注入训练数据中生成的，从而使该方法与许多用于估计参数化值函数的流行方法兼容。 通过为表格有限水平马尔可夫决策过程提供最坏情况的后悔约束，我们表明，针对这些随机值函数的规划可以诱导可证明有效的探索。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NIPS2019论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading List" scheme="https://yachenkang.github.io/blog/categories/Reading-List/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SQIL: IMITATION LEARNING VIA REINFORCEMENT LEARNING WITH SPARSE REWARDS</title>
    <link href="https://yachenkang.github.io/blog/2020/06/01/SQIL-IMITATION-LEARNING-VIA-REINFORCEMENT-LEARNING-WITH-SPARSE-REWARDS/"/>
    <id>https://yachenkang.github.io/blog/2020/06/01/SQIL-IMITATION-LEARNING-VIA-REINFORCEMENT-LEARNING-WITH-SPARSE-REWARDS/</id>
    <published>2020-05-31T21:16:39.000Z</published>
    <updated>2020-05-31T22:01:49.458Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>学习模仿演示中的专家行为可能是富有挑战性的，特别是在高维，观察连续以及动态未知的环境中。基于behavioral cloning（BC）的有监督学习方法存在分布偏移的问题：因为智能体贪婪地模仿演示的动作，它可能会由于误差累积而偏离演示的状态。近来基于强化学习（RL）的方法，例如逆强化学习（inverse RL）和生成对抗式模仿学习（GAIL），通过训练RL智能体去匹配长时程的演示来克服这个问题。由于该任务的真正奖励函数是未知的，因此这些方法通常通过使用复杂且脆弱的近似技术来参与对抗训练，从演示中学习奖励函数。我们提出了一个简单的替代方法，该替代方法仍然使用RL，但不需要学习奖励函数。关键思想是通过鼓励智能体在遇到新的、分布之外的状态时返回到演示状态，从而激励他们在很长的时间内匹配演示。为此，我们为智能体提供了在演示状态下匹配演示操作的$r=+1$的恒定奖励，以及对所有其他行为的$r=0$的恒定奖励。我们的方法，我们称为soft Q imitation learning（SQIL），可以通过对任何标准Q-learning或off-policy actor-critic算法进行少量的修改来实现。从理论上讲，我们表明SQIL可以解释为BC利用稀疏先验来鼓励长时程模仿的正则化变体。实验上，我们在Box2D，Atari和MuJoCo中的各种基于图像的以及低维的任务上，SQIL的性能优于BC，与GAIL相比也取得了相近的结果。本文证明了基于RL且具有固定奖励的简单模仿方法与使用学到奖励的更复杂方法一样有效。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><ul><li>作者：Reddy, S., Dragan, A. D., &amp; Levine, S.</li><li>出处：ICLR2020(poster)</li><li>机构：UCB</li><li>关键词：IMITATION LEARNING, RL, SPARSE REWARDS</li><li><a href="https://arxiv.org/abs/1905.11108" target="_blank" rel="noopener">论文链接</a></li></ul><!-- * 开源代码：* 其他资料： --><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_41.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;学习模仿演示中的专家行为可能是富有挑战性的，特别是在高维，观察连续以及动态未知的环境中。基于behavioral cloning（BC）的有监督学习方法存在分布偏移的问题：因为智能体贪婪地模仿演示的动作，它可能会由于误差累积而偏离演示的状态。近来基于强化学习（RL）的方法，例如逆强化学习（inverse RL）和生成对抗式模仿学习（GAIL），通过训练RL智能体去匹配长时程的演示来克服这个问题。由于该任务的真正奖励函数是未知的，因此这些方法通常通过使用复杂且脆弱的近似技术来参与对抗训练，从演示中学习奖励函数。我们提出了一个简单的替代方法，该替代方法仍然使用RL，但不需要学习奖励函数。关键思想是通过鼓励智能体在遇到新的、分布之外的状态时返回到演示状态，从而激励他们在很长的时间内匹配演示。为此，我们为智能体提供了在演示状态下匹配演示操作的$r=+1$的恒定奖励，以及对所有其他行为的$r=0$的恒定奖励。我们的方法，我们称为soft Q imitation learning（SQIL），可以通过对任何标准Q-learning或off-policy actor-critic算法进行少量的修改来实现。从理论上讲，我们表明SQIL可以解释为BC利用稀疏先验来鼓励长时程模仿的正则化变体。实验上，我们在Box2D，Atari和MuJoCo中的各种基于图像的以及低维的任务上，SQIL的性能优于BC，与GAIL相比也取得了相近的结果。本文证明了基于RL且具有固定奖励的简单模仿方法与使用学到奖励的更复杂方法一样有效。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review</title>
    <link href="https://yachenkang.github.io/blog/2020/04/10/Reinforcement-Learning-and-Control-as-Probabilistic-Inference-Tutorial-and-Review/"/>
    <id>https://yachenkang.github.io/blog/2020/04/10/Reinforcement-Learning-and-Control-as-Probabilistic-Inference-Tutorial-and-Review/</id>
    <published>2020-04-10T12:33:52.000Z</published>
    <updated>2020-04-16T02:26:12.999Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>强化学习或最优控制的框架提供了强大且广泛适用的智能决策的数学形式。尽管强化学习问题的一般形式可以对不确定性进行有效的推理，但是强化学习与概率模型中的推理之间的联系并不是很明显。但是，这种联系在算法设计方面具有相当大的价值：将问题形式化为概率推理，从原理上使我们能够使用各种各样的近似推理工具，以灵活强大的方式扩展模型，并了解组成性和部分可观察性的原因。在本文中，我们将讨论强化学习或最优控制问题的泛化（有时称为最大熵强化学习）如何等同于确定性动态情况下的精确概率推理和随机动态情况下的变分推理。我们将介绍此框架的详细信息，概述以此为基础的先前工作及相关思想，以提出新的强化学习和控制算法，并描述对未来研究的看法。<br><a id="more"></a></p><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><ul><li>作者：Sergey Levine</li><li>出处：arXiv</li><li>机构：UC Berkeley</li><li>关键词：</li><li><a href="https://arxiv.org/abs/1805.00909" target="_blank" rel="noopener">论文链接</a></li><li><p>其他资料：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/77099984" target="_blank" rel="noopener">知乎专栏解读</a></li></ul></li><li><p>开源代码：</p></li></ul><h3 id="A-Graphical-Model-for-Control-as-Inference"><a href="#A-Graphical-Model-for-Control-as-Inference" class="headerlink" title="A Graphical Model for Control as Inference"></a>A Graphical Model for Control as Inference</h3><p>在本节中，我们将介绍允许我们将控制嵌入到PGM框架中的基本图模型，并讨论如何使用该框架来派生几种标准强化学习和动态规划方法的变体。本节中介绍的PGM对应于标准强化学习问题的泛化，其中RL目标通过熵项进行了增强。奖励函数的大小在奖励最大化和熵最大化之间进行权衡，从而可以在无限大的奖励限制内恢复原始的RL问题。我们将首先定义符号，然后定义图模型，然后介绍几种推理方法，并描述它们与强化学习和动态规划中的标准算法之间的关系。最后，我们将讨论此方法的一些局限性并由此驱动<a href="#variational-inference-and-stochastic-dynamics">第3节</a>中的变分方法。</p><h4 id="The-Decision-Making-Problem-and-Terminology"><a href="#The-Decision-Making-Problem-and-Terminology" class="headerlink" title="The Decision Making Problem and Terminology"></a>The Decision Making Problem and Terminology</h4><p>首先，我们将介绍用于标准最优控制或强化学习公式的符号表示。我们将使用$\mathbf{s}\in \mathcal{S}$来表示状态，并使用$\mathbf{a}\in \mathcal{A}$来表示动作，均可以是离散的也可以是连续的。状态根据随机动态$p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$演化，这通常是未知的。我们将遵循有限离散时间推导，时长为$T$，并暂时不考虑折扣因子。只需通过修改转移动态就可以很容易地将折扣因子$\lambda$纳入此框架中，这样任何动作都会以$1-\lambda$的概率产生向吸收状态的转移，并且所有标准转移概率都将乘以$\lambda$。</p><p>该框架中的任务可以由奖励函数$r(\mathbf{s}_t,\mathbf{a}_t)$定义。解决任务通常涉及恢复一个策略$p(\mathbf{a}_t|\mathbf{s}_t,\theta)$，该策略指定了在以某些参数向量$\theta$所参数化的状态为条件的动作上的分布。然后，通过以下最大化给出标准强化学习策略搜索问题：</p><script type="math/tex; mode=display">\theta^{\star}=\arg \max _{\theta} \sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim p\left(\mathbf{s}_{t}, \mathbf{a}_{t} | \theta\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \tag{1}</script><p>该优化问题旨在找到一个策略参数向量$\theta$，以使策略的总期望奖励$\sum_t r(\mathbf{s}_t,\mathbf{a}_t)$最大化。期望值是根据策略的<em>轨迹</em>分布$p(\tau)$得出的，由以下公式得出：</p><script type="math/tex; mode=display">p(\tau)=p\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T} | \theta\right)=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} p\left(\mathbf{a}_{t} | \mathbf{s}_{t}, \theta\right) p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) \tag{2}</script><p>为简洁起见，通常将动作条件$p(\mathbf{a}_t|\mathbf{s}_t,\theta)$表示为$\pi_\theta(\mathbf{a}_t|\mathbf{s}_t)$，以强调它是由带参数$\theta$的参数化策略给出的。这些参数可能对应于例如神经网络中的权重。但是，我们也可以通过让$\theta$表示开环规划中的一系列动作来将标准规划问题嵌入该表述中。</p><p>以这种方式形式化了决策问题后，我们要问的将控制推导为推理框架的下一个问题是：我们如何构造概率图模型，以使最可能的轨迹与最优策略的轨迹相对应？或者，等效地，我们如何公式化概率图形模型，以便推断后验动作条件$p(\mathbf{a}_t|\mathbf{s}_t,\theta)$为我们提供最优策略？</p><h4 id="The-Graphical-Model"><a href="#The-Graphical-Model" class="headerlink" title="The Graphical Model"></a>The Graphical Model</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-04-11_Levine_-_2018_-_Reinforcement_Learning_and_Control_83.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>要将控制问题嵌入到图模型中，我们可以简单地通过对状态，动作和下一个状态之间的关系进行建模来开始。这种关系很简单，并且对应于具有公式$p(\mathbf{s}_t+1|\mathbf{s}_t,\mathbf{a}_t)$因子的图模型，如图1(a)所示。但是，这种图模型不足以解决控制问题，因为它没有奖励或成本的概念。因此，我们必须在此模型中引入一个附加变量，我们将其表示为$\mathcal{O}_t$。该附加变量是二进制随机变量，其中$\mathcal{O}_t=1$表示时间步t是<em>最优的</em>，而$\mathcal{O}_t=0$表示它不是最优的。我们将选择由以下方程式给出的该变量的分布：</p><script type="math/tex; mode=display">p\left(\mathcal{O}_{t}=1 | \mathbf{s}_{t}, \mathbf{a}_{t}\right)=\exp \left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \tag{3}</script><p>图1(b)总结了带有这些附加变量的图模型。虽然这乍看起来似乎是一个奇特而随意的选择，但是当我们对所有$t\in\{1,\dots,T\}$都有$\mathcal{O}_t=1$时，它会导致动作的一个非常自然的后验分布：</p><script type="math/tex; mode=display">\begin{aligned}p\left(\tau | \mathbf{o}_{1: T}\right) \propto p\left(\tau, \mathbf{o}_{1: T}\right) &=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} p\left(\mathcal{O}_{t}=1 | \mathbf{s}_{t}, \mathbf{a}_{t}\right) p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) \\&=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} \exp \left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) \\&=\left[p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \exp \left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\end{aligned} \tag{4}</script><p>也就是说，观察到给定轨迹的概率由其根据动态发生的概率（最后一行方括号中的项）与沿着该轨迹的总奖励的指数之间的乘积给出。在具有确定性动态的系统中，最容易理解该方程式，其中第一项对于动态可行的所有轨迹都是常数。在这种情况下，具有最高奖励的轨迹具有最高的概率，而具有较低奖励的轨迹则具有指数级降低的概率。如果我们要规划从某个初始状态$\mathbf{s}_t$开始的最优动作序列，则可以以$\mathbf{o}_{1:T}$为条件，并选择$p(\mathbf{s}_1)=\delta(\mathbf{s}_1)$，在这种情况下，最大后验推断对应于一种规划问题。很容易看出，在动态是确定性的情况下，这恰好与标准规划或轨迹优化相对应，在这种情况下，公式(4)简化为</p><script type="math/tex; mode=display">p\left(\tau | \mathbf{o}_{1: T}\right) \propto \mathbb{1}[p(\tau) \neq 0] \exp \left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \tag{5}</script><p>在这里，指示函数仅指示轨迹$\tau$是动态一致的（意味着$p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)\neq0$）并且初始状态正确。随机动态的情况带来了一些挑战，将在第<a href="#variational-inference-and-stochastic-dynamics">3</a>节中进行详细讨论。但是，即使在确定性动态下，我们通常也有兴趣恢复策略而不是规划。在此PGM中，最优策略可以写为$p(\mathbf{a}_t|\mathbf{s}_t,\mathcal{O}_{t:T}=1)$（为简洁起见，在其余推导中，我们将省略$= 1$）。这种分布与上一节中的$p(\mathbf{a}_t|\mathbf{s}_t,\theta^*)$有点相似，但有两个主要区别：首先，它与参数$\theta$无关，其次，我们将在稍后看到它优化了一个与公式（1）中的标准强化学习目标略微有些不同的目标。</p><h4 id="Policy-Search-as-Probabilistic-Inference"><a href="#Policy-Search-as-Probabilistic-Inference" class="headerlink" title="Policy Search as Probabilistic Inference"></a>Policy Search as Probabilistic Inference</h4><p>我们可以使用标准sum-product推断算法来恢复最优策略$p(\mathbf{a}_t|\mathbf{s}_t,\mathcal{O}_{t:T})$，类似于HMM-style动态贝叶斯网络中的推断。正如我们将在本节中看到的，计算以下形式的反馈（backward）消息就足够了</p><script type="math/tex; mode=display">\beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)</script><p>这些消息具有自然的解释：它们表示从状态$\mathbf{s}_t$和动作$\mathbf{a}_t$开始的$t$到$T$时间步的轨迹是最优的概率（请注意，$\beta_t(\mathbf{s}_t,\mathbf{a}_t)$<em>不是</em>在$\mathbf{s}_t$，$\mathbf{a}_t$的概率密度，而是$\mathcal{O}_{t:T}=1$的概率）。稍微重载符号，我们还将介绍消息</p><script type="math/tex; mode=display">\beta_{t}\left(\mathbf{s}_{t}\right)=p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}\right)</script><p>这些消息表示从状态$\mathbf{s}_t$开始的$t$到$T$的轨迹是最优的概率。我们可以通过对动作积分来从state-action消息中恢复state-only消息：</p><script type="math/tex; mode=display">\beta_{t}\left(\mathbf{s}_{t}\right)=p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}\right)=\int_{\mathcal{A}} p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) p\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) d \mathbf{a}_{t}=\int_{\mathcal{A}} \beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) p\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) d \mathbf{a}_{t}</script><p>因子$p(\mathbf{a}_t|\mathbf{s}_t)$是动作<em>先验</em>。请注意，它绝不是以$\mathcal{O}_{1:T}$为条件：它不表示最优动作的概率，而仅表示动作的先验概率。图1中的PGM实际上不包含该因子，为简单起见我们可以假设$p\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\frac{1}{|\mathcal{A}|}$，也就是说，它是一个常数，对应于一组动作的均匀分布。稍后我们将看到，这种假设实际上并不会带来任何泛化性的损失，因为可以通过奖励函数将任何不均匀的$p(\mathbf{a}_t|\mathbf{s}_t)$代入$p(\mathcal{O}_t|\mathbf{s}_t, \mathbf{a}_t)$。</p><p>用于计算$\beta_t(\mathbf{s}_t,\mathbf{a}_t)$的递归消息传递算法从最后一个时间步$t = T$沿时间回传至$t = 1$。（The recursive message passing algorithm for computing $\beta_t(\mathbf{s}_t,\mathbf{a}_t)$ proceeds from the last time step $t = T$ backward through time to $t = 1$）。在基本情况下，我们注意到$p(\mathcal{O}_T|\mathbf{s}_T, \mathbf{a}_T)$与 $\exp(r(\mathbf{s}_T，\mathbf{a}_T))$成比例，因为只有一个要考虑的因子。 递归的情况如下：</p><script type="math/tex; mode=display">\beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)=\int_{\mathcal{S}} \beta_{t+1}\left(\mathbf{s}_{t+1}\right) p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) p\left(\mathcal{O}_{t} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) d \mathbf{s}_{t+1} \tag{6}</script><p>从这些反馈消息中，我们可以得出最优策略$p(\mathbf{a}_t|\mathbf{s}_t,\mathcal{O}_{t:T})$。 首先，请注意$\mathcal{O}_{1:(t-1)}$在给定$\mathbf{s}_t$时条件独立于$\mathbf{a}_t$，这意味着$p(\mathbf{a}_t|\mathbf{s}_t,\mathcal{O}_{1:T})=p(\mathbf{a}_t|\mathbf{s}_t,\mathcal{O}_{t:T})$，在当前的动作分布下我们可以忽略过去。这很直观：在马尔可夫系统中，最优动作不依赖于过去。由此，我们可以使用两个反馈消息轻松地恢复最优动作分布：</p><script type="math/tex; mode=display">p\left(\mathbf{a}_{t} | \mathbf{s}_{t}, \mathcal{O}_{t: T}\right)=\frac{p\left(\mathbf{s}_{t}, \mathbf{a}_{t} | \mathcal{O}_{t: T}\right)}{p\left(\mathbf{s}_{t} | \mathcal{O}_{t: T}\right)}=\frac{p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) p\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t}\right)}{p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t}\right)} \propto \frac{p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}{p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t}\right)}=\frac{\beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}{\beta_{t}\left(\mathbf{s}_{t}\right)}</script><p>第三步中的条件顺序通过使用贝叶斯公式翻转，并约去分子和分母中的$p(\mathcal{O}_{t:T})$因子。 $p(\mathbf{a}_t|\mathbf{s}_t)$项消失了，因为我们先前假定它是均匀分布。</p><p>这种推导为我们提供了解决方案，但也许不是那么直观。 通过考虑这些方程在对数空间中的作用，可以提供一些直观的角度。 为此，我们将对数空间消息引入为</p><script type="math/tex; mode=display">\begin{aligned}Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) &=\log \beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \\V\left(\mathbf{s}_{t}\right) &=\log \beta_{t}\left(\mathbf{s}_{t}\right)\end{aligned}</script><p>在这里使用$Q$和$V$并非偶然：对数空间消息对应于状态value functions和状态动作value functions的“软”变体。首先，考虑对数空间中的动作边际：</p><script type="math/tex; mode=display">V\left(\mathbf{s}_{t}\right)=\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}</script><p>当$Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$的值较大时，上式表示对于$\mathbf{a}_t$的hard maximum。也就是说，对于较大的$Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$，</p><script type="math/tex; mode=display">V\left(\mathbf{s}_{t}\right)=\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t} \approx \max _{\mathbf{a}_{t}} Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)</script><p>对于较小的$Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$值，maximum是软的。因此，我们可以将$V$和$Q$分别称为soft value functions和Q-functions。 我们还可以考虑对数空间中公式(6)中的backup。对于确定性动态，此backup由下式给出：</p><script type="math/tex; mode=display">Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+V\left(\mathbf{s}_{t+1}\right)</script><p>完全对应于Bellman backup。但是，当动态随机时，backup由下式给出：</p><script type="math/tex; mode=display">Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log E_{\mathbf{s}_{t+1} \sim p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\exp \left(V\left(\mathbf{s}_{t+1}\right)\right)\right] \tag{7}</script><p>该backup是特殊的，因为它不考虑下一个状态的期望value，而是考虑下一个期望value的“soft max”。直观的，这会产生乐观的Q函数：如果在下一个状态的可能结果中，有一个结果具有很高的value，它仍将主导backup，即使在存在其他可能的状态并且具有非常低的value时。 这会产生寻求风险的行为：如果智能体根据此Q函数进行行动，则它所采取的操作可能具有极高的风险，只要它们具有一定的非零概率可以获得较高的回报。 显然，这种行为在许多情况下不是我们所希望的，并且本节中描述的标准PGM通常不太适合于随机动态。 在第<a href="#variational-inference-and-stochastic-dynamics">3</a>节中，我们将描述一个简单的修改，它通过使用变分推理的框架，使backup与随机动态情况下的soft Bellman backup相对应。</p><h4 id="Which-Objective-does-This-Inference-Procedure-Optimize"><a href="#Which-Objective-does-This-Inference-Procedure-Optimize" class="headerlink" title="Which Objective does This Inference Procedure Optimize?"></a>Which Objective does This Inference Procedure Optimize?</h4><p>在上一节中，我们导出了一个推理范式，该范式可用于获取以所有最优变量$p(\mathbf{a}_t|\mathbf{s}_t,\mathcal{O}_{1:T})$为条件的动作的分布。 但是，该策略实际上优化了哪个目标？ 回想一下，总体分布由下式给出：</p><script type="math/tex; mode=display">p(\tau)=\left[p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \exp \left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \tag{8}</script><p>在确定性动态的情况下，我们可以将其简化为公式(5)。 在这种情况下，条件分布$p(\mathbf{a}_t|\mathbf{s}_t,\mathcal{O}_{1:T})$可以通过边际化整个轨迹分布并在$\mathbf{s}_t$的每个时间步调整策略来简单地获得。我们可以针对此问题采用基于优化的近似推断方法，在这种情况下，目标是拟合一个近似值$\pi(\mathbf{a}_t|\mathbf{s}_t)$，以使轨迹分布</p><script type="math/tex; mode=display">\hat{p}(\tau) \propto \mathbb{1}[p(\tau) \neq 0] \prod_{t=1}^{T} \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)</script><p>匹配公式(5)中的分布。如上一节所述，在精确推理的情况下，匹配是精确的，这意味着$D_{KL}(\hat{p}(\tau)||p(\tau))=0$，其中$D_{KL}$是KL散度。 因此，我们可以将推理过程视为最小化$D_{KL}(\hat{p}(\tau)||p(\tau))$，这由下式给出：</p><script type="math/tex; mode=display">D_{\mathrm{KL}}(\hat{p}(\tau) \| p(\tau))=-E_{\tau \sim \hat{p}(\tau)}[\log p(\tau)-\log \hat{p}(\tau)]</script><p>将等式两边取反，并代入$p(\tau)$和$\hat{p}(\tau)$，我们得到</p><script type="math/tex; mode=display">\begin{aligned}-D_{\mathrm{KL}}(\hat{p}(\tau) \| p(\tau))&= E_{\tau \sim \hat{p}(\tau)}[\log p\left(\mathbf{s}_{1}\right)+\sum_{t=1}^{T}\left(\log p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)+r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \\&\qquad\qquad-\log p\left(\mathbf{s}_{1}\right)-\sum_{t=1}^{T}\left(\log p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)]\\&=E_{\tau \sim \hat{p}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right] \\&=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \hat{p}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right] \\&=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \hat{p}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+E_{\mathbf{s}_{t} \sim \hat{p}\left(\mathbf{s}_{t}\right)}\left[\mathcal{H}\left(\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\right]\end{aligned}</script><p>因此，与等式(1)中的标准控制目标（仅使奖励最大化）相反，使KL散度最小对应于使预期奖励<em>和</em>预期条件熵最大化。 因此，这种控制目标有时被称为最大熵强化学习或最大熵控制。</p><p>但是，在随机动态的情况下，解决方案并不是那么简单。 在随机动态下，最优分布由下式给出：</p><script type="math/tex; mode=display">\hat{p}(\tau)=p\left(\mathbf{s}_{1} | \mathcal{O}_{1: T}\right) \prod_{t=1}^{T} p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}, \mathcal{O}_{1: T}\right) p\left(\mathbf{a}_{t} | \mathbf{s}_{t}, \mathcal{O}_{1: T}\right) \tag{9}</script><p>其中初始状态分布和动态<em>也</em>以最优为条件。因此，KL散度中的动态和初始状态项<em>不会</em>抵消，并且目标也没有上面导出的简单熵最大化形式。（在确定性情况下，我们知道$p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t,\mathcal{O}_{1:T})=p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$，因为只有一种转移是可能的。）我们仍然可以在轨迹层面依靠原始的KL散度最小化，并将目标写为</p><script type="math/tex; mode=display">-D_{\mathrm{KL}}(\hat{p}(\tau) \| p(\tau))=E_{\tau \sim \hat{p}(\tau)}\left[\log p\left(\mathbf{s}_{1}\right)+\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+\mathcal{H}(\hat{p}(\tau)) \tag{10}</script><p>但是，由于$\log p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$项，在无模型设置中很难优化该目标。 如前一部分所述，它还导致了一种乐观的策略，其假定对动态的某种程度的控制在大多数控制问题中都是不现实的。在第<a href="#variational-inference-and-stochastic-dynamics">3</a>节中，我们将推导出一个变分推理范式，即使在随机动态的情况下，该范式也可简化为方程式(9)中的便捷目标，并且在此过程中，还将处理第<a href="#policy-search-as-probabilistic-inference">2.3</a>节中讨论的风险偏好行为。</p><h4 id="Alternative-Model-Formulations"><a href="#Alternative-Model-Formulations" class="headerlink" title="Alternative Model Formulations"></a>Alternative Model Formulations</h4><p>值得指出的是，等式(3)中$p\left(\mathcal{O}_{t}=1 | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$的定义需要一个额外的假设，即奖励$r(\mathbf{s}_{t}, \mathbf{a}_{t})$始终为负。（这个假设实际上并不是很强：如果我们假设奖励有上界，那么我们总是可以简单地通过减去最大奖励来构建完全等效的奖励。）否则，我们最终得到$p\left(\mathcal{O}_{t}=0 | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$的负概率。但是，实际上并不需要此假设：完全有可能用$(\mathbf{s}_{t}, \mathbf{a}_{t},\mathcal{O}_{t})$上的无向因子以及通过$\Phi_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}, \mathcal{O}_{t}\right)=\mathbf{1}_{\mathcal{O}_{t}=1} \exp \left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)$给出未归一化的可能性定义图模型。$\mathcal{O}_{t}=0$的可能性无关紧要，因为我们始终以$\mathcal{O}_{t}=1$为条件。这导致了与上述相同的精确推理范式，但没有负奖励假设。一旦我们满足于使用无向图模型，我们甚至可以完全删除变量$\mathcal{O}_{t}$，只需在$(\mathbf{s}_{t}, \mathbf{a}_{t})$上简单添加可能性为$\Phi_{t}(\mathbf{s}_{t}, \mathbf{a}_{t})=\exp \left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)$的无向因子，这在数学上是等效的。 这是Ziebart（Ziebart，2010）描述的条件随机场公式。 该模型中的分析和推理方法与针对具有显式最优变量$\mathcal{O}_{t}$的有向模型的分析和推理方法相同，并且模型的特定选择只是一种符号上的便利。我们将在本文中使用变量$\mathcal{O}_{t}$来简化推导过程，并将其保留在有向图模型框架内，但是所有推导都可以在条件随机场公式中直接再现。</p><p>此框架的另一个常见修改是将显式温度$\alpha$纳入$\mathcal{O}_{t}$的CPD中，以使$p\left(\mathcal{O}_{t} | \mathbf{s}_{t}, b\mathbf{a}_{t}\right)= \exp(\frac 1\alpha (\mathbf{s}_{t}, \mathbf{a}_{t}))$。 然后，可以将相应的最大熵目标等效地写为（原始）奖励的期望，并在熵项上附加乘数$\alpha$。这提供了在熵最大化和标准最优控制或RL之间进行插值的自然机制：当$\alpha\to0$时，最优解接近标准最优控制解。注意，这实际上并没有增加方法的泛化性，因为常数$\frac 1\alpha$总是可以乘以奖励，但是明确指定此温度常数可以帮助阐明标准与熵最大化最优控制之间的联系。</p><p>最后，值得再次提及折扣因子的作用：在强化学习中，使用如下形式的Bellman backup非常普遍</p><script type="math/tex; mode=display">Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \leftarrow r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma E_{\mathbf{s}_{t+1} \sim p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[V\left(\mathbf{s}_{t+1}\right)\right]</script><p>其中$\gamma \in(0,1]$是折扣因子。这允许在infinite-horizon的设置中学习value functions（否则对于$\gamma= 1$，backup将是不收敛的），并减小了策略梯度算法中Monte Carlo advantage estimators的方差（  Schulman et al。，2016）。折扣因子可以看作是系统动态的简单重新定义。如果初始动态由$p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$给出，则增加折扣因子等于未折扣的value拟合修改的动态$\bar{p}\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)=\gamma p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$，这时无论动作如何，都存在概率为$1-\gamma$的额外转移到奖励为零的吸收状态。我们将在本文的推导中忽略$\gamma$，但只要在出现$p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$期望值的任何地方通过修改(soft) Bellman backup在任何情况下简单地插入它，例如公式(7)或下一节中的公式(15)。</p><h3 id="Variational-Inference-and-Stochastic-Dynamics"><a href="#Variational-Inference-and-Stochastic-Dynamics" class="headerlink" title="Variational Inference and Stochastic Dynamics"></a>Variational Inference and Stochastic Dynamics</h3><p>在第<a href="#policy-search-as-probabilistic-inference">2.3</a>节和第<a href="#which-objective-does-this-inference-procedure-optimize">2.4</a>节中讨论的在随机动态情况下最大熵框架的问题性质实质上等于一个假设，即允许智能体控制其行为和系统动态以产生最优轨迹，但是其对动态的权限会根据与真实动态的偏差而受到惩罚。因此，可以从等式中分解出等式(10)中的$\log p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$项，从而产生additive terms对应于后验动态$p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t},\mathcal{O}_{1:T}\right)$与真实动态$p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$之间的交叉熵。 这解释了第<a href="#policy-search-as-probabilistic-inference">2.3</a>节中讨论的方法的风险偏好性：如果允许该智能体影响其动态，甚至仅是一点，它都将合理地选择消除风险动作的不太可能但极其糟糕的结果。</p><p>当然，在实际的强化学习和控制问题中，系统动态的这种操纵是不可能的，并且由此产生的策略可能导致灾难性的不良后果。我们可以通过修改推理过程来矫正此问题。在本节中，我们将通过固定系统动态，写下相应的最大熵目标并推导用于对其进行优化的动态规划过程来得出此矫正。 然后，我们将证明该过程相当于结构化变分推理的直接应用。</p><h4 id="Maximum-Entropy-Reinforcement-Learning-with-Fixed-Dynamics"><a href="#Maximum-Entropy-Reinforcement-Learning-with-Fixed-Dynamics" class="headerlink" title="Maximum Entropy Reinforcement Learning with Fixed Dynamics"></a>Maximum Entropy Reinforcement Learning with Fixed Dynamics</h4><p>在<a href="#which-objective-does-this-inference-procedure-optimize">2.4</a>节中讨论的随机动态问题可以简单地概括如下：由于后验动态分布$p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t},\mathcal{O}_{1:T}\right)$不一定与真实动态$p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$匹配，智能体假定它可以在一定程度上影响动态。解决此问题的一个简单方法是通过强制后验动态和初始状态分布分别匹配$p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)$和$p\left(\mathbf{s}_{t}\right)$来明确禁止此控制。然后，简单地给出优化后的轨迹分布： </p><script type="math/tex; mode=display">\hat{p}(\tau)=p\left(\mathrm{s}_{1}\right) \prod_{t=1}^{T} p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)</script><p>与第<a href="#which-objective-does-this-inference-procedure-optimize">2.4</a>节中介绍的确定性情况的推导相同，得出以下目标：</p><script type="math/tex; mode=display">-D_{\mathrm{KL}}(\hat{p}(\tau) \| p(\tau))=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \hat{p}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\mathcal{H}\left(\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\right] \tag{11}</script><p>也就是说，目标仍然是使奖励和熵最大化，但是现在处于随机转移动态之下。为了优化此目标，我们可以像在<a href="#policy-search-as-probabilistic-inference">2.3</a>节中一样计算反馈消息。但是，由于我们现在是从最大化等式（11）中的目标开始的，因此必须作为动态规划算法从优化的角度导出这些反馈消息。和以前一样，我们将从优化$\pi(\mathbf{a}_{t}| \mathbf{s}_{t})$这样的基本情况开始，其最大化</p><script type="math/tex; mode=display">\begin{aligned}&E_{\left(\mathbf{s}_{T}, \mathbf{a}_{T}\right) \sim \hat{p}\left(\mathbf{s}_{T}, \mathbf{a}_{T}\right)}\left[r\left(\mathbf{s}_{T}, \mathbf{a}_{T}\right)-\log \pi\left(\mathbf{a}_{T} | \mathbf{s}_{T}\right)\right]=\\&E_{\mathrm{s}_{T} \sim \hat{p}\left(\mathrm{s}_{T}\right)}\left[-D_{\mathrm{KL}}\left(\pi\left(\mathrm{a}_{T} | \mathrm{s}_{T}\right) \| \frac{1}{\exp \left(V\left(\mathrm{s}_{T}\right)\right)} \exp \left(r\left(\mathrm{s}_{T}, \mathrm{a}_{T}\right)\right)\right)+V\left(\mathrm{s}_{T}\right)\right]\end{aligned} \tag{12}</script><p>从KL散度的定义来看，等式成立，而$\exp(V(\mathbf{s}_T))$是关于$\mathbf{a}_T$的$\exp \left(r\left(\mathbf{s}_{T}, \mathbf{a}_{T}\right)\right)$的归一化常数，其中$V\left(\mathbf{s}_{T}\right)=\log \int_{\mathcal{A}} \exp \left(r\left(\mathbf{s}_{T}, \mathbf{a}_{T}\right)\right) d \mathbf{a}_{T}$，与<a href="#policy-search-as-probabilistic-inference">2.3</a>节中的软最大化相同。由于我们知道当两个参数表示相同的分布时，KL散度被最小化，因此最优策略为</p><script type="math/tex; mode=display">\pi\left(\mathbf{a}_{T} | \mathbf{s}_{T}\right)=\exp \left(r\left(\mathbf{s}_{T}, \mathbf{a}_{T}\right)-V\left(\mathbf{s}_{T}\right)\right) \tag{13}</script><p>然后可以按以下方式计算递归情况：对于给定的时间步$t$，$\pi(\mathbf{a}_t|\mathbf{s}_t)$必须最大化两个项：</p><script type="math/tex; mode=display">E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \hat{p}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right]+E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \hat{p}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[E_{\mathbf{s}_{t+1} \sim p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[V\left(\mathbf{s}_{t+1}\right)\right]\right] \tag{14}</script><p>第一项直接从等式(11)中的目标得出，而第二项表示$\pi(\mathbf{a}_t|\mathbf{s}_t)$对所有后续时间步的期望的贡献。第二项值得更深入的推导。首先，考虑一个基本情况：给定方程式(13)中的$\pi(\mathbf{a}_T|\mathbf{s}_T)$方程，我们可以通过将方程式直接代入方程式(12)来评估策略目标。由于KL散度计算为零，因此只剩下$V(\mathbf{s}_T)$项。在递归情况下，我们注意到我们可以将等式(14)中的目标重写为</p><script type="math/tex; mode=display">\begin{aligned}&E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \hat{p}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right]+E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \hat{p}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[E_{\mathbf{s}_{t+1} \sim p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[V\left(\mathbf{s}_{t+1}\right)\right]\right]=\\&E_{\mathbf{s}_{t} \sim \hat{p}\left(\mathbf{s}_{t}\right)}\left[-D_{\mathbf{K L}}\left(\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \| \frac{1}{\exp \left(V\left(\mathbf{s}_{t}\right)\right)} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right)+V\left(\mathbf{s}_{t}\right)\right]\end{aligned}</script><p>现在我们定义</p><script type="math/tex; mode=display">\begin{aligned}Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) &=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+E_{\mathbf{s}_{t+1} \sim p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[V\left(\mathbf{s}_{t+1}\right)\right] \\V\left(\mathbf{s}_{t}\right) &=\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}\end{aligned} \tag{15}</script><p>它对应于标准的具有针对value function软最大化的Bellman backup。选择</p><script type="math/tex; mode=display">\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-V\left(\mathbf{s}_{t}\right)\right) \tag{16}</script><p>我们再次看到KL散度计算为零，就像在$t=T$的基本情况下一样，$E_{\mathbf{s}_{t} \sim \hat{p}}\left(\mathbf{s}_{t}\right)\left[V\left(\mathbf{s}_{t}\right)\right]$作为时间步$t$的目标中唯一剩下的项。这意味着，如果我们固定动态和初始状态分布，并且只允许更改策略，我们将恢复Bellman backup运算，其使用下一个状态的期望值而不是在第<a href="#policy-search-as-probabilistic-inference">2.3</a>节中看到的乐观估计（比较式(15)与式(7)）。尽管这为寻求风险的策略的实际问题提供了解决方案，但它与概率图模型的便利框架之间的差异也许有点令人不满意。在下一部分中，我们将讨论此过程如何构成结构化变分推理的直接应用。</p><h4 id="Connection-to-Structured-Variational-Inference"><a href="#Connection-to-Structured-Variational-Inference" class="headerlink" title="Connection to Structured Variational Inference"></a>Connection to Structured Variational Inference</h4><p>第<a href="#maximum-entropy-reinforcement-learning-with-fixed-dynamics">3.1</a>节中解释优化过程的一种方法是将其作为结构化变分推断的一种特殊类型。在结构化变分推理中，我们的目标是用另一个可能更简单的分布$q(y)$近似某个分布$p(y)$。通常，$q(y)$被视为某种易于处理的因式分布，例如链或树中连接的条件分布的乘积，这有助于进行易处理的精确推理。在我们的例子中，我们的目标是逼近$p(\tau)$，由下式给出</p><script type="math/tex; mode=display">p(\tau)=\left[p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \exp \left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \tag{17}</script><p>通过下面的分布来逼近</p><script type="math/tex; mode=display">q(\tau)=q\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} q\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \tag{18}</script><p>如果我们固定$q(\mathbf{s}_1)=p(\mathbf{s}_1)$且$q(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)=p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$，则$q(\tau)$恰好是第<a href="#maximum-entropy-reinforcement-learning-with-fixed-dynamics">3.1</a>节中的分布$\hat p(\tau)$，在这里我们将其重命名为$q(\tau)$，以强调与结构化变分推理的联系。请注意，出于相同的原因，我们也将$\pi(\mathbf{a}_t|\mathbf{s}_t)$重命名为$q(\mathbf{a}_t|\mathbf{s}_t)$。在结构化变分推理中，近似推理是通过优化变分下界（也称为evidence lower bound）来执行的。回想一下，这里的evidence是，对于所有$t\in{1,\dots,T}$，都有$\mathcal{O}_t=1$，以及后验是以初始状态$\mathbf{s}_1$为条件。变分下界由下式给出</p><script type="math/tex; mode=display">\begin{aligned}\log p\left(\mathcal{O}_{1: T}\right) &=\log \iint p\left(\mathcal{O}_{1: T}, \mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) d \mathbf{s}_{1: T} d \mathbf{a}_{1: T} \\&=\log \iint p\left(\mathcal{O}_{1: T}, \mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \frac{q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)}{q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)} d \mathbf{s}_{1: T} d \mathbf{a}_{1: T} \\&=\log E_{\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \sim q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)}\left[\frac{p\left(\mathcal{O}_{1: T}, \mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)}{q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)}\right] \\& \geq E_{\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \sim q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)}\left[\log p\left(\mathcal{O}_{1: T}, \mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)-\log q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)\right]\end{aligned}</script><p>最后一行的不等式是通过詹森不等式获得的。通过等式(17)和(18)中的定义替换$p(\tau)$和$q(\tau)$，并注意由于$q(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)=p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$而引起的抵消 ，下界减小到</p><script type="math/tex; mode=display">\log p\left(\mathcal{O}_{1: T}\right) \geq E_{\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \sim q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right] \tag{19}</script><p>取决于一个加性常数。针对策略$q(\mathbf{a}_t|\mathbf{s}_t)$优化此目标正好对应于公式(11)中的目标。直观来讲，这意味着该目标试图找到与最大熵轨迹分布最接近的匹配，但要遵循这样的约束，即仅允许智能体修改策略，而不允许修改动态。请注意，此框架还可以轻松适应策略的任何其他结构性约束，包括对特定分布类别的约束（例如，条件高斯或由神经网络参数化的分类分布），或对部分可观察性的约束，其中整个状态$\mathbf{s}_t$不能用作输入，而策略只能访问该状态的某些不可逆函数。</p><h3 id="Approximate-Inference-with-Function-Approximation"><a href="#Approximate-Inference-with-Function-Approximation" class="headerlink" title="Approximate Inference with Function Approximation"></a>Approximate Inference with Function Approximation</h3><p>我们在上面的讨论中看到，具有类似于Bellman backup的更新的动态规划反馈算法可以在最大熵强化学习框架中恢复value function和Q函数的“软”类似物，并且可以从value function和Q函数中恢复出随机最优策略。在本节中，我们将讨论如何使用函数逼近从该理论框架中得出针对高维或连续强化学习问题的实用算法。 这将产生一些反映标准强化学习中相应技术的原型方法：policy gradients，actor-critic算法和Q-learning。</p><h4 id="Maximum-Entropy-Policy-Gradients"><a href="#Maximum-Entropy-Policy-Gradients" class="headerlink" title="Maximum Entropy Policy Gradients"></a>Maximum Entropy Policy Gradients</h4><p>一种执行结构化变分推理的方法是直接优化关于变分分布的evidence lower bound（Koller和Friedman，2009）。这种方法可以直接应用于最大熵强化学习。注意，变异分布由三个项组成：$q(\mathbf{s}_1)$，$q(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$和$q(\mathbf{a}_t|\mathbf{s}_t)$。 前两个项分别固定为$p(\mathbf{s}_1)$和$p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$，仅剩下$q(\mathbf{a}_t|\mathbf{s}_t)$可以变化。我们可以使用任何具有参数$\theta$的表达条件来对该分布进行参数化，因此将其表示为$q_\theta(\mathbf{a}_t|\mathbf{s}_t)$。 这些参数可以对应于例如深度神经网络中的权重，其将$\mathbf{s}_t$作为输入并输出某些分布类别的参数。在离散动作的情况下，网络可以直接输出分类分布的参数（例如，通过soft max运算）。在连续动作的情况下，网络可以输出指数族分布的参数，例如高斯分布。在所有情况下，我们都可以通过使用样本估算目标的梯度来直接优化方程(11)中的目标。这种梯度的形式几乎与标准策略梯度（Williams，1992）相同，为完整起见，我们在此对其进行总结。 首先，让我们将目标重述如下：</p><script type="math/tex; mode=display">J(\theta)=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\mathcal{H}\left(q_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\right]</script><p>梯度由下式给出</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\theta} J(\theta) &=\sum_{t=1}^{T} \nabla_{\theta} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\mathcal{H}\left(q_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\right] \\&=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\nabla_{\theta} \log q_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\log q_{\theta}\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)-1\right)\right] \\&=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\nabla_{\theta} \log q_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\log q_{\theta}\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)-b\left(\mathbf{s}_{t^{\prime}}\right)\right)\right]\end{aligned}</script><p>其中第二行来自似然比技巧（Williams，1992）和熵的定义，以获得对数$\log q_\theta(\mathbf{a}_{t’}|\mathbf{s}_{t’})$项。-1来自熵项的导数。 最后一行指出梯度估计器对于加法状态相关常数不变，并用状态相关基线b（st’）代替-1。 所得策略梯度估计量与标准策略梯度估计量完全匹配，唯一的修改是在每个时间步长t’处将-logqθ（at’| st’）项添加到奖励中。 直观地，通过减去当前策略下该操作的对数概率来修改每个操作的报酬，这会使该策略最大化熵。 该梯度估计量可以紧凑地写成</p><script type="math/tex; mode=display">\nabla_{\theta} J(\theta)=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\nabla_{\theta} \log q_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \hat{A}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]</script><p>其中Aˆ（st，at）是一个优势估算器。 可以使用任何标准优势估算器，例如GAE估算器（Schulman等人，2016）来代替上述标准基线蒙特卡罗收益率。 同样，唯一必要的修改是在每个时间步长t’处将-logqθ（at’| st’）添加到奖励中。 与标准策略梯度一样，此方法的实际实现通过从当前策略中采样轨迹来估计期望值，并且可以通过遵循自然梯度方向进行改进。</p><h4 id="Maximum-Entropy-Actor-Critic-Algorithms"><a href="#Maximum-Entropy-Actor-Critic-Algorithms" class="headerlink" title="Maximum Entropy Actor-Critic Algorithms"></a>Maximum Entropy Actor-Critic Algorithms</h4><p>我们可以采用一种消息传递方法，而不是直接区分变化的下限，这将在后面看到，它可以产生较低方差的梯度估计。 首先，请注意，对于q（at | st）的最优目标分布，我们可以写下以下等式：</p><script type="math/tex; mode=display">q^{\star}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\frac{1}{Z} \exp \left(E_{q\left(\mathbf{s}_{(t+1): r}, \mathbf{a}_{(t+1) ; T} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\sum_{t^{\prime}=t}^{T} \log p\left(\mathcal{O}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\sum_{t^{\prime}=t+1}^{T} \log q\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)\right]\right)</script><p>这是因为基于st的条件使动作处于完全独立于所有过去状态的状态，但是动作仍然取决于所有将来的状态和动作。 请注意，动力学项p（st + 1 | st，at）和q（st + 1 | st，at）不会出现在上述方程式中，因为它们会完美抵消。 我们可以简化以下期望：</p><script type="math/tex; mode=display">\begin{aligned}&E_{q\left(\mathbf{s}_{(t+1): T}, \mathbf{a}_{(t+1): T} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)\left[\log p\left(\mathcal{O}_{t: T} | \mathbf{s}_{t: T}, \mathbf{a}_{t: T}\right)\right]}=\\&\log p\left(\mathcal{O}_{t} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)+E_{q\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[E\left[\sum_{t^{\prime}=t+1}^{T} \log p\left(\mathcal{O}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\log q\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)\right]\right]\end{aligned}</script><p>在这种情况下，请注意内部期望不包含st或at，因此自然地表示了可以从将来状态发送的消息。 我们将表示此消息V（st + 1），因为它将与软值函数相对应：</p><script type="math/tex; mode=display">\begin{aligned}V\left(\mathbf{s}_{t}\right) &=E\left[\sum_{t^{\prime}=t+1}^{T} \log p\left(\mathcal{O}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\log q\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)\right] \\&=E_{q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\log p\left(\mathcal{O}_{t} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)+E_{q\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right.}\left[V\left(\mathbf{s}_{t+1}\right)\right]\right]\end{aligned}</script><p>为了方便起见，我们还可以将Q函数定义为</p><script type="math/tex; mode=display">Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\log p\left(\mathcal{O}_{t} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)+E_{q\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[V\left(\mathbf{s}_{t+1}\right)\right]</script><p>使得V（st）= Eq（at | st）[Q（st，at）-log q（at | st）]，最优策略为</p><script type="math/tex; mode=display">q^{\star}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\frac{\exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)}{\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}}</script><p>注意，在这种情况下，与动态编程的情况相比，值函数和Q函数对应于当前策略q（at | st）的值，而不是最优值函数和Q函数。 然而，在收敛时，当每个t的q（at | st）=q⋆（at | st）时，我们有</p><script type="math/tex; mode=display">\begin{aligned}V\left(\mathbf{s}_{t}\right) &=E_{q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right] \\&=E_{q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}\right] \\&=\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}\end{aligned}</script><p>这是2.3节中常见的最大柔度。 现在我们看到，可以通过将消息向后传递时间来计算q（at | st）的最优变化分布，并且消息由V（st）和Q（st，at）给出。</p><p>到目前为止，此推导假定可以准确表示策略和消息。 我们可以像上一节中一样放松第一个假设。 我们首先写下单个因子q（at | st）的变化下界，如下所示：</p><script type="math/tex; mode=display">\max _{q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} E_{\mathbf{s}_{t} \sim q\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right]\right]</script><p>显而易见，这个目标只是完全的变分下界，它由Eq（τ）[log p（）-log q（）]给出，只限于包含q（at | st）的项 。 如果我们限制策略的类别q（at | st）以使其不能精确地表示q⋆（at | st），我们仍然可以通过计算公式（22）的梯度来优化公式（22）中的目标</p><script type="math/tex; mode=display">E_{\mathbf{s}_{t} \sim q\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\nabla \log q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)-b\left(\mathbf{s}_{t}\right)\right)\right]\right]</script><p>其中b（st）是任何与状态有关的基线。 可以使用来自q（）的样本来计算此梯度，并且像上一节中的策略梯度一样，它直接类似于经典似然比策略梯度。 修改在于使用反向消息Q（st，at）代替蒙特卡洛优势估计。 因此，该算法对应于参与者评论算法，该算法通常提供较低的方差梯度估计。</p><p>为了将其转化为实用的算法，我们还必须能够近似估算后向消息Q（st，at）和V（st）。 一种简单明了的方法是用参数化函数Qφ（st，at）和Vψ（st）以及参数和来表示它们，并优化参数以最小化平方误差目标</p><script type="math/tex; mode=display">\mathcal{E}(\phi)=E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+E_{q\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[V_{\psi}\left(\mathbf{s}_{t+1}\right)\right]-Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)^{2}\right]</script><script type="math/tex; mode=display">\mathcal{E}(\psi)=E_{\mathbf{s}_{t} \sim q\left(\mathbf{s}_{t}\right)}\left[\left(E_{\mathbf{a}_{t} \sim q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right]-V_{\psi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)^{2}\right]</script><p>这种解释为最大熵actor-critic和策略迭代算法带来了一些有趣的可能性。 首先，它表明跟踪V（st）和Q（st，at）网络都是有益的。 这在消息传递框架中是完全合理的，并且在实践中可能具有与使用目标网络相同的许多好处，在目标网络中，Q和V的更新可能会交错或受阻以保持稳定性。 其次，这表明策略迭代或参与者批评方法可能是首选方法（例如，优于直接Q学习），因为它们显式地处理结构化变分近似中的近似消息和近似因子。 这正是软角色批评算法所采用的方案（Haarnoja et al。，2018b）。</p><h4 id="Soft-Q-Learning"><a href="#Soft-Q-Learning" class="headerlink" title="Soft Q-Learning"></a>Soft Q-Learning</h4><p>我们可以导出强化学习算法的另一种形式，而无需使用显式的策略参数化，仅拟合消息Qφ（st，at）。 在这种情况下，我们假设值函数V（st）和策略q（at | st）都隐式参数化，其中</p><script type="math/tex; mode=display">V\left(\mathbf{s}_{t}\right)=\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}</script><p>如公式（21）所示，以及</p><script type="math/tex; mode=display">q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-V\left(\mathbf{s}_{t}\right)\right)</script><p>它直接对应于等式（20）。 在这种情况下，不需要除Qφ（st，at）以外的其他参数，这可以通过最小化公式（23）中的误差，用隐式公式替换V（st）来代替Vψ（st）来学习。 我们可以将结果梯度更新写为</p><script type="math/tex; mode=display">\phi \leftarrow \phi-\alpha E\left[\frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\left(Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log \int_{\mathcal{A}} \exp \left(Q\left(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}\right)\right) d \mathbf{a}_{t+1}\right)\right)\right]</script><p>值得指出的是与标准Q学习更新的相似之处：</p><script type="math/tex; mode=display">\left.\phi \leftarrow \phi-\alpha E\left[\frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\left(Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\max _{\mathbf{a}_{t+1}} Q_{\phi}\left(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}\right)\right)\right)\right)\right]</script><p>如果标准Q学习更新的最大值大于1，则软Q学习更新的最大值为“ soft”。 随着奖励幅度的增加，软更新类似于硬更新。在离散操作的情况下，此更新易于实现，因为积分被总和取代，并且可以通过标准化Q函数简单地提取策略。<br>  在连续动作的情况下，需要进一步的近似值来使用样本评估积分。 正如Haarnoja等人所讨论的那样，从隐式策略中进行采样也是很重要的，并且需要一个近似的推理过程。  （Haarnoja et al。，2017）。</p><p>我们可以进一步使用该框架来说明软Q学习与策略梯度之间的有趣联系。 根据完全由Qφ（st，at）定义的等式（20）中策略的定义，我们可以从策略梯度开始得出替代梯度。 此推导表示策略梯度和Q学习之间的联系，这种联系在标准框架中并不明显，但在最大熵框架中却显而易见。 全部推导由Haarnoja等人提供。  （Haarnoja et al。，2017）（附录B）。<br>   最终梯度对应于</p><script type="math/tex; mode=display">\nabla_{\phi} J(\phi)=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\left(\nabla_{\phi} Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\nabla_{\phi} V\left(\mathbf{s}_{t}\right)\right) \hat{A}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]</script><p>软Q学习梯度可以等效地写为</p><script type="math/tex; mode=display">\nabla_{\phi} J(\phi)=\sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[\nabla_{\phi} Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \hat{A}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]</script><p>利用我们可以使用任何依赖于状态的基线这一事实，我们用目标值r（st，at）+ V（st + 1）代替Aˆ（st，at）。 尽管这些梯度并不完全相等，但附加项-∇φV（st）仅说明以下事实：仅策略梯度不足以解决Q（st，at）的一个额外自由度： 与动作无关的常量。 如果我们将政策梯度与V（st）的Bellman误差最小化相加，则可以消除该项</p><script type="math/tex; mode=display">\nabla_{\phi} V\left(\mathbf{s}_{t}\right) E_{\mathbf{a}_{t} \sim q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+E_{\mathbf{s}_{t+1} \sim q\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[V\left(\mathbf{s}_{t+1}\right)\right]\right]=\nabla_{\phi} V\left(\mathbf{s}_{t}\right) E_{\mathbf{a}_{t} \sim q\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\hat{Q}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]</script><p>注意到ˆQ（st，at）只是一个（非基线）回报估计，我们可以证明，对于特定于状态相关基线的选择，策略梯度和值梯度的总和与方程式（24）完全匹配。 项∇φV（st）Eat〜q（at | st）[ˆQ（st，at）]抵消了项∇φV（st）ˆ A（st，at）在when A（st，at）时的期望 ）= ˆQ（st，at）（也就是说，当我们使用零基准时）。 这就完成了软Q学习与策略梯度之间一般等效的证明。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;强化学习或最优控制的框架提供了强大且广泛适用的智能决策的数学形式。尽管强化学习问题的一般形式可以对不确定性进行有效的推理，但是强化学习与概率模型中的推理之间的联系并不是很明显。但是，这种联系在算法设计方面具有相当大的价值：将问题形式化为概率推理，从原理上使我们能够使用各种各样的近似推理工具，以灵活强大的方式扩展模型，并了解组成性和部分可观察性的原因。在本文中，我们将讨论强化学习或最优控制问题的泛化（有时称为最大熵强化学习）如何等同于确定性动态情况下的精确概率推理和随机动态情况下的变分推理。我们将介绍此框架的详细信息，概述以此为基础的先前工作及相关思想，以提出新的强化学习和控制算法，并描述对未来研究的看法。&lt;br&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Option Discovery using Deep Skill Chaining</title>
    <link href="https://yachenkang.github.io/blog/2020/03/20/Option-Discovery-using-Deep-Skill-Chaining/"/>
    <id>https://yachenkang.github.io/blog/2020/03/20/Option-Discovery-using-Deep-Skill-Chaining/</id>
    <published>2020-03-20T06:17:04.000Z</published>
    <updated>2020-03-24T09:06:42.060Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>自动发现时间上可扩展的动作或技能是分层强化学习的长期目标。我们提出了一种新的算法，该算法将技能链与深度神经网络相结合，可以在高维、连续域中自动发现技能。最终的算法，即<em>deep skill chaining</em>，通过这样的属性来构建技能，即当执行一个时使能智能体去执行另一个。（constructs skills with the property that executing one enables the agent to execute another.） 我们证明，在挑战性的连续控制任务中，<em>deep skill chaining</em>显着优于非分层智能体和其他最新的技能发现技术。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_option_discovery_using_deep_skill_chaining.pdf_-_C_72.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Bagaria, A., &amp; Konidaris, G.</li><li>出处：ICLR2020 Poster</li><li>机构：Brown University<!-- * 关键词： --></li><li><a href="https://openreview.net/forum?id=B1gqipNYwH" target="_blank" rel="noopener">论文链接</a><!-- * 开源代码： --></li><li>其他资料：<ul><li><a href="https://www.youtube.com/watch?v=MGvvPmm6JQg&amp;feature=youtu.be" target="_blank" rel="noopener">演示视频</a></li></ul></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_option_discovery_using_deep_skill_chaining-Origina_71.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ol><li>收集触发新option $o_k$的终止条件$\beta_{o_k}$的轨迹。</li><li>训练$o_k$的option policy $\pi_{o_k}$。</li><li>学习$o_k$的初始集分类器$\mathcal{I}_{o_k}$。</li><li>将$o_k$添加到智能体的option repertoire中。</li><li>创建一个新的option $o_{k+1}$，使$\beta_{o_{k+1}}=\mathcal{I}_{o_k}$。</li><li>训练option $\pi_\mathcal{O}$的policy。</li></ol><!-- 继续执行步骤1、3、4和5，直到MDP的启动状态在某个选项的启动集中。 无限期地继续执行步骤2和6。 --><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;自动发现时间上可扩展的动作或技能是分层强化学习的长期目标。我们提出了一种新的算法，该算法将技能链与深度神经网络相结合，可以在高维、连续域中自动发现技能。最终的算法，即&lt;em&gt;deep skill chaining&lt;/em&gt;，通过这样的属性来构建技能，即当执行一个时使能智能体去执行另一个。（constructs skills with the property that executing one enables the agent to execute another.） 我们证明，在挑战性的连续控制任务中，&lt;em&gt;deep skill chaining&lt;/em&gt;显着优于非分层智能体和其他最新的技能发现技术。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
      <category term="Skill Learning" scheme="https://yachenkang.github.io/blog/tags/Skill-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery</title>
    <link href="https://yachenkang.github.io/blog/2020/03/20/Dynamical-Distance-Learning-for-Semi-Supervised-and-Unsupervised-Skill-Discovery/"/>
    <id>https://yachenkang.github.io/blog/2020/03/20/Dynamical-Distance-Learning-for-Semi-Supervised-and-Unsupervised-Skill-Discovery/</id>
    <published>2020-03-20T05:56:49.000Z</published>
    <updated>2020-03-24T09:06:35.913Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>强化学习需要手动指定奖励函数才能学习任务。虽然原则上该奖励函数仅需要指定任务目标，但在实践中，强化学习可能非常耗时甚至不可行，除非对奖励函数进行了调整，以便产生平滑的梯度导向成功的结果。但手动调整是很难的，尤其是从原始观察结果（例如图像）获取任务时。<strong>在本文中，我们研究了如何自动学习动态距离：一种从任何其他状态到达给定目标状态的预期时间步个数的度量</strong>。这些动态距离可用于提供well-shaped奖励函数，以实现新的目标，从而有可能有效地学习复杂的任务。我们表明动态距离可以被用于半监督，其中无监督与环境的交互用于学习动态距离，而少量的偏好监督用于确定任务目标，而无需任何人工设计的奖励函数或目标示例。我们在真实机器人和仿真中都评估了我们的方法。我们展示了我们的方法可以使用原始的9自由度机械手学习阀门的转动，使用原始图像观察结果和十个偏好标签，而无需任何其他监督。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Hartikainen_et_al._-_2019_-_Dynamical_Distance_Lea_68.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Hartikainen, K., Geng, X., Haarnoja, T., &amp; Levine, S.</li><li>出处：ICLR2020 Poster</li><li>机构：Oxford, UCB, DeepMind<!-- * 关键词： --></li><li><a href="https://arxiv.org/abs/1907.08225" target="_blank" rel="noopener">论文链接</a><!-- * 开源代码： --></li><li>其他资料：<ul><li><a href="https://sites.google.com/view/dynamical-distance-learning" target="_blank" rel="noopener">主页</a></li></ul></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Hartikainen_et_al._-_2019_-_Dynamical_Distance_Lea_69.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Hartikainen_et_al._-_2019_-_Dynamical_Distance_Lea_75.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Hartikainen_et_al._-_2019_-_Dynamical_Distance_Lea_73.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Hartikainen_et_al._-_2019_-_Dynamical_Distance_Lea_74.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;强化学习需要手动指定奖励函数才能学习任务。虽然原则上该奖励函数仅需要指定任务目标，但在实践中，强化学习可能非常耗时甚至不可行，除非对奖励函数进行了调整，以便产生平滑的梯度导向成功的结果。但手动调整是很难的，尤其是从原始观察结果（例如图像）获取任务时。&lt;strong&gt;在本文中，我们研究了如何自动学习动态距离：一种从任何其他状态到达给定目标状态的预期时间步个数的度量&lt;/strong&gt;。这些动态距离可用于提供well-shaped奖励函数，以实现新的目标，从而有可能有效地学习复杂的任务。我们表明动态距离可以被用于半监督，其中无监督与环境的交互用于学习动态距离，而少量的偏好监督用于确定任务目标，而无需任何人工设计的奖励函数或目标示例。我们在真实机器人和仿真中都评估了我们的方法。我们展示了我们的方法可以使用原始的9自由度机械手学习阀门的转动，使用原始图像观察结果和十个偏好标签，而无需任何其他监督。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
      <category term="Skill Learning" scheme="https://yachenkang.github.io/blog/tags/Skill-Learning/"/>
    
  </entry>
  
  <entry>
    <title>COMPOSING TASK-AGNOSTIC POLICIES WITH DEEP REINFORCEMENT LEARNING</title>
    <link href="https://yachenkang.github.io/blog/2020/03/20/COMPOSING-TASK-AGNOSTIC-POLICIES-WITH-DEEP-REINFORCEMENT-LEARNING/"/>
    <id>https://yachenkang.github.io/blog/2020/03/20/COMPOSING-TASK-AGNOSTIC-POLICIES-WITH-DEEP-REINFORCEMENT-LEARNING/</id>
    <published>2020-03-20T05:44:24.000Z</published>
    <updated>2020-09-03T08:03:01.679Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>用基本行为的构成去解决迁移学习难题的是构建人工智能的关键要素之一。迄今为止，在学习task-specific的策略或技能方面已经有了大量工作，但几乎没有关注构建与任务无关的必要技能以找到新问题的解决方案。<strong>在本文中，我们提出了一种新的，基于深度强化学习的技能迁移和组合方法，该方法采用智能体的primitive策略来解决未曾见过的任务</strong>。我们在困难的环境中评估我们的方法，在这些环境中，通过标准强化学习（RL）甚至是分层RL的训练策略要么不可行，要么表现出较高的样本复杂性。我们证明了我们的方法不仅可以将技能迁移到新的问题设置中，而且还可以解决既需要任务计划又需要运动控制的挑战性环境，且数据效率很高。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Qureshi_et_al._-_2019_-_COMPOSING_TASK-AGNOSTIC_PO_65.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Qureshi, A. H., Johnson, J. J., Qin, Y., Henderson, T., Boots, B., &amp; Yip, M. C.</li><li>出处：ICLR2020 Poster</li><li>机构：UCSD<!-- * 关键词： --></li><li><a href="https://openreview.net/forum?id=H1ezFREtwH" target="_blank" rel="noopener">论文链接</a></li><li>开源代码：<ul><li><a href="https://github.com/ahq1993/compositional_reinforcement_learning" target="_blank" rel="noopener">https://github.com/ahq1993/compositional_reinforcement_learning</a></li></ul></li><li>其他资料：<ul><li><a href="https://sites.google.com/view/compositional-rl" target="_blank" rel="noopener">主页</a></li></ul></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Qureshi_et_al._-_2019_-_COMPOSING_TASK-AGNOSTIC_PO_66.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Qureshi_et_al._-_2019_-_COMPOSING_TASK-AGNOSTIC_PO_67.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;用基本行为的构成去解决迁移学习难题的是构建人工智能的关键要素之一。迄今为止，在学习task-specific的策略或技能方面已经有了大量工作，但几乎没有关注构建与任务无关的必要技能以找到新问题的解决方案。&lt;strong&gt;在本文中，我们提出了一种新的，基于深度强化学习的技能迁移和组合方法，该方法采用智能体的primitive策略来解决未曾见过的任务&lt;/strong&gt;。我们在困难的环境中评估我们的方法，在这些环境中，通过标准强化学习（RL）甚至是分层RL的训练策略要么不可行，要么表现出较高的样本复杂性。我们证明了我们的方法不仅可以将技能迁移到新的问题设置中，而且还可以解决既需要任务计划又需要运动控制的挑战性环境，且数据效率很高。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
      <category term="Skill Learning" scheme="https://yachenkang.github.io/blog/tags/Skill-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Discovering Motor Programs By Recomposing Demonstrations</title>
    <link href="https://yachenkang.github.io/blog/2020/03/20/Discovering-Motor-Programs-By-Recomposing-Demonstrations/"/>
    <id>https://yachenkang.github.io/blog/2020/03/20/Discovering-Motor-Programs-By-Recomposing-Demonstrations/</id>
    <published>2020-03-20T05:23:41.000Z</published>
    <updated>2020-03-24T09:06:33.670Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>在本文中，我们提出了一种从大规模且多样化的操作演示中来学习可重构motor primitives的方法。当前将演示分解为primitives的方法通常采用手动定义的primitives，而绕开了发现这些primitives的难度。另一方面，用于发现primitives的方法对primitive的复杂性进行了限制性假设，这使得任务的适用性限制在了狭窄的范围。<strong>我们的方法试图通过同时学习基础的motor primitives并重组这些primitives以重构原始演示来应对这些挑战</strong>。通过限制primitives分解的简约性和给定primitive的简单性，我们能够学习各种不同的motor primitives，以及它们的连贯潜在表示。我们从定性和定量两个方面证明了我们所学的primitives捕获了演示中语义上有意义的方面。这使我们能够在分层强化学习设置中组合这些primitives，以有效解决机器人操作任务，例如伸手和推手。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Discovering-Motor-Programs-by-Recomposing-Demonstr_60.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Tanmay Shankar, Shubham Tulsiani, Lerrel Pinto, Abhinav Gupta</li><li>出处：ICLR2020 Poster</li><li>机构：Facebook, CMU<!-- * 关键词： --></li><li><a href="https://openreview.net/forum?id=rkgHY0NYwr" target="_blank" rel="noopener">论文链接</a><!-- * 开源代码： --><!-- * 其他资料： --></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_discovering_motor_programs_by_recomposing_demonstr_61.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_discovering_motor_programs_by_recomposing_demonstr_63.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在本文中，我们提出了一种从大规模且多样化的操作演示中来学习可重构motor primitives的方法。当前将演示分解为primitives的方法通常采用手动定义的primitives，而绕开了发现这些primitives的难度。另一方面，用于发现primitives的方法对primitive的复杂性进行了限制性假设，这使得任务的适用性限制在了狭窄的范围。&lt;strong&gt;我们的方法试图通过同时学习基础的motor primitives并重组这些primitives以重构原始演示来应对这些挑战&lt;/strong&gt;。通过限制primitives分解的简约性和给定primitive的简单性，我们能够学习各种不同的motor primitives，以及它们的连贯潜在表示。我们从定性和定量两个方面证明了我们所学的primitives捕获了演示中语义上有意义的方面。这使我们能够在分层强化学习设置中组合这些primitives，以有效解决机器人操作任务，例如伸手和推手。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
      <category term="Skill Learning" scheme="https://yachenkang.github.io/blog/tags/Skill-Learning/"/>
    
  </entry>
  
  <entry>
    <title>LEARNING TO COORDINATE MANIPULATION SKILLS VIA SKILL BEHAVIOR DIVERSIFICATION</title>
    <link href="https://yachenkang.github.io/blog/2020/03/20/LEARNING-TO-COORDINATE-MANIPULATION-SKILLS-VIA-SKILL-BEHAVIOR-DIVERSIFICATION/"/>
    <id>https://yachenkang.github.io/blog/2020/03/20/LEARNING-TO-COORDINATE-MANIPULATION-SKILLS-VIA-SKILL-BEHAVIOR-DIVERSIFICATION/</id>
    <published>2020-03-20T02:18:55.000Z</published>
    <updated>2020-03-24T09:06:38.999Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>当完成一个复杂的操纵任务时，人们经常将任务分解为身体各个部分的子技能，独立地练习这些子技能，然后一起执行这些子技能。同样，具有多个末端执行器的机器人可以通过协调每个末端执行器的子技能来执行复杂的任务。<strong>为了实现技能的时间和行为协调，我们提出了一个模块化框架，该框架首先通过skill behavior diversification分别训练每个末端执行器的子技能，然后学习使用技能的多种行为来协调末端执行器</strong>。我们证明了我们提出的框架能够有效地协调技能，以解决具有挑战性的协作控制任务，例如捡起一根长棒，在用两个机械手推动容器的同时在容器内放置一个块以及用两个蚂蚁推动容器。 视频和代码可在上获得。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Lee%2C_Yang%2C_Lim_-_2019_-_LEARNING_TO_COORDINATE_MAN_58.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Lee, Y., Yang, J., &amp; Lim, J. J. (2019).</li><li>出处：ICLR2020 Poster</li><li>机构：University of Southern California<!-- * 关键词： --></li><li><a href="https://openreview.net/forum?id=ryxB2lBtvH" target="_blank" rel="noopener">论文链接</a></li><li>开源代码：<ul><li><a href="https://github.com/clvrai/coordination" target="_blank" rel="noopener">https://github.com/clvrai/coordination</a></li></ul></li><li>其他资料：<ul><li><a href="https://clvrai.com/coordination" target="_blank" rel="noopener">主页</a></li></ul></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Lee%2C_Yang%2C_Lim_-_2019_-_LEARNING_TO_COORDINATE_MAN_64.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-20_Lee%2C_Yang%2C_Lim_-_2019_-_LEARNING_TO_COORDINATE_MAN_59.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;当完成一个复杂的操纵任务时，人们经常将任务分解为身体各个部分的子技能，独立地练习这些子技能，然后一起执行这些子技能。同样，具有多个末端执行器的机器人可以通过协调每个末端执行器的子技能来执行复杂的任务。&lt;strong&gt;为了实现技能的时间和行为协调，我们提出了一个模块化框架，该框架首先通过skill behavior diversification分别训练每个末端执行器的子技能，然后学习使用技能的多种行为来协调末端执行器&lt;/strong&gt;。我们证明了我们提出的框架能够有效地协调技能，以解决具有挑战性的协作控制任务，例如捡起一根长棒，在用两个机械手推动容器的同时在容器内放置一个块以及用两个蚂蚁推动容器。 视频和代码可在上获得。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
      <category term="Skill Learning" scheme="https://yachenkang.github.io/blog/tags/Skill-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Contrastive Learning of Structured World Models</title>
    <link href="https://yachenkang.github.io/blog/2020/03/19/Contrastive-Learning-of-Structured-World-Models/"/>
    <id>https://yachenkang.github.io/blog/2020/03/19/Contrastive-Learning-of-Structured-World-Models/</id>
    <published>2020-03-19T07:26:06.000Z</published>
    <updated>2020-04-11T08:53:01.705Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一个挑战。为了朝这个目标迈进，我们引入了Contrastively-trained Structured World Models（C-SWMs）。C-SWMs利用对比方法在具有合成结构的环境中进行表示学习。<strong>我们通过图神经网络建模将每个state嵌入构造为一组对象表示及其关系。这允许模型从原始像素观察中发现对象，而无需把直接监督作为学习过程的一部分</strong>。我们在包含多个交互对象的合成环境中评估C-SWMs，这些交互对象均可以被智能体相互独立操作，包括简单的Atari游戏和多对象物理模拟器。我们的实验表明，C-SWMs可以在学习到可解释的基于对象的表示形式的基础上，克服基于像素重构的模型的局限性，并在高度结构化的环境中胜过此类模型的典型代表。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><!-- 图片 --><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-19_Kipf%2C_van_der_Pol%2C_Welling_-_2019_-_Contrastive_Le_57.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Kipf, T., van der Pol, E., &amp; Welling, M. (2019).</li><li>出处：ICLR2020 Oral</li><li>机构：University of Amsterdam</li><li>关键词：表示学习，图神经网络，自监督</li><li><a href="http://arxiv.org/abs/1911.12247" target="_blank" rel="noopener">论文链接</a><!-- * 开源代码： --><!-- * 其他资料： --></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_25.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一个挑战。为了朝这个目标迈进，我们引入了Contrastively-trained Structured World Models（C-SWMs）。C-SWMs利用对比方法在具有合成结构的环境中进行表示学习。&lt;strong&gt;我们通过图神经网络建模将每个state嵌入构造为一组对象表示及其关系。这允许模型从原始像素观察中发现对象，而无需把直接监督作为学习过程的一部分&lt;/strong&gt;。我们在包含多个交互对象的合成环境中评估C-SWMs，这些交互对象均可以被智能体相互独立操作，包括简单的Atari游戏和多对象物理模拟器。我们的实验表明，C-SWMs可以在学习到可解释的基于对象的表示形式的基础上，克服基于像素重构的模型的局限性，并在高度结构化的环境中胜过此类模型的典型代表。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Fast Task Inference with Variational Intrinsic Successor Features</title>
    <link href="https://yachenkang.github.io/blog/2020/03/19/Fast-Task-Inference-with-Variational-Intrinsic-Successor-Features/"/>
    <id>https://yachenkang.github.io/blog/2020/03/19/Fast-Task-Inference-with-Variational-Intrinsic-Successor-Features/</id>
    <published>2020-03-19T03:21:00.000Z</published>
    <updated>2020-04-11T08:53:07.772Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>已经确定，张成马尔可夫决策过程可控子空间的多样性行为可以通过奖励与其他policy有区别的policy来训练(Gregor et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018)。但是，这种方法的一个局限性是难以推广到超出可明确学习的有限行为集的范围，而这在后续任务中可能是必需的。Successor features(Dayan, 1993; Barreto et al., 2017)提供了一个吸引人的解决方案，适用于此泛化问题，但需要在某些基础特征空间中将奖励函数定义为线性。在本文中，我们展示了可以将这两种技术结合使用，并且相互可以解决彼此的主要局限。为此，<strong>我们引入了Variational Intrinsic Successor FeatuRes（VISR），这是一种新的算法，能够学习可控特征，可通过Successor features框架利用可控特征来提供增强的泛化能力和快速的任务推断能力</strong>。我们在全部Atari套件上对VISR进行了实验验证，我们使用了新的设置，其中奖励仅是在漫长的无监督阶段之后才短暂显示。在12场比赛中达到人类水平的表现并超过所有baselines，使我们认为VISR代表了朝着能够从有限的反馈中快速学习的智能体迈出的一步。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><!-- 图片 --><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-19_Hansen_DeepMind_et_al._-_2019_-_Fast_Task_Inferenc_56.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Hansen DeepMind, S., Dabney DeepMind, W., Barreto DeepMind, A., Warde-Farley DeepMind, D., Van de Wiele, T., &amp; Mnih DeepMind, V.</li><li>出处：ICLR2020 Oral</li><li>机构：DeepMind</li><li>关键词：diverse behavior，Successor features</li><li><a href="https://openreview.net/forum?id=BJeAHkrYDS" target="_blank" rel="noopener">论文链接</a><!-- * 开源代码： --><!-- * 其他资料： --></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_28.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;已经确定，张成马尔可夫决策过程可控子空间的多样性行为可以通过奖励与其他policy有区别的policy来训练(Gregor et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018)。但是，这种方法的一个局限性是难以推广到超出可明确学习的有限行为集的范围，而这在后续任务中可能是必需的。Successor features(Dayan, 1993; Barreto et al., 2017)提供了一个吸引人的解决方案，适用于此泛化问题，但需要在某些基础特征空间中将奖励函数定义为线性。在本文中，我们展示了可以将这两种技术结合使用，并且相互可以解决彼此的主要局限。为此，&lt;strong&gt;我们引入了Variational Intrinsic Successor FeatuRes（VISR），这是一种新的算法，能够学习可控特征，可通过Successor features框架利用可控特征来提供增强的泛化能力和快速的任务推断能力&lt;/strong&gt;。我们在全部Atari套件上对VISR进行了实验验证，我们使用了新的设置，其中奖励仅是在漫长的无监督阶段之后才短暂显示。在12场比赛中达到人类水平的表现并超过所有baselines，使我们认为VISR代表了朝着能够从有限的反馈中快速学习的智能体迈出的一步。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ICLR2020 Reinforcement Learning Reading List</title>
    <link href="https://yachenkang.github.io/blog/2020/03/06/ICLR2020-Reinforcement-Learning-Reading-List/"/>
    <id>https://yachenkang.github.io/blog/2020/03/06/ICLR2020-Reinforcement-Learning-Reading-List/</id>
    <published>2020-03-06T01:44:37.000Z</published>
    <updated>2020-09-17T23:41:27.632Z</updated>
    
    <content type="html"><![CDATA[<p>ICLR2020论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。</p><a id="more"></a><p>*标注的为值得精读论文</p><h2 id="Oral（9篇）"><a href="#Oral（9篇）" class="headerlink" title="Oral（9篇）"></a>Oral（9篇）</h2><ul><li><p><strong>Contrastive Learning of Structured World Models.</strong> Kipf, T., van der Pol, E., &amp; Welling, M. (2019). [<a href="http://arxiv.org/abs/1911.12247" target="_blank" rel="noopener">论文链接</a>]*</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_25.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>表示学习，图神经网络，自监督</li><li>从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一个挑战。为了朝这个目标迈进，我们引入了Contrastively-trained Structured World Models（C-SWMs）。C-SWMs利用对比方法在具有合成结构的环境中进行表示学习。<strong>我们通过图神经网络建模将每个state嵌入构造为一组对象表示及其关系。这允许模型从原始像素观察中发现对象，而无需把直接监督作为学习过程的一部分</strong>。我们在包含多个交互对象的合成环境中评估C-SWMs，这些交互对象均可以被智能体相互独立操作，包括简单的Atari游戏和多对象物理模拟器。我们的实验表明，C-SWMs可以在学习到可解释的基于对象的表示形式的基础上，克服基于像素重构的模型的局限性，并在高度结构化的环境中胜过此类模型的典型代表。</li></ul></li><li><p><strong>IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO.</strong> Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://github.com/openai/baselines" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>OpenAI</li><li>强化学习实现</li><li>我们通过对两个流行算法（近似策略优化PPO和信任区域策略优化TRPO）的案例，研究了深度策略梯度算法中算法进步的根源。我们调研了“代码级优化”（仅在实现中发现或被描述为核心算法的辅助细节的算法增强）的结果。看起来是次要的，然而<strong>这种优化对智能体行为有重大影响</strong>。我们的结果表明，这些优化（a）贡献了PPO优于TRPO累积奖励中的大部分收益，以及（b）从根本上改变RL方法的功能。这些观察表明了在强化学习中对效果提升的归因是困难和重要的。</li></ul></li><li><p><strong>A CLOSER LOOK AT DEEP POLICY GRADIENTS.</strong> Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://openreview.net/forum?id=ryxdEkHtPS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>MIT</li><li>强化学习评估方法</li><li>我们研究了深度策略梯度算法的行为如何反映激励其发展的概念框架。为此，<strong>我们基于该框架的关键要素（gradient estimation, value prediction, 和optimization landscapes）提出了一种对SOTA的细粒度分析方法</strong>。我们的结果表明，深度策略梯度算法的行为通常会偏离其激励框架的预测：替代目标与真实奖励机制不匹配，学到的value estimators无法匹配真实的value function，并且gradient estimates与“真正”的梯度之间的相关性很差。我们发现的预测行为和实验行为之间的不匹配，凸显了我们对当前方法的理解不足，并表明需要超越当前以benchmark为中心的评估方法。</li></ul></li><li><p><strong>Meta-Q-Learning.</strong> Fakoor, R., Chaudhari, P., Soatto, S., &amp; Smola, A. J. (2019). [<a href="http://arxiv.org/abs/1910.00125" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>元强化学习，Q-learning</li><li>本文介绍了Meta-Q-Learning（MQL），这是<strong>一种用于元强化学习（meta-RL）的新的off-policy算法</strong>。MQL基于三个简单的想法。首先，我们展示了如果可以访问表示过去轨迹的上下文变量，Q-learning可以匹敌最新的meta-RL算法。其次，最大化训练任务的平均奖励这样一种多任务目标是对RL策略进行元训练的有效方法。第三，元训练replay buffer中的过去数据可以被循环利用，在新任务上可以使用off-policy更新来更新策略。MQL借鉴了propensity estimation中的想法，从而扩大了可用于更新的数据量。在标准连续控制benchmark上进行的实验表明，MQL与最新的meta-RL算法相比具有优势。</li></ul></li><li><p><strong>POSTERIOR SAMPLING FOR MULTI-AGENT REINFORCE-MENT LEARNING: SOLVING EXTENSIVE GAMES WITH IMPERFECT INFORMATION.</strong> Zhou, Y., Li, J., &amp; Zhu, J. (2019). [<a href="https://openreview.net/forum?id=Syg-ET4FPS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>清华</li><li>强化学习后验采样，multi-agent，CFR</li><li>Posterior sampling for reinforcement learning（PSRL）是在未知环境中进行决策的有用框架。PSRL维护环境的后验分布，然后在后验分布上采样的环境中进行规划。尽管PSRL在单智能体强化学习问题上表现良好，但如何将PSRL应用于多智能体强化学习问题仍待探索。在这项工作中，<strong>我们将PSRL扩展到具有不完全信息的双人零和博弈（TEGI）</strong>，这是一类多智能体系统。从技术上讲，我们将PSRL与counterfactual regret minimization（CFR，这是对环境已知的TEGI上的领先算法）相结合。我们的主要贡献是互动策略的新设计。通过我们的交互策略，我们的算法可证明以$O(\sqrt{\log T / T})$的速度收敛至Nash均衡。实验结果表明，我们的算法效果很好。</li></ul></li></ul><!-- - **A Generalized Training Approach for Multiagent Learning.** Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., … Munos, R. (2019). [[论文链接](http://arxiv.org/abs/1909.12823)]  - 博弈论，Policy-Spaced Response Oracles，$\alpha$-Rank  - 本文研究了一种基于博弈论原理的种群训练范式，该范式被称为“Policy-Spaced Response Oracles”（PSRO）。PSRO具有一般意义，因为它（1）包含了众所周知的算法，例如fictitious play和double oracle as special cases，并且（2）原则上适用于general-sum多玩家游戏。尽管如此，以前对PSRO的研究都集中在双人零和博弈上，这种范式中纳什均衡是可计算的。从双人零和博弈转移到更一般的设置时，纳什均衡的计算很快变得不可行。在这里，我们通过考虑替代的解决方案即$\alpha$-Rank概念来扩展PSRO的理论基础，$\alpha$-Rank是唯一的（因此与Nash不同，不存在均衡选择问题），并且易于应用于general-sum多玩家设置。我们在几个游戏类中建立收敛性保证，并确定纳什均衡与$\alpha$-Rank之间的联系。我们证明了在双人游戏Kuhn and Leduc Poker中基于$\alpha$-Rank的PSRO与基于Nash求解器的PSRO相比具有相似性能。然后，通过考虑3至5人扑克游戏，我们超越了先前的PSRO应用，产生了$\alpha$-Rank比近似Nash解算器具有更快收敛速度的实例，因此证明其为良好的一般游戏解算器。我们还对MuJoCo足球进行了初步的实验验证，说明了该方法在另一个复杂领域中的可行性。 --><ul><li><p><strong>Harnessing Structures for Value-Based Planning and Reinforcement Learning.</strong> Yang, Y., Zhang, G., Xu, Z., &amp; Katabi, D. (2019). [<a href="http://arxiv.org/abs/1909.12255" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_27.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>MIT</li><li>Q函数，低秩结构</li><li>Value-based方法是规划与深度强化学习（RL）的基本方法之一。本文中，我们建议在规划和深度强化学习中利用state-action value函数（即Q函数）的潜在结构。特别是，如果潜在的系统动态导致了Q函数的某些全局结构，则应该能够通过利用这种结构更好地推断该函数。具体来说，我们研究了低秩结构，它在大数据矩阵中广泛存在。我们在控制和深度强化学习任务的环境中通过实验验证了低秩Q函数的存在。作为我们的主要贡献，<strong>通过利用矩阵估计（ME）技术，我们提出了一个通用框架来利用Q函数中的底层低秩结构</strong>。这使得对经典控制任务的规划程序效率更高，此外，可以将简单方案应用于value-based强化学习技术，以在“低秩”任务上始终获得更好的性能。在控制任务和Atari游戏的大量实验证实了我们方法的有效性。</li></ul></li><li><p><strong>Fast Task Inference with Variational Intrinsic Successor Features.</strong> Hansen DeepMind, S., Dabney DeepMind, W., Barreto DeepMind, A., Warde-Farley DeepMind, D., Van de Wiele, T., &amp; Mnih DeepMind, V. (2019). [<a href="https://openreview.net/forum?id=BJeAHkrYDS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_28.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>DeepMind</li><li>diverse behavior，Successor features</li><li>已经确定，张成马尔可夫决策过程可控子空间的多样性行为可以通过奖励与其他policy有区别的policy来训练(Gregor et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018)。但是，这种方法的一个局限性是难以推广到超出可明确学习的有限行为集的范围，而这在后续任务中可能是必需的。Successor features(Dayan, 1993; Barreto et al., 2017)提供了一个吸引人的解决方案，适用于此泛化问题，但需要在某些基础特征空间中将奖励函数定义为线性。在本文中，我们展示了可以将这两种技术结合使用，并且相互可以解决彼此的主要局限。为此，<strong>我们引入了Variational Intrinsic Successor FeatuRes（VISR），这是一种新的算法，能够学习可控特征，可通过Successor features框架利用可控特征来提供增强的泛化能力和快速的任务推断能力</strong>。我们在全部Atari套件上对VISR进行了实验验证，我们使用了新的设置，其中奖励仅是在漫长的无监督阶段之后才短暂显示。在12场比赛中达到人类水平的表现并超过所有baselines，使我们认为VISR代表了朝着能够从有限的反馈中快速学习的智能体迈出的一步。</li></ul></li><li><p><strong>Observational Overfitting in Reinforcement Learning.</strong> Song, X., Jiang, Y., Tu, S., Du, Y., &amp; Neyshabur, B. (2019). [<a href="http://arxiv.org/abs/1912.02975" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_29.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>Google，MIT</li><li>过拟合，分析框架</li><li>在无模型强化学习（RL）中过拟合的一个主要组成部分涉及以下情况：<strong>智能体可能会根据马尔可夫决策过程（MDP）产生的观察结果错误地将奖励与某些虚假特征相关联</strong>。我们提供了一个用于分析这种情况的通用框架，我们使用该框架通过仅修改MDP的观察空间来设计了多个综合benchmarks。当智能体过拟合到不同的观察空间（即使潜在的MDP动态是固定的）时，我们称之为<em>observational overfitting</em>。我们的实验揭示了有趣的属性，尤其是在<em>implicit regularization</em>方面，还证实了先前工作中RL泛化和监督学习（SL）的结果。</li></ul></li><li><p><strong>Dynamics-Aware Unsupervised Discovery of Skills.</strong> Sharma, A., Gu, S., Levine, S., Kumar, V., &amp; Hausman, K. (2019). [<a href="http://arxiv.org/abs/1907.01657" target="_blank" rel="noopener">论文链接</a>]*</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_31.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_30.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>Google Brain</li><li>传统上，基于模型的强化学习（MBRL）旨在学习环境动态的全局模型。一个好的模型可以潜在地使规划算法生成多样化的行为并解决各种不同的任务。但是，要为复杂的动态系统学习准确的模型仍然很困难，即使成功，该模型也可能无法很好地推广到训练时的状态分布之外。在这项工作中，<strong>我们将基于模型的学习与针对原语的无模型学习结合在一起，从而使基于模型的规划变得容易</strong>。为此，我们旨在回答这个问题：我们如何发现结果易于预测的技能？我们提出了一种无监督的学习算法，即“Dynamics-Aware Discovery of Skills（DADS）”，它可以同时发现<em>可预测</em>的行为并学习其动态。从理论上讲，我们的方法可以利用连续的技能空间，使我们即使面对高维状态空间也可以不停学习许多行为。我们证明，在学习到的潜在空间中进行<em>zero-shot planning</em>明显优于标准MBRL和model-free goal-conditioned RL，可以处理稀疏奖励任务，并且在无监督技能发现方面大大优于现有的分层RL方法。 我们在以下网址公开了我们的实现：<a href="https://github.com/google-research/dads" target="_blank" rel="noopener">https://github.com/google-research/dads</a></li></ul></li></ul><h2 id="Spotlight（15篇）"><a href="#Spotlight（15篇）" class="headerlink" title="Spotlight（15篇）"></a>Spotlight（15篇）</h2><ul><li><p><strong>DOUBLY ROBUST BIAS REDUCTION IN INFINITE HORIZON OFF-POLICY ESTIMATION.</strong> Tang, Z., Feng, Y., Li, L., Research, G., Zhou, D., &amp; Liu, Q. (2019). [<a href="https://arxiv.org/abs/1910.07186" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Austin，Google Research</li><li>off-policy</li><li>由于典型importance sampling（IS）估计量的方差过大，因此<em>Infinite horizon</em> off-policy policy的评估是一项极具挑战性的任务。最近，Liu et al. (2018a)提出了一种方法，该方法通过估算固定密度比来显着减少infinite horizon off-policy的评估的方差，但这是以引入密度比估计误差引起的biases为代价的。在本文中，<strong>我们开发了一种对他们方法减少bias的改进，可以利用学到的value function来提高精度</strong>。我们的方法具有<em>双重鲁棒性</em>，因为当密度比或value function估计完美时，bias消失。通常，当它们中的任何一个准确时，也可以减小bias。理论和实验结果均表明，我们的方法比以前的方法具有明显的优势。</li></ul></li><li><p><strong>INFLUENCE-BASED MULTI-AGENT EXPLORATION.</strong> Wang, T., Wang, J., Wu, Y., &amp; Zhang, C. (2019). [<a href="https://arxiv.org/abs/1910.05512" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>清华</li><li>transition-dependent multi-agent settings</li><li>内在驱动的强化学习旨在解决稀疏奖励任务的探索挑战。但是，文献中基本上没有研究transition-dependent的多主体环境中的探索方法。我们旨在朝着解决这个问题迈出一步。<strong>我们介绍了两种探索方法：exploration via information-theoretic influence（EITI）和exploration via decision-theoretic influence（EDTI），利用智能体在协作行为中的交互</strong>。EITI使用互信息来获取智能体transition dynamics之间的相互依存关系。EDTI使用一种称为Value of Interaction（VoI）的新的内在奖励来表征和量化一个智能体的行为对其他智能体的return期望的影响。通过优化EITI或EDTI目标作为正则项，鼓励智能体协调其探索和学习策略以优化集体效果。我们展示了如何优化这些正则项，以便它们可以轻松地与策略梯度强化学习集成。由此产生的更新规则在协同探索和内在reward分布之间建立了联系。最后，我们通过实验证明了我们的方法在多种多智能体场景中的强大优势。</li></ul></li><li><p><strong>MODEL BASED REINFORCEMENT LEARNING FOR ATARI.</strong> Kaiser, Ł., Babaeizadeh, M., Miłos, P., Zej Osí Nski, B., Campbell, R. H., Czechowski, K., … Ai, D. (2019). [<a href="https://goo.gl/itykP8" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_32.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>Google Brain</li><li>model-based，视频预测模型</li><li>无模型强化学习（RL）可以用于学习复杂任务（例如Atari游戏）的有效策略，甚至可以从图像观察中学习。但是，这通常需要非常大量的交互——实际上，比人类学习相同游戏所需的交互要多得多。人是如何能如此迅速地学习？答案的部分原因可能是人们可以了解游戏的运作方式并预测哪些动作将导致理想的结果。在本文中，我们探索视频预测模型如何类似地使智能体能够以比无模型方法用更少的交互来完成Atari游戏。<strong>我们描述了Simulated Policy Learning（SimPLe），这是一种完全model-based的基于视频预测模型的深度RL算法</strong>，并提供了几种模型结构的比较，其中包括一种在我们的环境中产生最佳效果的新结构。我们的实验在智能体与环境之间进行100k次交互的低数据状态下的一系列Atari游戏中评估SimPLe，这相当于两个小时的实时播放。在大多数游戏中，SimPLe的性能优于最新的无模型算法，在某些游戏中，SimPLe的性能甚至超越一个数量级。</li></ul></li><li><p><strong>Behaviour Suite for Reinforcement Learning.</strong> Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., … Deepmind, H. (2019). [<a href="https://arxiv.org/abs/1908.03568" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>DeepMind</li><li>benchmark</li><li>本文介绍了<em>Behaviour Suite for Reinforcement Learning</em>，简称bsuite。<strong>bsuite是经过精心设计的实验的集合，这些实验通过两个目标研究了强化学习（RL）智能体的核心功能</strong>。首先，要收集清晰，信息丰富和可扩展的问题，以捕捉通用和高效的学习算法设计中的关键问题。第二，通过智能体在这些共享benchmark上的表现来研究他们的行为。为了补充这项工作，我们开源了<a href="https://github.com/deepmind/bsuite" target="_blank" rel="noopener">https://github.com/deepmind/bsuite</a>，可以自动评估和分析bsuite上的任何智能体。该库有助于对RL中的核心问题进行可重复且易于访问的研究，并最终设计出卓越的学习算法。我们的代码是Python，易于在现有项目中使用。我们包含了OpenAI Baselines，多巴胺以及新的参考实现的示例。未来，我们希望纳入学界的更多出色实验，并承诺定期由著名研究人员委员会审查bsuite。</li></ul></li><li><p><strong>EMERGENT TOOL USE FROM MULTI-AGENT AUTOCURRICULA.</strong> Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., … Brain, G. (2019). [<a href="https://arxiv.org/abs/1909.07528" target="_blank" rel="noopener">论文链接</a>]*</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-13_Mendeley_Desktop_34.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>OpenAI</li><li>multi-agent</li><li>通过多智能体竞争，捉迷藏的简单目标以及大规模的标准强化学习算法，我们发现主体创建了自监督的自动课程，包含多回合不同的策略，其中许多回合需要复杂的工具使用和协作。我们发现在我们的环境中智能体策略的六个紧急阶段是显而易见的，每个阶段都会给对方团队带来新的压力。例如，智能体学会使用可移动的盒子来建造multi-object掩体，这反过来又导致智能体发现他们可以使用坡道克服障碍。<strong>我们进一步提供的证据表明，与其他自监督的强化学习方法（例如内在驱动）相比，多智能体竞争可能会随着环境复杂性的提高而更好地拓展，并导致以人类相关技能为中心的行为</strong>。最后，我们提出迁移和fine-tuning作为定量评估目标能力的一种方法，并且我们在一组特定领域的智力测验中将捉迷藏智能体和内在驱动与随机初始化baseline进行了比较。</li></ul></li><li><p><strong>Dream to Control: Learning Behaviors by Latent Imagination.</strong> Hafner, D., Lillicrap, T., Ba, J., &amp; Norouzi, M. (2019). [<a href="http://arxiv.org/abs/1912.01603" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Google Brain，Deepmind</li><li>学习得到的世界模型总结了智能体的经验，以促进学习复杂行为。虽然通过深度学习从高维sensory输入中学习世界模型变得可行，但是仍有许多潜在的方法可以从中推导行为。我们研究了Dreamer，这是一种强化学习智能体，可以只通过潜在的想象力解决图像中的长时程任务。<strong>我们通过在学到的世界模型紧凑状态空间中想象轨迹传播学到的state values的解析梯度来有效地学习行为</strong>。（We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model.） 在完成20项具有挑战性的视觉控制任务后，Dreamer在数据效率，计算时间和最终性能方面都超过了现有方法。</li></ul></li><li><p><strong>SIMPLIFIED ACTION DECODER FOR DEEP MULTI-AGENT REINFORCEMENT LEARNING.</strong> Hu, H., &amp; Foerster, J. N. (2019). [<a href="https://arxiv.org/abs/1912.02288" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Facebook AI Research</li><li>Hanabi，向合作者提供信息</li><li>近年来，我们在AI的许多benchmark问题上看到了快速的进步，现代方法在Go，Poker和Dota中达到了近乎或超乎人类的表现。所有这些挑战的一个共同方面是，它们在设计上是<em>对抗</em>的，或者更技术地说是零和的。与这些设置相反，在现实世界中，成功通常需要人类在至少部分合作的设置下与他人合作和交流。去年，纸牌游戏<em>Hanabi</em>被确立为AI的新benchmark环境，以填补这一空白。特别是，<em>Hanabi</em>对人类很有趣，因为它完全专注于思想理论，即在观察其他玩家的行为时能够有效地推理其他玩家的意图，信念和观点的能力。<strong>强化学习（RL）面临着一个有趣的挑战，即在被他人观察时学习如何提供信息：强化学习从根本上要求智能体进行探索，以便发现良好的policy。但是，如果仅仅简单地做到这一点，这种随机性将固有地使他们的动作在训练过程中给他人提供的信息少。我们提出了一种新的深度多智能体RL方法，即<em>Simplified Action Decoder</em>（SAD），该方法通过集中训练阶段解决了这一矛盾</strong>。在训练过程中，SAD允许其他智能体不仅能观察所选择的（<em>exploratory</em>）行为，而且智能体还观察其队友的<em>greedy</em>行为。通过将这种简单的intuition与用于多智能体学习的最佳实现相结合，SAD在Hanabi挑战的独立游戏部分为2-5名智能体提供达到了新SOTA的学习方法。与最佳实现组件相比，我们的ablations显示了SAD的贡献。 我们所有的代码和训练好的智能体都可以在<a href="https://github.com/facebookresearch/Hanabi_SAD" target="_blank" rel="noopener">https://github.com/facebookresearch/Hanabi_SAD</a>上找到。</li></ul></li><li><p><strong>IS A GOOD REPRESENTATION SUFFICIENT FOR SAM-PLE EFFICIENT REINFORCEMENT LEARNING?</strong> Du, S. S., Kakade, S. M., Wang, R., &amp; Yang, L. F. (2019). [<a href="https://arxiv.org/abs/1910.03016" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>IAS</li><li><p>现代深度学习方法提供了学习良好表示的有效手段。但是，良好的表示形式本身是否足以进行样本有效的强化学习？仅在更经典的近似动态规划文献中，针对（最坏情况）近似误差研究了该问题。从统计学角度看，这个问题在很大程度上尚待探讨，并且现有文献主要集中在<em>允许</em>样本进行有效强化学习而几乎不了解有效强化学习的<em>必要</em>条件的情况下。</p><p>这项工作表明，从统计学的角度来看，情况比传统的近似观点所建议的要微妙得多，在传统的近似观点中，对满足样本有效RL的表示要求更加严格。我们的主要结果为强化学习方法提供了清晰的门槛，表明在构成良好的函数逼近（就表示的维数而言）方面存在严格的限制，我们专注于与value-based, model-based, 以及policy-based的学习相关的自然表示条件。<strong>这些下限突显出，除非其近似值的质量超过某些严格的阈值，否则本身具有良好的（value- based, model-based, 或policy-based）表示不足以进行有效的强化学习</strong>。此外，这一下限还意味着样本复杂性在以下几点对比间指数级的分离：1）具有完美表示的value-based learning与具有良好但不完美表示的value-based learning；2）value-based learning与policy-based learning，3）policy-based learning和监督学习，以及4）强化学习和模仿学习。</p></li></ul></li><li><p><strong>Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees.</strong> Chen, B., Dai, B., Lin, Q., Ye, G., Liu, H., &amp; Song, L. (2019). [<a href="http://arxiv.org/abs/1903.00070" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Georgia Tech</li><li><strong>我们提出一种名为Neural Exploration-Exploitation Trees（NEXT）的元路径规划算法，以从先前的经验中学习，用来解决高维连续状态和动作空间中的新路径规划问题</strong>。与更经典的基于采样的方法（如RRT）相比，我们的方法在高维度上可获得更高的采样效率，并且可以从类似环境的规划经验中受益。更具体地说，NEXT利用一种新的神经体系结构，可以从问题结构中学习有前景的搜索方向。然后，将学习到的先验知识整合到UCB类型的算法中，以在解决新问题时实现<em>exploration</em>和<em>exploitation</em>之间的在线平衡。我们进行了彻底的实验，以表明NEXT通过更紧凑的搜索树解决了新的计划问题，并在某些benchmark上明显优于SOTA。</li></ul></li><li><p><strong>Making Sense of Reinforcement Learning and Probabilistic Inference.</strong> O’Donoghue, B., Osband, I., &amp; Ionescu, C. (2020). [<a href="http://arxiv.org/abs/2001.00805" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>DeepMind</li><li>强化学习（RL）将控制问题与统计估计结合在一起：智能体不知道系统动态，但可以通过经验来学习。最近的研究工作阐述了“RL作为推理”，并提出了一个特殊的框架将RL问题推广为概率推论。<strong>我们的论文揭示了该方法的主要缺点，并阐明了将RL连贯地转换为推理问题的意义</strong>。具体来说，RL智能体必须考虑其行为对未来reward和观察的影响：exploration-exploitation的权衡。在除最简单的设置之外的所有条件下，得出的推论在计算上都是棘手的，因此实际的RL算法必须重新近似。我们证明了流行的“RL作为推论”近似方法即使在非常基本的问题中也可能表现不佳。但是，我们展示了只需稍加修改，该框架就可以产生可证明具有良好性能的算法，并且我们表明，所得算法等同于最近提出的K学习，我们还进一步将其与Thompson采样结合在一起。</li></ul></li><li><p><strong>IMPROVING GENERALIZATION IN META REINFORCE-MENT LEARNING USING LEARNED OBJECTIVES.</strong> Kirsch, L., Van Steenkiste, S., &amp; Urgen Schmidhuber, J. ¨. (2019). [<a href="http://louiskirsch.com/code/metagenrl" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_36.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>生物进化将许多学习者的经验提炼为人类的通用学习算法。我们新的元强化学习算法<em>MetaGenRL</em>受此过程启发。<strong><em>MetaGenRL</em>提取了许多复杂智能体的经验，元学习一种低复杂度的神经目标函数，该函数决定了个体在未来将如何学习</strong>。与最近的元强化算法不同，<em>MetaGenRL</em>可以推广到与元训练阶段完全不同的新环境。在某些情况下，它甚至优于人工设计的RL算法。<em>MetaGenRL</em>在元训练期间使用off-policy二阶梯度，可大大提高其样本效率。</li></ul></li><li><p><strong>Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning.</strong> Scobee, D. R. R., &amp; Sastry, S. S. (2019, September 25). [<a href="https://arxiv.org/abs/1909.05477" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>UCB</li><li>尽管逆强化学习（IRL）问题的大多数方法都集中在估计可以最好地解释专家的policy或在控制任务上的演示行为的奖励函数，但通常情况下，这种行为可以通过简单奖励结合一系列严格的约束更简洁地表示。在这种情况下，智能体正试图在这些给定的行为约束框架下最大化累积奖励。<strong>我们对马尔可夫决策过程（MDP）上的IRL问题进行了重新表述，以便在给定环境的nominal模型和nominal奖励函数的情况下，我们寻求在激励了智能体行为的环境中估计状态，动作和特征约束条件</strong>。 我们的方法基于最大熵IRL框架，这使我们能够根据我们对MDP的了解来推断专家演示的似然。使用我们的方法，我们可以推断能将哪些约束添加到MDP，以最大程度地增加观察这些演示得到的似然。我们提出了一种算法，该算法可迭代地推断最大似然约束以最好地解释观察到的行为，并且我们将使用模拟行为和在障碍物附近行走的人类记录数据来评估其效果。</li></ul></li><li><p><strong>THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMENT LEARNING.</strong> Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., … Levine, S. (2019). [<a href="https://openreview.net/forum?id=rJe2syrtvS" target="_blank" rel="noopener">论文链接</a>]*</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_37.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>UCB</li><li>在现实世界中，强化学习的成功仅限于仪器化的实验室场景，通常需要艰苦的人工和监督才能实现持续学习。<strong>在这项工作中，我们讨论了可以不断地，自主地通过现实世界中收集的数据进行改善的机器人学习系统所需的要素</strong>。我们使用dexterous manipulation作为案例研究，提出了这样一个系统的特定实例。随后，我们研究了在没有仪器的情况下学习时会遇到的许多挑战。在这种情况下，学习必须在无需人工设计的复位，仅使用板载感知器并且没有手工设计的奖励函数条件下仍然是可行的。我们提出了针对这些挑战的简单且可扩展的解决方案，然后证明了我们提出的系统在一组机器人dexterous manipulation任务上的有效性，从而提供了与该学习范式相关的挑战的深入分析。我们证明，我们的完整系统可以在没有任何人工干预的情况下进行学习，并通过真实的三爪机器人获得各种基于视觉的技能。</li></ul></li><li><p><strong>Measuring the Reliability of Reinforcement Learning Algorithms.</strong> Chan, S. C. Y., Fishman, S., Canny, J., Korattikara, A., &amp; Guadarrama, S. (2019). [<a href="http://arxiv.org/abs/1912.05663" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_38.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>Google Research</li><li>缺乏可靠性是强化学习（RL）算法的一个众所周知的问题。近年来，这个问题已引起越来越多的关注，并且为改善它而进行的努力已大大增加。<strong>为了帮助RL研究人员和生产用户评估和提高可靠性，我们提出了一套可定量测量可靠性各个方面的指标</strong>。在这项工作中，我们专注于训练期间和学习后（固定policy）的变异性和风险。我们将这些指标设计为通用的，还设计了补充统计测试以对这些指标进行严格的比较。在本文中，我们首先描述度量标准及其设计的期望属性，度量标准的可靠性方面以及它们在不同情况下的适用性。然后，我们描述统计测试并为报告结果提出其他实用建议。度量标准和随附的统计工具已作为开源代码库<a href="https://github.com/google-research/rl-reliability-metrics" target="_blank" rel="noopener">https://github.com/google-research/rl-reliability-metrics</a>提供。我们将度量标准应用于一组通用RL算法和环境，进行比较并分析结果。</li></ul></li><li><p><strong>DISAGREEMENT-REGULARIZED IMITATION LEARNING.</strong> Brantley, K., Sun, W., &amp; Henaff, M. (2019). [<a href="https://openreview.net/forum?id=rkgbYyHtwB" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Microsoft Research</li><li><strong>我们提出了一种简单有效的算法，旨在解决模仿学习中的协变量偏移问题</strong>。它进行如下操作：通过在专家演示数据上训练一组集成policy，然后将其预测的方差作为cost，通过RL与有监督的行为克隆cost一同最小化。与对抗式模仿方法不同，它使用易于优化的固定奖励函数。我们证明了该算法的regret界，该算法在时域内是线性的，乘以一个对于行为克隆失败的某些问题显示为低的系数。我们在多个基于像素的Atari环境和连续控制任务上对算法进行了实验评估，结果表明该算法与行为克隆和生成对抗模仿学习相近或明显胜过。</li></ul></li></ul><h2 id="Poster"><a href="#Poster" class="headerlink" title="Poster"></a>Poster</h2><ul><li><p><strong>Graph Convolutional Reinforcement Learning.</strong> Jiang, J., Dun, C., Huang, T., &amp; Lu, Z. (2018). [<a href="http://arxiv.org/abs/1810.09202" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_39.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>北大</li><li>在多智能体环境中，学习如何合作至关重要。关键是要了解智能体之间的相互影响。但是，多智能体环境是高度动态的，智能体不断移动，其邻居不断改变。这使得学习智能体之间相互作用的抽象表示变得困难。<strong>为了解决这些困难，我们提出了图卷积增强学习，其中图卷积适应于多智能体环境潜在图的动态，并且关系内核通过它们的关系表示来捕获智能体之间的相互作用</strong>。利用卷积层从逐渐增加的感受野产生的潜在特征来学习协作，并且通过时间关系正则化来进一步改善协作以保持一致性。从实验结果来看，我们证明了我们的方法在各种协作方案中都大大优于现有方法。</li></ul></li><li><p><strong>Sharing Knowledge in Multi-Task Deep Reinforcement Learning.</strong> Eramo, C. D., Tateo, D., Bonarini, A., Restelli, M., Milano, P., &amp; Peters, J. (2020). 1–18. [<a href="https://openreview.net/forum?id=rkgpv2VFvr" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_40.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li><strong>我们研究了在任务之间共享表示的好处，以便在多任务强化学习中有效使用深度神经网络</strong>。我们利用这样的假设，即从不同的任务中学习，共享共同的属性，有助于推广它们的知识，与学习单个任务相比，可以更有效地提取特征。直观地讲，当由强化学习算法使用时，所得的功能集可提供性能优势。我们通过提供理论上的保证来证明这一点，这些保证强调了方便在任务之间共享表示的条件，并将众所周知的Approximate Value-Iteration的有限时间范围扩展到了多任务设置。此外，我们通过提出三种强化学习算法的多任务扩展来补充我们的分析，这些算法是我们在广泛使用的强化学习基准上进行实验评估的结果，在样本效率和性能方面，它们比单任务同类算法有了显着改进。</li></ul></li><li><p><strong>SQIL: IMITATION LEARNING VIA REINFORCEMENT LEARNING WITH SPARSE REWARDS.</strong> Reddy, S., Dragan, A. D., &amp; Levine, S. (2019). [<a href="https://arxiv.org/abs/1905.11108" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_41.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>UCB</li><li>学习模仿演示中的专家行为可能是富有挑战性的，特别是在高维，观察连续以及动态未知的环境中。基于behavioral cloning（BC）的有监督学习方法存在分布偏移的问题：因为智能体贪婪地模仿演示的动作，它可能会由于误差累积而偏离演示的状态。近来基于强化学习（RL）的方法，例如逆强化学习（inverse RL）和生成对抗式模仿学习（GAIL），通过训练RL智能体去匹配长时程的演示来克服这个问题。由于该任务的真正奖励函数是未知的，因此这些方法通常通过使用复杂且脆弱的近似技术来参与对抗训练，从演示中学习奖励函数。<strong>我们提出了一个简单的替代方法，该替代方法仍然使用RL，但不需要学习奖励函数。关键思想是通过鼓励智能体在遇到新的、分布之外的状态时返回到演示状态，从而激励他们在很长的时间内匹配演示。</strong>为此，我们为智能体提供了在演示状态下匹配演示操作的$r=+1$的恒定奖励，以及对所有其他行为的$r=0$的恒定奖励。我们的方法，我们称为soft Q imitation learning（SQIL），可以通过对任何标准Q-learning或off-policy actor-critic算法进行少量的修改来实现。从理论上讲，我们表明SQIL可以解释为BC利用稀疏先验来鼓励长时程模仿的正则化变体。实验上，我们在Box2D，Atari和MuJoCo中的各种基于图像的以及低维的任务上，SQIL的性能优于BC，与GAIL相比也取得了相近的结果。本文证明了基于RL且具有固定奖励的简单模仿方法与使用学到奖励的更复杂方法一样有效。</li></ul></li><li><p><strong>MAXMIN Q-LEARNING: CONTROLLING THE ESTIMATION BIAS OF Q-LEARNING.</strong> Lan, Q., Pan, Y., Fyshe, A., &amp; White, M. (2019). [<a href="https://github.com/qlan3/Explorer" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Q学习遭受过高估计bias，因为Q学习使用最大估计action value来近似最大action value。已经提出了减少过高估计bias的算法，但是我们对bias与性能如何相互作用以及现有算法减轻bias的程度缺乏了解。<strong>在本文中，我们1）强调高估bias对学习效率的影响取决于环境。2）提出Q学习的一种泛化，称为Maxmin Q-learning，它提供了一个参数来灵活地控制bias；3）从理论上表明，存在一个用于Maxmin Q-learning的参数选择，该参数选择导致无偏估计，且近似方差比Q-learning低；4）使用新的广义Q-learning框架，证明了我们的算法在tabular case下的收敛性，以及多个之前的Q-learning变体的收敛性</strong>。我们通过实验验证了我们的算法可以更好地控制toy environment中的估计bias，并且可以在几个benchmark问题上实现出色的性能。</li></ul></li><li><p><strong>LEARNING EXPENSIVE COORDINATION: AN EVENT-BASED DEEP RL APPROACH.</strong> Yu, R., Wang, X., Wang, R., Zhang, Y., An, B., Shi, Z., &amp; Lai, H. (2019). [<a href="https://openreview.net/forum?id=ryeG924twB" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_42.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>NTU，中山大学</li><li>深度Multi-Agent Reinforcement Learning（MARL）中的现有工作主要着眼于协调合作智能体共同完成某些任务。 但是，在现实世界的许多情况下，智能体都是自私的，例如公司的员工和联赛中的俱乐部。因此，领导者，即公司或联盟的经理，需要向follower提供奖金，以进行有效的协调，我们称之为<em>expensive coordination</em>。<strong>expensive coordination工作的主要困难是：i）领导者在分配奖金时必须考虑长期影响并预测跟随者的行为，并且ii）跟随者之间的复杂互动使训练过程难以收敛，尤其是在领导者的policy会随着时间而改变。在这项工作中，我们通过基于事件的深度RL方法来解决此问题</strong>。我们的主要贡献是三方面的。（1）我们将领导者的决策过程建模为半马尔可夫决策过程，并提出一种新的multi-agent event-based policy gradient来学习领导者的长期policy。（2）我们利用leader-follower consistency scheme来设计follower-aware module和follower-specific attention module，以预测follower的行为并对其行为做出准确的响应。（3）我们提出了一种action abstraction-based policy gradient算法，以减少follower的决策空间，从而加快follower的训练过程。在资源收集，导航和捕食者-猎物游戏中进行的实验表明，我们的方法大大优于最新方法。</li></ul></li><li><p><strong>SVQN: SEQUENTIAL VARIATIONAL SOFT Q-LEARNING NETWORKS.</strong> Huang, S., Su, H., Zhu, J., &amp; Chen, T. (2019). [<a href="https://openreview.net/forum?id=r1xPh2VtPB" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_43.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>清华</li><li>部分可观察的马尔可夫决策过程（POMDP）是流行的、灵活的模型，用于现实世界中的决策应用程序，需要从过去的观察中获取信息以做出最佳决策。用于解决马尔可夫决策过程（MDP）任务的标准强化学习算法不适用，因为它们无法推断未观察到的状态。<strong>在本文中，我们提出了一种用于POMDP的新算法，称为sequential variational soft Q-learning networks（SVQN），它在统一的图模型下形式化了隐藏状态的推断和最大熵强化学习（MERL），并对两者进行了联合优化</strong>。我们进一步设计了一个深度RNN以减少算法的计算复杂度。实验结果表明，SVQN可以利用过去的信息来帮助进行有效的推理决策，并且在一些具有挑战性的任务上优于其他baseline。我们的消融实验表明，SVQN具有随时间推移的泛化能力，并且对观察的干扰具有鲁棒性。</li></ul></li><li><p><strong>RANKING POLICY GRADIENT.</strong> Lin, K., &amp; Zhou, J. (2019). [<a href="https://arxiv.org/abs/1906.09674" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>MSU</li><li>样本效率低下是强化学习（RL）中一个长期存在的问题。最先进的技术可以估计最佳action value，而通常它需要对state-action空间进行广泛搜索并进行不稳定的优化。<strong>为了实现样本效率较高的RL，我们提出了ranking policy gradient（RPG），这是一种学习一组离散action的最佳排名的策略梯度方法</strong>。为了加快对策略梯度方法的学习，我们建立了在最大化收益下限和模仿near-optimal policy而无需访问任何oracle之间的等价关系。这些结果导致了一个通用的off-policy学习框架，该框架保留了最优性，减少了方差并提高了样本效率。我们进行了广泛的实验，结果表明，与最新的policy结合时，RPG可以大大降低样本的复杂性。</li></ul></li><li><p><strong>A Study on Overfitting in Deep Reinforcement Learning.</strong> Zhang, C., Vinyals, O., Munos, R., &amp; Bengio, S. (2018). [<a href="http://arxiv.org/abs/1804.06893" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>google</li><li>近年来，深度强化学习（RL）取得了重大进展。通过大规模的神经网络，精心设计的架构，新的训练算法和大规模并行计算设备，研究人员能够应对许多具有挑战性的RL问题。但是，在机器学习中，更多的训练能力伴随着过拟合的潜在风险。随着深度RL技术被应用于诸如医疗保健和财务等关键问题上时，了解训练好的智能体的普遍行为是重要的。<strong>在本文中，我们对标准RL智能体进行了系统研究，发现它们可能以各种方式过拟合。此外，过拟合可能“稳健”地发生：RL中常用的增加随机性的技术不一定能防止或检测到过拟合</strong>。尤其是，即使所有智能体和学习算法在训练期间都获得了最佳reward时，他们的测试性能也可能大不相同。这些发现要求在RL中使用更有原则性和更仔细评估的协议。我们以对RL过拟合的一般性讨论作为结束，并从归纳偏差的角度研究泛化行为。</li></ul></li><li><p><strong>Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation.</strong> Nair, S., &amp; Finn, C. (2019). [<a href="http://arxiv.org/abs/1909.05829" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_45.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>Google Brain</li><li>视频预测模型与规划算法相结合已显示出使机器人能够仅通过自我监督来学习执行许多基于视觉的任务的希望，从而在杂物丛生且存在看不见物体的场景中实现了新目标。但是，由于长时程视频预测的不确定性以及sampling-based planning optimizer的可伸缩性差，这些方法的一个显着局限性在于进行长时程规划以实现远期目标。<strong>为此，我们提出了一个用于子目标生成和规划的框架，即hierarchical visual foresight（HVF），该框架可生成以目标图像为条件的子目标图像，并将其用于规划</strong>。对子目标图像进行了直接优化，以将任务分解为易于规划的部分，结果，我们观察到该方法自然会将语义上有意义的state标识为子目标。在四项基于视觉的模拟操纵任务中，有三项发现，与没有子目标和无模型RL方法的规划相比，我们的方法将性能提高了近200％。此外，我们的实验表明，我们的方法扩展到了真实，混乱的视觉场景。</li></ul></li><li><p><strong>MULTI-AGENT REINFORCEMENT LEARNING FOR NETWORKED SYSTEM CONTROL.</strong> Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., Abbeel, P., &amp; Science, C. (2017). (1), 1–14. [<a href="https://openreview.net/forum?id=Syx7A3NFvH" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Stanford</li><li><strong>本文考虑了网络系统控制中的多智能体强化学习（MARL）。具体来说，每个智能体都基于local observation和来自相邻邻居的消息来学习分布式控制策略</strong>。我们将这种网络化的MARL（NMARL）问题公式化为时空马尔可夫决策过程，并引入空间discount factor来稳定每个local智能体的训练。此外，我们提出了一种新的可微通信协议，称为NeurComm，以减少NMARL中的信息丢失和非平稳性。基于现实的NMARL自适应交通信号控制和协同自适应巡航控制场景的实验，适当的空间discount factor有效地增强了non-communicative MARL算法的学习曲线，而NeurComm在学习效率和控制性能方面均优于现有的通信协议。</li></ul></li><li><p><strong>THE VARIATIONAL BANDWIDTH BOTTLENECK: STOCHASTIC EVALUATION ON AN INFORMATION BUDGET.</strong> Goyal, A., Bengio, Y., Botvinick, M., &amp; Levine, S. (n.d.). [<a href="https://openreview.net/forum?id=Hye1kTVFDS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_46.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>University of Montreal，Deepmind，UCB</li><li>在许多应用中，期望仅从复杂的输入数据中提取相关信息，这涉及做出决定关于哪些输入特征是相关的。信息瓶颈方法通过在压缩（丢弃无关的输入信息）和预测目标之间保持最佳折衷，将其形式化为信息论优化问题。在许多问题中，包括我们在本工作中考虑的强化学习问题，我们可能更喜欢仅压缩部分输入。这种情况通常是当我们具有标准条件输入（例如状态观察）和“专用的”输入（这可能对应于任务的目标，昂贵的计划算法的输出或与其他智能体的通信）时。 在这种情况下，我们可能更喜欢压缩专用输入，以实现更好的通用性（例如，相对于目标），或者最小化对昂贵信息的访问（例如，在交流的情况下）。基于变分推断的信息瓶颈的实际实现需要访问专用输入才能计算瓶颈变量，因此，尽管它们执行压缩，但此压缩操作本身需要不受限制的无损访问。<strong>在这项工作中，我们提出了可变带宽瓶颈，该瓶颈会在查看专用信息之前就为每个示例决定专用信息的估计值，即仅基于标准输入，然后相应地随机选择是访问专用输入还是不访问</strong>。我们为该框架制定了一个易于处理的近似，并在一系列强化学习实验中证明了它可以提高泛化能力并减少对计算上昂贵的信息的访问。</li></ul></li><li><p><strong>LEARNING THE ARROW OF TIME FOR PROBLEMS IN REINFORCEMENT LEARNING.</strong> Nasim Rahaman, Steffen Wolf, Anirudh Goyal, Roman Remme, Y. B. (n.d.). [<a href="https://openreview.net/forum?id=rylJkpEtwS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们人类对时间的不对称发展有着天生的理解，我们可以用来高效、安全地感知和操纵环境。<strong>从中汲取灵感，我们解决了在马尔可夫（决策）过程中学习arrow of time的问题</strong>。我们将说明学到的arrow of time如何捕获有关环境的重要信息，这些信息又可以用于衡量可达性，检测副作用并获得内在的奖励信号。最后，我们提出一种简单而有效的算法来参数化当前问题，并使用函数逼近器（此处为深度神经网络）学习arrow of time。我们的实验结果涵盖了一系列离散和连续的环境，并针对一类随机过程证明了学习的arrow of time与Jordan, Kinderlehrer, and Otto (1998)提出的arrow of time概念相当吻合。</li></ul></li><li><p><strong>Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives.</strong> Goyal, A., Sodhani, S., Binas, J., Peng, X. Bin, Levine, S., &amp; Bengio, Y. (2019). [<a href="http://arxiv.org/abs/1906.10667" target="_blank" rel="noopener">论文链接</a>] [<a href="https://sherlockbear.github.io/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/" target="_blank" rel="noopener">阅读笔记</a>]*</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/AcroRd32_CCYGAesAxx.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>UCB</li><li>原语生成，信息论，去中心化原语决策</li><li>在各种复杂环境中运行的强化学习智能体可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息论机制来实现此分散决策：<strong>每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互</strong>。对原语进行正则化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。</li></ul></li><li><p><strong>EXPLORATION IN REINFORCEMENT LEARNING WITH DEEP COVERING OPTIONS.</strong> Jinnai, Y., Park, J. W., Machado, M. C., Brain, G., &amp; Konidaris, G. (2019). [<a href="https://openreview.net/forum?id=SkeIyaVtwB" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_49.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>Brown University</li><li>尽管已经提出了许多选择发现方法来加速强化学习中的探索，但它们常常是启发式的。最近，covering options被提出，以发现一组可证明减少了环境覆盖时间（用于衡量探索难度）上限的选项。但是，它们仅限于表格任务，不适用于具有较大或连续状态空间的任务。<strong>我们介绍了<em>deep covering options</em>，这是一种在线方法，可以将covering options扩展到大型状态空间，自动发现与任务无关的选项以鼓励探索</strong>。我们在几个具有挑战性的稀疏奖励域中评估了我们的方法，结果表明我们的方法识别出了状态空间中较少探索的区域，并成功地生成了访问这些区域的选择，从而极大地改善了探索和总累积奖励。</li></ul></li><li><p><strong>WATCH, TRY, LEARN: META-LEARNING FROM DEMONSTRATIONS AND REWARDS.</strong> Zhou, A., Jang Google Brain, E., Kappler, D., Herzog, A. X., Khansari, M., Wohlart, P., … Finn Google Brain, C. (2019). [<a href="https://sites.google.com/view/watch-try-learn-project" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_50.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>Google Brain</li><li>模仿学习使智能体可以从演示中学习复杂的行为。但是，学习基于视觉的复杂任务可能需要大量不现实的演示。元模仿学习是一种有前途的方法，可以使智能体通过学习类似任务的经验，从一个或几个演示中学习新任务。<strong>在任务模棱两可或没有观察到动态的情况下，仅凭演示可能无法提供足够的信息。智能体还必须尝试执行任务以成功推断策略。在这项工作中，我们提出了一种方法，该方法可以从演示和具有稀疏奖励反馈的试错经验中学习</strong>。与元模仿相比，此方法使智能体能够有效且高效地自我改进，超越了演示数据。与元强化学习相比，由于演示减轻了探索负担，因此我们可以扩展到更广泛的任务分布上。我们的实验表明，在一系列具有挑战性，基于视觉的控制任务上，我们的方法明显优于以前的方法。</li></ul></li><li><p><strong>V-MPO: ON-POLICY MAXIMUM A POSTERIORI POLICY OPTIMIZATION FOR DISCRETE AND CONTINUOUS CONTROL.</strong> Song, H. F., Abdolmaleki, A., Springenberg, T., Clark, A., Soyer, H., Rae, J. W., … Botvinick, M. M. (2019). [<a href="https://arxiv.org/abs/1909.12238" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_51.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>DeepMind</li><li>一些应用于有挑战的离散和连续控制领域里最成功的深度强化学习，在on-policy设置中使用了策略梯度方法。但是，策略梯度可能会受到较大方差的影响，这可能会限制性能，并且在实践中需要仔细调整熵正则化以防止策略崩溃。<strong>作为策略梯度算法的替代方法，我们介绍了V-MPO，它是Maximum a Posteriori Policy Optimization（MPO）的on-policy变体，它基于学到的state-value函数执行策略迭代</strong>。我们展示了V-MPO在多任务设置中超过了Atari-57和DMLab-30 benchmark套件的先前报告的分数，并且在没有重要度加权，熵正则化或population-based超参数调整的情况下可靠地做到了这一点。在individual DMLab和Atari级别上，我们提出的算法可以获得比以前报告的分数更高的分数。V-MPO也适用于具有高维连续动作空间的问题，我们在学习控制具有22个全状态观察自由度和56个像素观查自由度的人体模拟的过程中证明了这一点，以及例如，OpenAI Gym任务中，V-MPO的渐近得分远高于以前报道的水平。</li></ul></li><li><p><strong>AUTOMATED CURRICULA THROUGH SETTER-SOLVER INTERACTIONS.</strong> Racanì, S., Lampinen, A. K., Santoro, A., Reichert, D. P., Firoiu, V., &amp; Lillicrap Deepmind, T. P. (2019). [<a href="https://arxiv.org/abs/1909.12892" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-17_Mendeley_Desktop_52.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>DeepMind</li><li>强化学习算法使用策略和奖励之间的关联来提高智能体的性能。但是在动态或稀疏的奖励环境中，这些关联通常太小，或者奖励事件太少而无法使学习可行。相反，人类教育依靠课程（将任务分解为简单的，具有丰厚奖励的静态挑战）来建立复杂的行为。尽管课程对于人工智能体也有用，但是手工制作却很耗时。这引导了研究人员去探索自动课程的生成。<strong>在这里，我们探索了在丰富、动态的环境中自动生成课程的方法。通过使用setter-solver范式，我们展示了考虑目标有效性、目标可行性和目标覆盖范围以构建有用课程的重要性</strong>。我们展示了我们的方法在丰富但稀疏的2D和3D环境中的成功，在这种环境中，智能体要实现一个目标，该目标选自episode之间可能变化的一组可行目标，并确定未来工作的挑战。最后，我们演示了一种新技术的价值，该技术可以指导智能体朝着理想的目标分布方向发展。总而言之，这些结果代表了朝着应用自动任务课程去学习复杂的、用其他方法无法学习的目标迈出的重要一步，据我们所知，这是第一个展示可行目标在episode之间变化的环境中，为goal-conditioned智能体自动生成课程。</li></ul></li><li><p><strong>LEARNING SELF-CORRECTABLE POLICIES AND VALUE FUNCTIONS FROM DEMONSTRATIONS WITH NEGATIVE SAMPLING.</strong> Luo, Y., Xu, H., &amp; Ma, T. (2019). [<a href="https://arxiv.org/abs/1907.05634" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Princeton，UCB，Stanford</li><li>模仿学习，再加上强化学习算法，是有希望样本有效地解决复杂控制任务的范式。然而，从演示中学习常常会遭受协变量偏移问题，这会导致所学策略的级联错误。<strong>我们引入了一个conservatively-extrapolated value function的概念，该函数可产生具有自我校正的策略。我们设计了Value Iteration with Negative Sampling（VINS）算法，该算法实际上通过conservative extrapolation来学习此类value functions</strong>。我们展示了VINS可以纠正模拟机器人benchmark任务上的行为克隆策略的错误。我们还提出了使用VINS初始化强化学习算法的算法，该算法在样本效率方面表现优于以前的工作。</li></ul></li><li><p><strong>EXPLORING MODEL-BASED PLANNING WITH POLICY NETWORKS.</strong> Wang, T., &amp; Ba, J. (2019). [<a href="https://github.com/WilsonWangTHU/POPLIN" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>具有模型预测控制或在线规划的Model-based reinforcement learning（MBRL）在样本效率和渐进性能方面都显示了在运动控制任务上的巨大潜力。尽管取得了成功，但是现有的规划方法是从动作空间中随机生成的候选序列中搜索的，这在复杂的高维环境中效率不高。<strong>在本文中，我们提出了一种新颖的MBRL算法，即model-based policy planning（POPLIN），该算法将策略网络与在线规划相结合</strong>。更具体地说，我们使用神经网络将每个时间步的行动规划形式化为优化问题。我们尝试了两种优化从策略网络初始化动作序列，然后直接在线优化策略网络的参数。我们证明，在MuJoCo benchmark测试环境中，POPLIN的采样效率比以前的最新算法（例如PETS，TD3和SAC）高出约3倍。为了说明我们算法的有效性，我们证明了参数空间中的优化曲面比动作空间中的曲面更平滑。此外，我们发现对于某些环境（例如，Cheetah），在测试期间无需扩展模型预测控制的情况下，就可以有效地应用提炼策略网络。代码在此处发布<a href="https://github.com/WilsonWangTHU/POPLIN" target="_blank" rel="noopener">https://github.com/WilsonWangTHU/POPLIN</a>。</li></ul></li><li><p><strong>ADVERSARIAL POLICIES: ATTACKING DEEP REINFORCEMENT LEARNING.</strong> Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S., &amp; Russell, S. (2019). [<a href="https://adversarialpolicies.github.io/" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-18_Mendeley_Desktop_53.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>UCB</li><li>众所周知，深度强化学习（RL）策略容易受到其观察结果的对抗性干扰，类似于分类器的对抗性示例。但是，攻击者通常无法直接修改其他智能体的观察结果。这可能会让人产生疑问：是否可以通过选择在多智能体环境中采取的对抗策略来攻击RL智能体，从而创建具有对抗性的自然观察结果？<strong>我们演示了在具有本体感受观察的模拟人形机器人之间的零和博弈中对抗策略的存在，对抗通过self-play训练对对手具有鲁棒性的state-of-the-art victim</strong>。对抗性政策可靠地击败了victim，但产生了看似随机和不协调的行为。我们发现，这些策略在高维环境中更为成功，并在victim策略网络中诱导出与victim对抗正常对手时完全不同的激活。微调可以保护victim免受特定对手的攻击，但是可以成功地重新应用攻击方法以找到新的对手策略。可以从<a href="https://adversarialpolicies.github.io/" target="_blank" rel="noopener">https://adversarialpolicies.github.io/</a>获得视频。</li></ul></li><li><p><strong>VIDEOFLOW: A CONDITIONAL FLOW-BASED MODEL FOR STOCHASTIC VIDEO GENERATION.</strong> Kumar, M., Babaeizadeh, M., Erhan, D., Finn, C., Levine, S., Dinh, L., &amp; Kingma, D. (2019). [<a href="https://arxiv.org/abs/1903.01434" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>GoogleBrain</li><li>原则上，可以建模和预测未来事件序列的生成模型可以学习捕获复杂的现实世界现象，例如物理交互。但是，视频预测中的一个主要挑战是，未来是高度不确定的：过去对事件的观察序列可能暗示着许多可能的未来。尽管最近的许多工作已经研究了可以表示不确定未来的概率模型，但是这种模型要么是像素级自回归模型那样在计算上非常昂贵，要么不能直接优化数据的似然。<strong>据我们所知，我们的工作是第一个提出具有normalizing flow的多帧视频预测的方法，该方法可以直接优化数据似然，并产生高质量的随机预测。我们描述了一种对潜在空间动力学进行建模的方法，并证明了基于flow的生成模型为视频生成建模提供了一种可行且具有竞争力的方法</strong>。</li></ul></li><li><p><strong>Variational Recurrent Models for Solving Partially Observable Control Tasks.</strong> Han, D., Doya, K., &amp; Tani, J. (2019). [<a href="http://arxiv.org/abs/1912.10703" target="_blank" rel="noopener">论文链接</a>]</p><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-18_Mendeley_Desktop_54.png" alt title>                </div>                <div class="image-caption"></div>            </figure></li><li>在部分可观察（PO）的环境中，深度强化学习（RL）智能体经常会表现出无法令人满意的性能，因为有两个问题需要一起解决：如何从原始观察中提取信息以解决任务，以及如何改进策略。<strong>在这项研究中，我们提出了用于解决PO任务的RL算法。我们的方法包括两部分：用于对环境建模的variational recurrent model（VRM），以及可以访问环境和VRM的RL控制器</strong>。所提出的算法在两种类型的PO机器人控制任务中进行了测试，包括坐标或速度均不可观察到，或者需要长期记忆的任务。我们的实验表明，在无法通过简单方式从原始观测值中推断未观测状态的任务中，所提出的算法比其他替代方法具有更高的数据效率和/或学习了更多的最佳策略。代码可以从这里获取到：<a href="https://github.com/oist-cnru/Variational-Recurrent-Models" target="_blank" rel="noopener">https://github.com/oist-cnru/Variational-Recurrent-Models</a></li></ul></li><li><p><strong>POPULATION-GUIDED PARALLEL POLICY SEARCH FOR REINFORCEMENT LEARNING.</strong> Jung, W., Park, G., &amp; Sung, Y. (2019). [<a href="https://openreview.net/forum?id=rJeINp4KwH" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在本文中，提出了一种新的population-guided的并行学习方案，以增强off-policy强化学习（RL）的性能。<strong>在提出的方案中，具有相同的value-function和策略的多个相同的学习器共享一个共同的经验replay buffer，并在最佳策略信息的指导下协作搜索一个好的策略</strong>。关键是通过构建用于策略更新的增强损失函数以扩大多个学习器的整体搜索范围，从而以柔和的方式融合最佳策略的信息。先前最佳策略的指导和扩大的范围可以更快，更好地搜索策略。理论上证明了所提方案对预期累积收益的单调改善。通过将提出的方案应用于twin delayed deep deterministic（TD3）策略梯度算法来构造当前算法。数值结果表明，所构造的算法优于大多数当前最新的RL算法，并且在稀疏的奖励环境下，收益非常可观。</li></ul></li></ul><!-- TODO 仍在更新…… --><ul><li><p><strong>Q-LEARNING WITH UCB EXPLORATION IS SAMPLE EFFICIENT FOR INFINITE-HORIZON MDP.</strong> Dong, K., Wang, Y., Chen, X., &amp; Wang, L. (2019). [<a href>论文链接</a>]</p><ul><li>强化学习中的一个基本问题是，无模型算法是否有效。 最近，Jin等。 （2018）提出了一种基于UCB探索策略的Q学习算法，并证明了它对于有限水平的情节式MDP具有几乎最佳的后悔约束。 在本文中，我们在不访问生成模型的情况下，将具有UCB探索奖金的Q学习应用于具有折扣奖励的无限水平MDP。 我们证明了我们算法探索的样本复杂度受〜O（SAα2（1-γ）7）。 在这种情况下，通过延迟Q学习（Strehl）可以达到〜O（SA？4（1-γ）8）的最佳结果。等（2006年），并根据？匹配下限。 以及S和A直至对数因子。</li></ul></li><li><p><strong>ACTION SEMANTICS NETWORK: CONSIDERING THE EFFECTS OF ACTIONS IN MULTIAGENT SYSTEMS.</strong> Wang, W., Yang, T., Liu, Y., Hao, J., Hao, X., Hu, Y., … Gao, Y. (2019). [<a href="https://sites.google.com/view/iclrasn" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在多智能体系统（MASs）中，每个智能体都做出各自的决定，但是它们全都对系统演化做出了全局贡献。在MASs中学习很困难，因为每个智能体的行动选择必须在其他共同学习的智能体在场的情况下进行。此外，环境随机性和不确定性随着智能体数量的增加而呈指数增长。先前的工作将各种多智能体协调机制借鉴到深度学习架构中，以促进多智能体协调。但是，它们都不明确考虑智能体之间动作的语义，即不同的动作对其他智能体的影响不同。在本文中，<strong>我们提出了一种新的网络体系结构，称为动作语义网络（ASN），可以明确表示智能体之间的这种动作语义</strong>。 ASN使用基于它们之间动作语义的神经网络来表征不同动作对其他智能体的影响。ASN可以轻松地与现有的深度强化学习（DRL）算法结合使用，以提高其性能。在StarCraft II micromanagement和Neural MMO上的实验结果表明，与几种网络体系结构相比，ASN可以显着提高最新DRL方法的性能。</li></ul></li><li><p><strong>VID2GAME: CONTROLLABLE CHARACTERS EX-TRACTED FROM REAL-WORLD VIDEOS.</strong> Gafni, O., Wolf, L., &amp; Taigman, Y. (2019). [<a href>论文链接</a>]</p><ul><li>我们从执行某项活动的人的视频中提取可控模型。 该模型根据用户定义的控制信号生成该人的新颖图像序列，该信号通常会标记移动物体的位移。 生成的视频可以具有任意背景，并且可以有效地捕获人的动态和外观。 该方法基于两个网络。 第一个将当前姿势和单实例控制信号映射到下一个姿势。 第二个将当前姿势，新姿势和给定的背景映射到输出帧。 这两个网络都包含实现高质量性能的多种新颖性。 从舞者和运动员的各种视频中提取的多个角色可以证明这一点。<br>1个</li></ul></li><li><p><strong>Optimistic Exploration even with a Pessimistic Initialisation.</strong> Rashid, T., Peng, B., Böhmer, W., &amp; Whiteson, S. (2020). [<a href="http://arxiv.org/abs/2002.12174" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>乐观初始化是在强化学习（RL）中进行有效探索的有效策略。在表格形式的情况下，所有可证明有效的无模型算法都依赖于它。然而，尽管从这些可证明有效的表格算法中获得了启发，但无模型的深度RL算法并未使用乐观初始化。特别地，在仅具有正回报的情况下，由于常用的网络初始化方案（悲观的初始化），Q值被初始化为它们的最低可能值。仅初始化网络以输出乐观的Q值是不够的，因为我们不能确保它们对于新颖的状态-动作对保持乐观，这对探索至关重要。我们提出了一个简单的基于计数的增值，以悲观地初始化Q值，从而将乐观的来源与神经网络分开。我们证明了该方案在表格设置中可证明是有效的，并将其扩展到深度RL设置。我们的算法，乐观悲观初始化的Q学习（OPIQ），使用基于计数的奖金来增加基于DQN的代理的Q值估计，以确保在操作选择和自举过程中保持乐观。我们显示，OPIQ优于非乐观DQN变体，这些变体在艰苦的探索任务中利用了基于伪计数的内在动机，并且预测了新型状态作用对的乐观估计。<br>1个</li></ul></li><li><p><strong>FINDING AND VISUALIZING WEAKNESSES OF DEEP REINFORCEMENT LEARNING AGENTS.</strong> Rupprecht, C., Ibrahim, C., &amp; Pal, C. J. (2019). [<a href>论文链接</a>]</p><ul><li>随着视觉感知驱动的深度强化学习变得越来越广泛，人们越来越需要更好地理解和探究所学习的主体。 了解决策过程及其与视觉输入的关系对于识别学习行为中的问题非常有价值。 但是，这个话题在研究界相对未被充分研究。 在这项工作中，我们提出了一种为受过训练的特工合成感兴趣的视觉输入的方法。 这样的输入或状态可能是需要采取特定行动的情况。 此外，能够获得非常高或非常低的报酬的临界状态通常对于理解系统的态势感知很有趣，因为它们可以对应于危险状态。 为此，我们学习了环境状态空间上的生成模型，并使用其潜在空间来优化目标状态的目标函数。 在我们的实验中，我们证明了这种方法可以为各种环境和强化学习方法提供见解。 我们在标准Atari基准游戏以及自动驾驶模拟器中探索结果。 基于我们使用这种技术能够识别行为缺陷的效率，我们相信这种通用方法可以作为AI安全应用程序的重要工具。<br>1个</li></ul></li><li><p><strong>ACTOR-CRITIC PROVABLY FINDS NASH EQUILIBRIA OF LINEAR-QUADRATIC MEAN-FIELD GAMES.</strong> Fu, Z., Yang, Z., Chen, Y., &amp; Wang, Z. (2019). [<a href>论文链接</a>]</p><ul><li>我们研究了具有无限数量座席的离散时间均值马尔可夫博弈，其中每个座席旨在使遍历成本最小化。 我们考虑这样的设置：代理具有相同的线性状态转换和二次成本函数，而代理的聚集效应则由其状态的总体平均值即均值场状态来捕获。 对于这种博弈，基于纳什确定性等价原理，我们为其纳什均衡的存在和唯一性提供了充分的条件。 此外，为了找到Nash平衡，我们提出了一种具有线性函数近似的均值actor-critic算法，该算法不需要了解动力学模型。 具体来说，在我们算法的每次迭代中，我们都使用单主体参与者评判算法在给定当前平均场状态的情况下大致获得每个主体的最优策略，然后更新平均场状态。 特别地，我们证明了我们的算法以线性速率收敛到Nash平衡。 据我们所知，这是将具有函数逼近的无模型强化学习应用于具有可证明的非渐近全局收敛性保证的离散时间均值马尔可夫博弈的第一个成功。</li></ul></li><li><p><strong>Option Discovery using Deep Skill Chaining.</strong> Bagaria, A., &amp; Konidaris, G. (2020). ICLR. [<a href="https://openreview.net/forum?id=B1gqipNYwH" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>自动发现时间上可扩展的动作或技能是分层强化学习的长期目标。<strong>我们提出了一种新的算法，该算法将技能链与深度神经网络相结合，可以在高维、连续域中自动发现技能</strong>。最终的算法，即<em>deep skill chaining</em>，通过这样的属性来构建技能，即当执行一个时使能智能体去执行另一个。（constructs skills with the property that executing one enables the agent to execute another.） 我们证明，在挑战性的连续控制任务中，<em>deep skill chaining</em>显着优于非分层智能体和其他最新的技能发现技术。</li></ul></li><li><p><strong>Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery.</strong> Hartikainen, K., Geng, X., Haarnoja, T., &amp; Levine, S. (2019). [<a href="http://arxiv.org/abs/1907.08225" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>强化学习需要手动指定奖励函数才能学习任务。虽然原则上该奖励函数仅需要指定任务目标，但在实践中，强化学习可能非常耗时甚至不可行，除非对奖励函数进行了调整，以便产生平滑的梯度导向成功的结果。但手动调整是很难的，尤其是从原始观察结果（例如图像）获取任务时。<strong>在本文中，我们研究了如何自动学习动态距离：一种从任何其他状态到达给定目标状态的预期时间步个数的度量</strong>。这些动态距离可用于提供well-shaped奖励函数，以实现新的目标，从而有可能有效地学习复杂的任务。我们表明动态距离可以被用于半监督，其中无监督与环境的交互用于学习动态距离，而少量的偏好监督用于确定任务目标，而无需任何人工设计的奖励函数或目标示例。我们在真实机器人和仿真中都评估了我们的方法。我们展示了我们的方法可以使用原始的9自由度机械手学习阀门的转动，使用原始图像观察结果和十个偏好标签，而无需任何其他监督。</li></ul></li><li><p><strong>IMITATION LEARNING VIA OFF-POLICY DISTRIBUTION MATCHING.</strong> Kostrikov, I., Nachum, O., Tompson, J., &amp; Research, G. (2019). IMITATION LEARNING VIA OFF-POLICY DISTRIBUTION MATCHING. [<a href="https://github.com/google-research/" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>当从专家演示中进行模仿学习时，分布匹配是一种流行的方法，其中一种是在估计分配比率之间交替，然后在标准强化学习（RL）算法中将这些比率用作奖励。 传统上，分配比率的估计需要策略数据，这导致先前的工作要么数据效率过高，要么以能够极大地改变其最佳目标的方式更改原始目标。 在这项工作中，我们展示了如何以一种有原则的方式将原始分配比率估算目标转化为完全脱离政策的目标。 除了提供的数据效率外，我们还可以证明该目标也使得无需使用单独的RL优化。 而是，可以从该目标直接学习模仿策略，而无需使用明确的奖励。 我们将得到的算法称为ValueDICE，并在一套流行的模仿学习基准上对其进行评估，发现它可以实现最新的样本效率和性能。11</li></ul></li><li><p><strong>CAQL: CONTINUOUS ACTION Q-LEARNING.</strong> Ryu, M., Chow, Y., Anderson, R., Tjandraatmadja, C., Boutilier, C., &amp; Research, G. (2019). CAQL: CONTINUOUS ACTION Q-LEARNING.[<a href>论文链接</a>]</p><ul><li>基于价值的强化学习（RL）方法（例如Q学习）已在多个领域取得成功。 然而，将Q学习应用于连续动作RL问题的一个挑战是最佳Bellman备份所需的连续动作最大化（max-Q）。 在这项工作中，我们开发了CAQL，这是一种用于连续动作Q学习的算法（类），可以使用多个即插即用优化器来解决max-Q问题。 利用最新的深度神经网络优化结果，我们表明可以使用混合整数编程（MIP）来最佳解决max-Q问题。 当Q函数表示具有足够的功效时，基于MIP的优化会产生更好的策略，并且比近似方法（例如梯度上升，交叉熵搜索）更健壮。 我们进一步开发了几种技术来加速CAQL中的推理，尽管它们具有近似性质，但它们的性能很好。 我们将CAQL与具有不同动作约束度的基准连续控制问题上最先进的RL算法进行比较，结果表明，CAQL在严重受限的环境中通常优于基于策略的方法。<br>1个</li></ul></li><li><p><strong>AMRL: AGGREGATED MEMORY FOR REINFORCEMENT LEARNING.</strong> Beck, J., Ciosek, K., Devlin, S., Tschiatschek, S., Zhang, C., &amp; Hofmann, K. (2019). [<a href>论文链接</a>]</p><ul><li>在许多部分可观察的方案中，强化学习（RL）代理必须依靠长期记忆才能学习最佳策略。 我们证明，由于来自环境和探索的随机性，使用自然语言处理和监督学习的技术在RL任务中失败了。 利用我们对RL中传统存储方法的局限性的见解，我们提出了AMRL，这是一类模型，可以学习更好的策略，具有更高的采样效率，并且对噪声输入具有弹性。 具体来说，我们的模型使用标准内存模块来总结短期上下文，然后从标准模型中汇总所有先前状态，而不考虑顺序。 我们表明，这提供了随时间变化的梯度衰减和信噪比方面的优势。 在测试长期记忆的Minecraft和迷宫环境中进行评估后，我们发现，与参数数量相同的基准相比，我们的模型将平均收益提高了19％，与参数更多的基准相比，将其平均收益提高了9％。<br>1个</li></ul></li><li><p><strong>DEEP IMITATIVE MODELS FOR FLEXIBLE INFERENCE, PLANNING, AND CONTROL.</strong> Rhinehart, N., Mcallister, R., &amp; Levine, S. (2019). [<a href="https://sites.google.com/view/imitative-models" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>模仿学习（IL）是学习理想的自主行为的一种吸引人的方法。 但是，指导IL实现任意目标是困难的。 相反，基于计划的算法使用动力学模型和奖励函数来实现目标。 然而，通常难以指定唤起期望行为的奖励功能。 在本文中，我们提出了“模仿模型”，以结合IL和目标导向计划的好处。 模仿模型是期望行为的概率预测模型，能够预测可解释的类似专家的轨迹以实现特定目标。 我们得出了一系列灵活的目标目标，包括受约束的目标区域，不受约束的目标集和基于能量的目标。 我们证明了我们的方法可以利用这些目标成功地指导行为。 在动态模拟自动驾驶任务中，我们的方法明显优于六个IL方法和基于计划的方法，并且可以从专家演示中高效学习而无需在线数据收集。 我们还展示了我们的方法对于未明确指定的目标（如道路错误一侧的目标）是可靠的。</li></ul></li><li><p><strong>CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning.</strong> Yang, J., Nakhaei, A., Isele, D., Fujimura, K., &amp; Zha, H. (2018). [<a href="http://arxiv.org/abs/1809.05188" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>多种协作式多主体控制问题要求主体在实现集体成功的同时实现个人目标。 这种多目标的多主体设置为最近的算法带来了困难，这些算法主要针对具有单一全局奖励的设置，这归因于两个新的挑战：有效地探索学习个人目标达成和合作以他人的成功，以及为他人分配成功的学分 不同代理商的行动与目标之间的互动。 为了解决这两个挑战，我们将问题重构为一个新颖的两阶段课程，其中在学习多人合作之前先学习单人目标达成，然后得出一个新的多目标多人政策梯度。 用于本地化信用分配的信用函数。 我们使用功能增强方案来桥接整个课程的价值和政策功能。 完整的架构称为CM3，其在三种具有挑战性的多目标多主体问题上的学习速度远比现有算法的直接改编快：在困难编队中进行协作导航，在SUMO交通模拟器中协商多车道变更以及战略合作 在Checkers环境中。<br>1个</li></ul></li><li><p><strong>Intrinsic Motivation for Encouraging Synergistic Behavior.</strong> Chitnis, R., Tulsiani, S., Gupta, S., &amp; Gupta, A. (2020). [<a href="http://arxiv.org/abs/2002.05189" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们研究内在驱动在稀疏奖励协同任务中作为强化学习探索bias的作用，这些任务是多个智能体必须共同努力才能实现他们无法单独实现的目标的任务。<strong>我们的核心思想是，协同任务内在驱动的一个好的指导原则是，采取那些如果智能体自己行动，就无法实现的影响世界的行动。因此，我们建议激励智能体采取（联合）行动，这些行动的效果无法通过每个单独智能体的预测组合来预测</strong>。我们研究了这种想法的两种实例，一种基于遇到的真实状态，另一种基于与策略同时训练的动力学模型。尽管前者较为简单，但后者的好处是对采取的动作可微。我们在奖励稀疏的机器人双臂操作和多智能体移动任务中验证了我们的方法。我们发现，与以下两种方法相比，我们的方法产生的效率更高：1）仅使用稀疏奖励进行训练； 2）使用surprise-based的典型内在驱动形式，而这种形式并不偏向于协同行为。视频可在项目网页上找到：<a href="https://sites.google.com/view/iclr2020-synergistic" target="_blank" rel="noopener">https://sites.google.com/view/iclr2020-synergistic</a>。</li></ul></li><li><p><strong>GRAPH CONSTRAINED REINFORCEMENT LEARNING FOR NATURAL LANGUAGE ACTION SPACES.</strong> Ammanabrolu, P., &amp; Hausknecht, M. (2019). [<a href="https://github.com/microsoft/jericho" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>交互式小说游戏是基于文本的模拟，其中，代理人完全通过自然语言与世界互动。 它们是研究如何扩展强化学习代理来满足自然语言理解，部分可观察性以及组合大的基于文本的动作空间中的动作生成等挑战的理想环境。 我们介绍了KG-A2C1，它是一种在基于模板的动作空间中探索并生成动作的同时构建动态知识图的代理。 我们认为，知识图的双重使用来推理游戏状态并限制自然语言的生成是组合性大自然语言动作的可扩展探索的关键。 各种IF游戏的结果表明，尽管动作空间大小呈指数增长，但KG-A2C的表现仍优于目前的IF代理。<br>1个</li></ul></li><li><p><strong>COMPOSING TASK-AGNOSTIC POLICIES WITH DEEP REINFORCEMENT LEARNING.</strong> Qureshi, A. H., Johnson, J. J., Qin, Y., Henderson, T., Boots, B., &amp; Yip, M. C. (2019). [<a href="https://sites.google.com/view/compositional-rl" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>用基本行为的构成去解决迁移学习难题的是构建人工智能的关键要素之一。迄今为止，在学习task-specific的策略或技能方面已经有了大量工作，但几乎没有关注构建与任务无关的必要技能以找到新问题的解决方案。<strong>在本文中，我们提出了一种新的，基于深度强化学习的技能迁移和组合方法，该方法采用智能体的primitive策略来解决未曾见过的任务</strong>。我们在困难的环境中评估我们的方法，在这些环境中，通过标准强化学习（RL）甚至是分层RL的训练策略要么不可行，要么表现出较高的样本复杂性。我们证明了我们的方法不仅可以将技能迁移到新的问题设置中，而且还可以解决既需要任务计划又需要运动控制的挑战性环境，且数据效率很高。</li></ul></li><li><p><strong>Discovering Motor Programs By Recomposing Demonstrations.</strong> Excellence, P. D. (2020). 1–21. [<a href="https://openreview.net/forum?id=rkgHY0NYwr" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在本文中，我们提出了一种从大规模且多样化的操作演示中来学习可重构motor primitives的方法。当前将演示分解为primitives的方法通常采用手动定义的primitives，而绕开了发现这些primitives的难度。另一方面，用于发现primitives的方法对primitive的复杂性进行了限制性假设，这使得任务的适用性限制在了狭窄的范围。<strong>我们的方法试图通过同时学习基础的motor primitives并重组这些primitives以重构原始演示来应对这些挑战</strong>。通过限制primitives分解的简约性和给定primitive的简单性，我们能够学习各种不同的motor primitives，以及它们的连贯潜在表示。我们从定性和定量两个方面证明了我们所学的primitives捕获了演示中语义上有意义的方面。这使我们能够在分层强化学习设置中组合这些primitives，以有效解决机器人操作任务，例如伸手和推手。</li></ul></li><li><p><strong>JELLY BEAN WORLD: A TESTBED FOR NEVER-ENDING LEARNING.</strong> Platanios, E. A., Saparov, A., &amp; Mitchell, T. (2019). [<a href>论文链接</a>]</p><ul><li>机器学习近年来已显示出越来越大的成功。但是，当前的机器学习系统高度专业化，经过特定问题或领域的培训，通常在单个狭窄的数据集上进行培训。另一方面，人类学习具有高度的通用性和适应性。永无止境的学习是一种旨在弥合这一鸿沟的机器学习范例，其目的是鼓励研究人员设计能够学习在更复杂的环境中执行各种相互关联的任务的机器学习系统。迄今为止，还没有任何环境或测试平台可以促进永无止境的学习系统的开发和评估。为此，我们提出了Jelly Bean World测试平台。 “果冻豆世界”允许您在二维网格世界中进行实验，该世界充满了物品并且代理可以在其中导航。该测试平台提供了足够复杂的环境，并且在一般情况下，智能算法应比当前最新的强化学习方法具有更好的性能。它通过产生非平稳环境并促进多任务，多代理，多模式和课程学习设置的实验来实现。我们希望，“果冻豆世界”将激发人们对永无止境的学习以及更广泛的通用情报发展的新兴趣。<br>1个</li></ul></li><li><p><strong>Single Episode Policy Transfer in Reinforcement Learning.</strong> Yang, J., Petersen, B., Zha, H., &amp; Faissol, D. (2019). [<a href="http://arxiv.org/abs/1910.07719" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>转移和适应新的未知环境动态是强化学习（RL）的关键挑战。更大的挑战是在测试时间的一次尝试中可能几乎无法达到最佳效果，而可能无法获得丰厚的回报，而当前的方法却无法解决这一问题，而当前的方法需要多次使用经验来进行适应。为了在具有相关动力学的环境系列中实现单集传输，我们提出了一种通用算法，该算法可优化探测器和推理模型，以快速估算测试动力学的潜在潜变量，然后将其立即用作通用控制策略的输入。这种模块化的方法可以集成最新的算法以用于变化推理或RL。此外，我们的方法不需要在测试时获得奖励，因此可以在现有的自适应方法无法执行的环境中执行。在具有单个情节测试约束的不同实验领域中，我们的方法明显优于现有的自适应方法，并且相对于可靠的传输性能，在基线方面显示出良好的性能。<br>1个</li></ul></li><li><p><strong>MODEL-AUGMENTED ACTOR-CRITIC: BACKPROPAGATING THROUGH PATHS.</strong> Clavera, I., Fu, Y., &amp; Abbeel, P. (2019). [<a href>论文链接</a>]</p><ul><li>当前基于模型的强化学习方法只是将模型用作学习的黑匣子模拟器，以扩充数据以进行策略优化或价值函数学习。在本文中，我们展示了如何通过利用模型的可区分性来更有效地利用该模型。我们构建了一个策略优化算法，该算法在未来的时间范围内使用学习的模型和策略的路径派生方法。通过使用终极价值函数，以行为者批判的方式学习策略，可以防止跨多个时间步骤学习的不稳定性。此外，我们根据模型和值函数中的梯度误差提出了对我们目标的单调改进的推导。我们证明，我们的方法（i）始终比现有的基于模型的现有算法效率更高；（ii）匹配无模型算法的渐近性能；（iii）可以扩展到很长一段时间，通常过去的基于模型的方法都难以解决的问题。<br>1个</li></ul></li><li><p><strong>Synthesizing Programmatic Policies that Inductively Generalize.</strong> Jeevana Priya Inala, Osbert Bastani, Zenna Tavares, A. S.-L. (2019). [<a href="https://openreview.net/forum?id=S1l8oANFDH" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>深度强化学习已成功解决了许多具有挑战性的控制任务。 但是，学到的策略通常很难推广到新颖的环境。 我们提出了一种用于学习可捕获重复行为的程序化状态机策略的算法。 这样一来，他们便具有将泛化到需要任意重复次数的实例的能力，我们称其为归纳泛化。 但是，状态机策略由连续结构和离散结构组成，因此很难学习。 我们提出了一种称为自适应教学的学习框架，该框架通过模仿老师来学习状态机策略。 与传统的模仿学习相反，我们的老师根据学生的结构来自适应地自我更新。 我们展示了如何使用我们的算法来学习归纳推广到新型环境的策略，而传统的神经网络策略却无法做到。<br>1个</li></ul></li><li><p><strong>Robust Reinforcement Learning for Continuous Control with Model Misspecification.</strong> Mankowitz, D. J., Levine, N., Jeong, R., Shi, Y., Kay, J., Abdolmaleki, A., … Riedmiller, M. (2019). [<a href="http://arxiv.org/abs/1906.07516" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们提供了一个框架，可将鲁棒性（过渡动态中的扰动，我们称为模型错误指定）纳入连续控制强化学习（RL）算法中。我们特别专注于将鲁棒性整合到最新的连续控制RL算法中，该算法称为最大后验策略优化（MPO）。我们通过学习针对最坏情况的预期回报目标进行优化并得出相应的鲁棒熵正则化Bellman压缩算子的策略来实现这一目标。此外，我们引入了一个相对保守，软鲁棒，熵调节的物镜，并带有相应的Bellman算子。我们显示，在环境扰动下，健壮和软健政策在9个Mujoco域中的性能均优于非健壮政策。此外，我们在高维，模拟，灵巧的机器人手上显示出改进的鲁棒性能。最后，我们提出了多个调查性实验，这些实验提供了对鲁棒性框架的更深入了解。这包括对另一种连续控制RL算法的适应，以及从离线数据中学习不确定性集。可以在<a href="https://sites.google.com/view/robust-rl上在线找到表演视频。" target="_blank" rel="noopener">https://sites.google.com/view/robust-rl上在线找到表演视频。</a></li></ul></li><li><p><strong>FREQUENCY-BASED SEARCH-CONTROL IN DYNA.</strong> Pan, Y., Mei, J., &amp; Farahmand, A.-M. (2019). [<a href>论文链接</a>]</p><ul><li>基于模型的强化学习已通过经验证明是提高样本效率的成功策略。特别地，Dyna是一个优雅的基于模型的体系结构，将学习和计划结合在一起，为使用模型提供了极大的灵活性。 Dyna中最重要的组件之一称为搜索控制，它是指生成状态或状态-动作对的过程，我们从中查询模型以获取模拟体验。搜索控制对于提高学习效率至关重要。在这项工作中，我们通过搜索值函数的高频区域，提出了一种简单而新颖的搜索控制策略。我们的主要直觉是基于信号处理的香农采样定理，这表明高频信号需要更多的样本来进行重构。我们凭经验表明，高频函数很难近似。这建议了一种搜索控制策略：我们应该使用值函数高频区域的状态来查询模型以获取更多样本。我们开发了一种简单的策略，可以通过梯度和粗麻布范数局部测量函数的频率，并为该方法提供理论依据。然后，我们将我们的策略应用于Dyna中的搜索控制，并进行实验以显示其在基准域上的性质和有效性。<br>1个</li></ul></li><li><p><strong>Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning.</strong> Ali Mousavi, Lihong Li, Qiang Liu, D. Z. (n.d.). [<a href="https://openreview.net/forum?id=S1ltg1rFDS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在许多现实生活中的应用（例如医疗保健和机器人技术）中，对长视距问题的非策略估计很重要，在这些应用中，可能无法使用高保真模拟器，而对策略的评估却非常昂贵或不可能。 最近，刘等。 （2018）提出了一种避免典型的基于重要性抽样的方法遭受地平线诅咒的方法。 尽管显示出令人鼓舞的结果，但这种方法在实践中受到限制，因为它需要从已知行为策略的固定分布中提取数据。 在这项工作中，我们提出了一种消除此类限制的新颖方法。 特别是，我们将问题公式化为解决某个运营商的固定点，并开发了一种新的估算器，该估算器可以计算平稳分布的重要性比，而无需知道如何收集非政策数据。 我们分析其渐近一致性和有限样本推广。 基准测试证明了该方法的有效性。<br>1个</li></ul></li><li><p><strong>MULTI-AGENT INTERACTIONS MODELING WITH CORRELATED POLICIES.</strong> Liu, M., Zhou, M., Zhang, W., Zhuang, Y., Wang, J., Liu, W., &amp; Yu, Y. (2019). [<a href="https://openreview.net/forum?id=B1gZV1HYvS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>上交, 华为</li><li>在多智能体系统中，由于主体之间的高关联性，导致了复杂的交互行为。但是，以前通过演示对多智能体交互建模的工作主要是通过假设策略及其奖励结构之间相互独立来进行约束的。<strong>本文将多智能体交互建模问题转化为一个多智能体模仿学习框架，该框架通过对对手策略的近似，对相关策略进行显式建模，从而恢复出能够重新生成相似交互的智能体策略</strong>。 因此，我们开发了Decentralized Adversarial Imitation Learning algorithm with Correlated policies（CoDAIL），该算法可进行去中心化的训练和执行。 各种实验表明，CoDAIL可以更好地重新生成复杂的与演示者相似的交互，并且胜过最新的多智能体模仿学习方法。 我们的代码可从<a href="https://github.com/apexrl/CoDAIL" target="_blank" rel="noopener">https://github.com/apexrl/CoDAIL</a>获得。</li></ul></li><li><p><strong>THINKING WHILE MOVING: DEEP REINFORCEMENT LEARNING WITH CONCURRENT CONTROL.</strong> Xiao, T., Jang, E., Kalashnikov, D., Levine, S., Ibarz, J., Hausman, K., … Berkeley, U. C. (2019). [<a href="https://sites.google.com/view/thinkingwhilemoving" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们在这样的环境中研究强化学习，即必须在受控系统的时间演变过程中同时从策略中采样一个动作，例如何时机器人必须在决定下一动作的同时仍执行前一个动作。就像人或动物一样，机器人必须同时思考和移动，在上一个动作完成之前决定下一个动作。为了开发用于此类并发控制问题的算法框架，我们从贝尔曼方程的连续时间公式开始，然后以了解系统延迟的方式离散化它们。通过对现有基于值的深度强化学习算法的简单体系结构扩展，我们实例化了此类新的近似动态编程方法。我们在模拟基准任务和大型机器人抓握任务（机器人必须“边走边思考”）上评估我们的方法。可以在<a href="https://sites.google.com/view/thinkingwhilemoving上获得视频。" target="_blank" rel="noopener">https://sites.google.com/view/thinkingwhilemoving上获得视频。</a><br>1个</li></ul></li><li><p><strong>EVOLUTIONARY POPULATION CURRICULUM FOR SCALING MULTI-AGENT REINFORCEMENT LEARNING.</strong> Long, Q., Zhou, Z., Gupta, A., Fang, F., Wu, Y., &amp; Wang, X. (2019). [<a href="https://openreview.net/forum?id=SJxbHkrKDH" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>CMU, 上交</li><li>在多智能体游戏中，环境的复杂性会随着智能体数量的增加而呈指数增长，因此，当智能体数量众多时，学习良好的策略尤其具有挑战性。<strong>在本文中，我们介绍了<em>Evolutionary Population Curriculum</em>（EPC），这是一种课程学习范式，它通过逐步增加训练智能体的数量来扩大多智能体强化学习（MARL）的规模</strong>。此外，EPC使用进化方法来解决整个课程中的objective misalignment问题：在早期以少量成功训练的智能体不一定是适应数量规模较大的后期的最佳人选。具体而言，EPC在每个阶段维护多组智能体，对这些组执行混合匹配和微调，提拔最具适应性的智能体集合到下一阶段。我们在流行的MARL算法MADDPG上实现了EPC，并通过实验证明，随着智能体数量呈指数增长，我们的方法始终在性能上始终优于baselines。可以在<a href="https://sites.google.com/view/epciclr2020" target="_blank" rel="noopener">https://sites.google.com/view/epciclr2020</a>上找到源码和视频。</li></ul></li><li><p><strong>SAMPLE EFFICIENT POLICY GRADIENT METHODS WITH RECURSIVE VARIANCE REDUCTION.</strong> Xu, P., Gao, F., &amp; Gu, Q. (2019). [<a href>论文链接</a>]</p><ul><li>在强化学习中提高样本效率一直是一个长期存在的研究问题。 在这项工作中，我们旨在降低现有策略梯度方法的样本复杂性。 我们提出了一种新的策略梯度算法，称为SRVR-PG，该算法仅需O（1 /？3/2）1次即可找到非凹面性能函数J（θ）的α-近似平稳点（即， 令？∇J（θ）？2 2≤？）。 该样本复杂度将随机方差减少策略梯度算法的现有结果O（1 /？5/3）提高了O（1 /？1/6）。 此外，我们还提出了带有参数探索功能的SRVR-PG的变体，它从先验概率分布中探索初始策略参数。 我们对强化学习中的经典控制问题进行了数值实验，以验证所提出算法的性能。<br>1个</li></ul></li><li><p><strong>STATE-ONLY IMITATION WITH TRANSITION DYNAM-ICS MISMATCH.</strong> Gangwani, T., &amp; Peng, J. (2019). [<a href="https://github.com/tgangwani/RL-Indirect-imitation" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>模仿学习（IL）是一种流行的范式，它用于培训代理人通过利用专家的行为来实现复杂的目标，而不是处理设计正确的奖励功能的困难。 在将环境建模为马尔可夫决策过程（MDP）的情况下，大多数现有的IL算法都依赖于与要学习新的模仿策略的MDP相同的MDP中的专家演示的可用性。 这在专家和模仿者MDP之间的差异很普遍的许多现实场景中并不常见，特别是在过渡动力学功能中。 此外，获得专家的行动可能代价高昂或不可行，这使得近来朝着仅基于状态的IL（专家演示仅构成状态或观察结果）的趋势大有希望。 在最近基于散度最小化思想的对抗模仿方法的基础上，本文提出了一种新的仅状态IL算法。 它通过引入间接步骤将总体优化目标分为两个子问题，并迭代地解决子问题。 我们表明，当专家和模仿者MDP之间的转换动力学不匹配时，我们的算法特别有效，而基准线IL方法会导致性能下降。 为了对此进行分析，我们通过修改来自OpenAI Gym 1的MuJoCo运动任务的配置参数来构造一些有趣的MDP。<br>1个</li></ul></li><li><p><strong>Explain Your Move: Understanding Agent Actions Using Focused Feature Saliency.</strong> Gupta, P., Puri, N., Verma, S., Kayastha, D., Deshmukh, S., Krishnamurthy, B., &amp; Singh, S. (2019). [<a href="http://arxiv.org/abs/1912.12191" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>随着深度强化学习（RL）应用于更多的任务，就需要形象化和了解所学特工的行为。 显着性图通过突出显示输入状态与代理采取行动最相关的特征来解释代理行为。 现有的基于扰动的计算显着性的方法通常会突出显示与代理采取的行动无关的输入区域。 我们的方法通过平衡两个方面（特异性和相关性）来生成更突出的显着性地图，这两个方面捕获了不同的显着期望。 第一个记录了摄动对将要解释的动作的相对预期回报的影响。 第二减重无关的功能改变了除了要说明的动作以外的动作的相对预期奖励。 我们将我们的方法与经过培训的可玩棋盘游戏（国际象棋和围棋）和Atari游戏（突破，Pong和太空侵略者）的代理商的现有方法进行比较。 通过示例性示例（Chess，Atari，Go），人体研究（Chess）和自动评估方法（Chess），我们的方法所产生的显着性图比现有方法对人类的解释性更高。</li></ul></li><li><p><strong>Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning.</strong> Lee, K., Lee, K., Shin, J., &amp; Lee, H. (2019). [<a href="http://arxiv.org/abs/1910.05396" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>深度强化学习（RL）主体通常无法推广到看不见的环境（但在语义上与受过训练的主体相似），尤其是当它们在高维状态空间（如图像）上受到训练时。 在本文中，我们提出了一种简单的技术，通过引入随机扰乱输入观测值的随机（卷积）神经网络来提高深层RL代理的泛化能力。 通过学习跨变化和随机环境不变的强大功能，它使受过训练的代理能够适应新的领域。 此外，我们考虑了一种基于蒙特卡洛近似的推理方法，以减少由该随机化引起的方差。 我们证明了我们的方法在2D CoinRun，3D DeepMind Lab探索和3D机器人控制任务中的优越性：在同一目的上，它明显优于各种正则化和数据增强方法。 可以在github.com/pokaxpoka/netrand上找到代码。<br>1个</li></ul></li><li><p><strong>RIDE: REWARDING IMPACT-DRIVEN EXPLORATION FOR PROCEDURALLY-GENERATED ENVIRONMENTS.</strong> Raileanu, R., &amp; Rocktäschel, T. (2019). [<a href>论文链接</a>]</p><ul><li>在稀疏奖励环境中进行探索仍然是无模型强化学习的主要挑战之一。 许多最先进的方法不仅仅依赖于环境提供的外部奖励，而是使用内在奖励来鼓励探索。 但是，我们表明，现有方法在程序生成的环境中不起作用，在该环境中，代理不可能多次访问某个状态。 我们提出了一种新型的内在奖励，这种内在奖励会鼓励代理采取行动，从而导致其学习状态表示形式发生重大变化。 我们在MiniGrid中对多个具有挑战性的程序生成的任务以及先前工作中使用的具有高维观察力的任务上评估我们的方法。 我们的实验表明，这种方法比现有的探索方法更有效，尤其是在程序生成的MiniGrid环境中。 此外，我们还分析了代理商的学习行为以及内在报酬。 与以前的方法相比，我们的内在报酬在训练过程中不会减少，并且由于与它可以控制的对象进行交互，它会大大奖励代理商。<br>1个</li></ul></li><li><p><strong>PROJECTION-BASED CONSTRAINED POLICY OPTIMIZATION.</strong> Yang, T.-Y., Rosca, J., Narasimhan, K., &amp; Ramadge, P. J. (2019). [<a href="https://sites.google.com/view/iclr2020-pcpo" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>我们考虑了学习控制策略的问题，该策略优化了奖励功能，同时又出于安全性，公平性或其他成本的考虑而满足约束条件。 我们提出了一种新算法，即基于投影的约束策略优化（PCPO）。 这是一种通过两步过程优化策略的迭代方法：第一步执行本地奖励改进更新，而第二步通过将策略投影回约束集来解决任何约束违规问题。 我们从理论上分析PCPO，并为每次策略更新提供了奖励改进的下限和约束违反的上限。 我们基于两个不同的指标进一步描述了PCPO的收敛性：L2范数和Kullback-Leibler差异。 我们在几个控制任务上的经验结果表明，与最先进的方法相比，PCPO的性能更高，约束违规平均减少3.5倍以上，回报提高约15％。11</li></ul></li><li><p><strong>COMBINING Q-LEARNING AND SEARCH WITH AMORTIZED VALUE ESTIMATES.</strong> Hamrick DeepMind, J. B., Bapst DeepMind, V., Sanchez-Gonzalez DeepMind, A., Pfaff DeepMind, T., Weber DeepMind, T., Buesing DeepMind, L., &amp; Battaglia DeepMind, P. W. (2019). [<a href>论文链接</a>]</p><ul><li>我们介绍了“使用摊销价值估计进行搜索”（SAVE），一种将无模型Q学习与基于模型的蒙特卡洛树搜索（MCTS）相结合的方法。 在SAVE中，学习的先于状态操作值用于指导MCTS，MCTS估计状态操作值的改进集合。 然后将新的Q估计值与实际经验结合使用以更新先验值。 这有效地摊销了MCTS进行的价值计算，从而导致了无模型学习与基于模型的搜索之间的合作关系。  SAVE可以在任何具有模型访问权限的Q学习代理上实现，我们通过将其合并到执行具有挑战性的物理推理任务和Atari的代理中来进行演示。 与典型的基于模型的搜索方法相比，SAVE始终以较少的培训步骤来获得更高的回报，并且以很小的搜索预算即可获得出色的性能。 通过将实际经验与搜索过程中计算出的信息相结合，SAVE证明了可以同时提高无模型学习的性能和计划的计算成本。<br>1个</li></ul></li><li><p><strong>TOWARD EVALUATING ROBUSTNESS OF DEEP REIN-FORCEMENT LEARNING WITH CONTINUOUS CONTROL.</strong> Weng, T.-W., Dvijotham, K., Uesato, J., Xiao, K., Gowal, S., Stanforth, R., &amp; Kohli, P. (2019). [<a href>论文链接</a>]</p><ul><li>深度强化学习在许多以前很难进行的强化学习任务中都取得了巨大的成功，但是最近的研究表明，类似于分类任务中的深层神经网络，深层RL主体也不可避免地容易受到对抗性干扰。 先前的工作主要集中在无模型的广告攻击和具有离散动作的代理上。 在这项工作中，我们研究了具有对抗性攻击的深层RL中的连续控制主体问题，并基于学习的模型动力学提出了第一个两步算法。 在各种MuJoCo域（Cartpole，Fish，Walker，Humanoid）上的大量实验表明，我们提出的框架在降低代理性能以及将代理驱动到不安全状态方面比无模型攻击基准有效得多。</li></ul></li><li><p><strong>STRUCTURED OBJECT-AWARE PHYSICS PREDICTION FOR VIDEO MODELING AND PLANNING.</strong> Kossen, J., Stelzner, K., Hussing, M., Voelcker, C., &amp; Kersting, K. (2019). [<a href>论文链接</a>]</p><ul><li>当人们观察物理系统时，他们可以轻松地定位物体，了解其相互作用并预测未来的行为。 然而，对于计算机而言，以无人监督的方式从视频中学习此类模型是尚未解决的搜索问题。 在本文中，我们介绍了STOVE，这是一种新颖的视频状态空间模型，可明确说明物体及其位置，速度和相互作用的原因。 它是通过以组合的方式将图像模型和动力学模型组合在一起而构造的，并且通过将动力学模型用于推理，加速和规范化训练来改进以前的工作。  STOVE可以预测经过数千步具有令人信服的身体行为的视频，胜过以前的非监督模型，甚至可以达到监督基线的性能。 我们进一步证明了我们的模型作为仿真器的优势，该仿真器用于在具有大量交互对象的任务中进行基于样本的高效模型控制。<br>1个</li></ul></li><li><p><strong>INFINITE-HORIZON OFF-POLICY POLICY EVALUATION WITH MULTIPLE BEHAVIOR POLICIES.</strong> Chen, X., Wang, L., Hang, Y., Ge, H., &amp; Zha, H. (2019). [<a href>论文链接</a>]</p><ul><li>当轨迹数据由多个行为策略生成时，我们将考虑策略外策略评估。 最近的工作表明，在无限期的背景下，国家或国家行动的平稳分布校正对离岸政策评估起着关键作用。 我们提出了估计混合策略（EMP），这是一种与策略无关的方法，可以准确地估计这些数量。 通过仔细的分析，我们表明EMP产生了减少的方差估计值，用于估计状态平稳分布校正，同时它还提供了有用的归纳偏差，用于估计状态作用平稳分布校正。 在连续和离散环境下的大量实验中，我们证明了与现有技术方法相比，我们的算法可显着提高准确性。<br>1个</li></ul></li><li><p><strong>MAKING EFFICIENT USE OF DEMONSTRATIONS TO SOLVE HARD EXPLORATION PROBLEMS.</strong> Gulcehre, C., Le Paine, T., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., … Wang, Z. (2019). [<a href>论文链接</a>]</p><ul><li>本文介绍了R2D3，它是一种可以有效利用演示来解决在初始条件高度可变的部分可观察环境中解决硬勘探问题的代理。 我们还介绍了包含这三个属性的八个任务套件，并显示了R2D3可以解决其他一些现有技术方法（无论有无演示）都无法在数百亿个成功轨迹之后看到的几个任务 探索步骤。<br>1个 </li></ul></li><li><p><strong>VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning.</strong> Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., &amp; Whiteson, S. (2019). [<a href="http://arxiv.org/abs/1910.08348" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在未知环境中权衡探索和开发是最大程度地提高学习期间的预期回报的关键。 一种贝叶斯最佳策略，该策略以最佳方式进行操作，不仅取决于环境状态，还取决于智能体对环境的不确定性，决定其行动。 但是，除了最小的任务外，计算贝叶斯最优策略是很棘手的。 在本文中，我们介绍了变分贝叶斯自适应深度RL（variBAD），这是一种在未知环境中进行元学习以进行近似推理的方法，并直接在操作选择过程中纳入了不确定性。 在网格世界中，我们说明了variBAD如何根据任务不确定性执行结构化的在线探索。</li></ul></li><li><p><strong>KEEP DOING WHAT WORKED: BEHAVIOR MODELLING PRIORS FOR OFFLINE REIN-FORCEMENT LEARNING.</strong> Siegel, N. Y., Springenberg, T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., … Deepmind, M. R. (2019). [<a href>论文链接</a>]</p><ul><li>非政策强化学习算法有望适用于只有固定的环境交互数据集（批次）可用且无法获得新经验的环境。 此属性使这些算法对诸如机器人控制之类的现实世界问题具有吸引力。 但是，实际上，标准的非策略算法在批量设置中无法进行连续控制。 在本文中，我们提出了一个简单的解决方案。 它允许使用由任意行为策略生成的数据，并使用获悉的先验知识-优势加权行为模型（ABM）-将RL策略偏向于先前已执行且可能会成功完成新任务的行为。 我们的方法可以看作是对批处理RL的最新工作的扩展，它可以从冲突的数据源中进行稳定的学习。 我们发现各种RL任务的竞争基准都有改进-包括标准连续控制基准和针对模拟和现实机器人的多任务学习。 可以从以下网址获得视频：https：//sites.google.com/view/behavior-modelling-priors。<br>1个</li></ul></li><li><p><strong>META REINFORCEMENT LEARNING WITH AUTONOMOUS INFERENCE OF SUBTASK DEPENDENCIES.</strong> Sohn, S., Woo, H., Choi, J., &amp; Lee, H. (2019). [<a href="https://openreview.net/forum?id=HkgsWxrtPB" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>Michigan, Google Brain</li><li>我们提出并解决了一个新的小样本RL问题，其中任务以子任务图为特征，该子任务图描述了对于智能体来说未知的一组子任务及相互间的依赖关系。智能体需要在适应阶段的几个循环中快速适应任务，以使测试阶段的收益最大化。<strong>我们没有直接学习元策略，而是开发了Meta-learner with Subtask Graph Inference（MSGI），通过与环境交互来推断任务的潜在参数，并在给定潜在参数的情况下最大化回报</strong>。 为了促进学习，我们采用了内在奖励，这种奖励是受到用于鼓励有效探索的upper confidence bound（UCB）的启发。 我们在两个grid-world domains和StarCraft II环境上的实验结果表明，与现有的元RL和分层RL方法相比，该方法能够准确地推断潜在任务参数，并且能够更有效地进行自适应。<a href="https://bit.ly/msgi-videos" target="_blank" rel="noopener">https://bit.ly/msgi-videos</a>可看到demo视频。</li></ul></li><li><p><strong>Dynamics-aware Embeddings.</strong> Whitney, W., Agarwal, R., Cho, K., &amp; Gupta, A. (2019). [<a href="http://arxiv.org/abs/1908.09357" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>在本文中，我们考虑了自我监督的表示学习，以提高强化学习（RL）中的样本效率。 我们提出了一种前向预测目标，用于同时学习状态和动作序列的嵌入。 这些嵌入捕获了环境动态的结构，从而实现了有效的策略学习。 我们证明，仅通过动作嵌入，就可以提高无模型RL在低维状态控制下的采样效率和峰值性能。 通过结合状态和动作嵌入，我们可以在仅1-2百万个环境步骤中从像素观察中高效学习目标条件连续控制的高质量策略。<br>1个</li></ul></li><li><p><strong>NEVER GIVE UP: LEARNING DIRECTED EXPLORATION STRATEGIES.</strong> Puigdomènech Badia, A., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S., … Blundell, C. (2019). [<a href>论文链接</a>]</p><ul><li>我们建议一个强化学习代理，通过学习一系列定向探索政策来解决艰苦的探索游戏。 我们根据代理人的最新经验，使用k近邻来构造基于情节记忆的内在奖励，以训练定向探索策略，从而鼓励代理人反复访问其环境中的所有状态。 使用自我监督的逆动力学模型来训练最近邻居查找的嵌入，从而将新颖性信号偏向于代理可以控制的范围。 我们采用通用价值函数逼近器（UVFA）框架，以相同的神经网络同时学习许多定向勘探策略，并且在勘探和开发之间进行了折衷。 通过将相同的神经网络用于不同程度的勘探/开发，可以从主要探索性政策中产生转移，从而产生有效的利用性政策。 可以将提出的方法与现代分布式RL代理一起运行，该代理从在不同环境实例上并行运行的许多参与者收集大量经验。 在Atari-57套件的所有艰苦探索中，我们的方法使基本代理的性能翻了一番，同时在其余游戏中保持了很高的分数，获得了人类标准化分数的中位数为1344.0％。 值得注意的是，所提出的方法是第一个在“陷阱”游戏中获得非零奖励（平均分数为8,400）的算法！ 无需使用演示或手工制作的功能。<br>1个</li></ul></li><li><p><strong>ON THE INTERACTION BETWEEN SUPERVISION AND SELF-PLAY IN EMERGENT COMMUNICATION.</strong> Lowe, R., Gupta, A., Foerster, J., Kiela, D., &amp; Pineau, J. (2019). [<a href="https://github.com/backpropper/s2p" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>教人工代理使用自然语言的一种有前途的方法包括使用在环培训。 但是，最近的工作表明，当前的机器学习方法数据效率太低，无法从头开始以这种方式进行训练。 在本文中，我们研究了两类学习信号之间的关系，其最终目的是提高样本效率：通过监督学习来模仿人类语言数据，并通过自我玩耍在模拟的多主体环境中最大化报酬（已完成）。  （在紧急通信中），并为使用这两种信号的算法引入术语监督自播放（S2P）。 我们发现，通过在人类数据上进行有监督的学习，然后通过自学的方式进行的第一个培训代理的表现要好于相反的情况，这表明从头开始出现语言是无益的。 然后，我们根据经验研究各种S2P计划，这些计划从两种环境下的监督学习开始：具有符号输入的Lewis信号游戏和具有自然语言描述的基于图像的参照游戏。 最后，我们介绍了基于群体的S2P方法，与单代理方法相比，它进一步提高了性能。11</li></ul></li><li><p><strong>STATE ALIGNMENT-BASED IMITATION LEARNING.</strong> Liu, F., Ling, Z., Mu, T., &amp; Su, H. (2019). [<a href>论文链接</a>]</p><ul><li>考虑一个模仿者和专家具有不同动力学模型的模仿学习问题。 当前大多数模仿学习方法失败，因为它们专注于模仿动作。 我们提出了一种基于状态对齐的新型模仿学习方法，以训练模仿者在专家演示中尽可能遵循状态序列。 国家的协调来自本地和全球的观点，我们通过定期的政策更新目标将它们整合为强化学习框架。 我们展示了我们的方法在标准模仿学习设置和专家和模仿者具有不同动力学模型的模仿学习设置上的优势。</li></ul></li><li><p><strong>Discriminative Particle Filter Reinforcement Learning for Complex Partial Observations.</strong> Ma, X., Karkus, P., Hsu, D., Lee, W. S., &amp; Ye, N. (2020). [<a href="http://arxiv.org/abs/2002.09884" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>深度强化学习在诸如Atari，Go等复杂游戏的决策中是成功的。但是，现实世界中的决策通常需要推理，并从复杂的视觉观察中提取部分信息。本文介绍了判别式粒子滤波强化学习（DPFRL），这是一种用于复杂局部观测的新型强化学习框架。 DPFRL对神经网络策略中的可微分粒子过滤器进行编码，以进行显式推理，并随时间进行部分观察。粒子过滤器使用经过学习的判别式更新来保持信念，该判别式更新经过端到端的决策训练。我们证明，使用判别式更新而不是标准生成模型可以显着提高性能，特别是对于具有复杂视觉观察的任务，因为他们避免了建模与决策无关的复杂观测的困难。此外，为了从粒子置信度中提取特征，我们基于矩生成函数提出了一种新型的置信度特征。在现有的POMDP RL基准测试《 Flickering Atari》游戏中，DPFRL优于最新的POMDP RL模型；在本文中介绍的新的，更具挑战性的POMDP RL基准测试《 Natural Flickering Atari Games》中，DPFRL优于最新的POMDP RL模型。此外，DPFRL在人居环境中使用真实数据进行视觉导航时表现出色。该代码可在线获得1。<br>1个</li></ul></li><li><p><strong>Episodic Reinforcement Learning With Associiative Memory.</strong> Zhu, G., Lin, Z., Yang, G., &amp; Zhang, C. (2020). [<a href="https://openreview.net/forum?id=HkxjqxBYDB" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>样本效率一直是深度强化学习的主要挑战之一。已经提出了非参数情景控制，通过快速锁定先前成功的策略来加速参数强化学习。但是，以前的情景强化学习工作忽略了状态之间的关系，只将经验存储为无关的项目。为了提高强化学习的样本效率，我们提出了一种新颖的框架，即带有联想记忆的情节强化学习（ER-LAM），该框架将相关的经验轨迹相关联，以实现推理有效策略。我们基于状态转移在内存中的状态之上构建图形，并开发一种有效的反向轨迹传播策略，以允许值通过图形快速传播。我们使用非参数联想记忆作为参数强化学习模型的早期指导。 Atari游戏的结果表明，我们的框架具有显着更高的采样效率，并且胜过了最新的情节式强化学习模型。<br>1个</li></ul></li><li><p><strong>SUB-POLICY ADAPTATION FOR HIERARCHICAL REINFORCEMENT LEARNING.</strong> Li, A. C., Florensa, C., Clavera, I., &amp; Abbeel, P. (2019). [<a href="https://openreview.net/forum?id=ByeWogStDS" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>UCB</li><li>分层强化学习是一种可解决具有稀疏奖励的长期决策问题的有效方法。不幸的是，大多数方法仍然使低级技能的获取过程与控制新任务中技能的高级训练分离开来。保持技能固定会导致迁移设置中出现明显的局部最优。<strong>在这项工作中，我们提出了一种新颖的算法，用于发现一组技能，在接受新任务训练时，仍然可以同高级技能一起不断地适应</strong>。 我们的主要贡献是双重的。首先，我们推导了一个新的具有无偏潜在依赖baseline的分层策略梯度，并引入了Hierarchical Proximal Policy Optimization（HiPPO），一种有效地联合训练分层结构各个级别的on-policy方法。 其次，我们提出了一种训练时间抽象的方法，提高了所获得的技能对环境变化的鲁棒性。代码和视频可以从<a href="https://sites.google.com/view/hippo-rl" target="_blank" rel="noopener">https://sites.google.com/view/hippo-rl</a>找到。</li></ul></li><li><p><strong>LEARNING NEARLY DECOMPOSABLE VALUE FUNC-TIONS VIA COMMUNICATION MINIMIZATION.</strong> Wang, T., Wang, J., Zheng, C., &amp; Zhang, C. (2019). [<a href="https://openreview.net/forum?id=HJx-3grYDB" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>增强学习在多智能体设置中遇到了主要挑战，例如可伸缩性和非平稳性。最近，value function分解学习已成为解决协作多智能体系统中这些挑战的一种有效的方法。但是，现有方法一直专注于学习完全去中心化的value function，这对于需要通信的任务而言效率不高。<strong>为了解决这一局限性，本文提出了一种新的框架，用于通过通信最小化学习<em>nearly decomposable Q-functions</em>（NDQ），智能体大部分时间都在自己行动，但偶尔会向其他智能体发送消息以进行有效的协调。该框架通过引入两个信息论正则化项，将value function分解学习和通信学习结合在一起</strong>。这些正则化函数使智能体的动作选择和通信消息之间的互信息最大化，同时使智能体之间的消息熵最小。我们展示了如何以易于与现有的value function分解方法（例如QMIX）集成的方式来优化这些正则项。最后，我们证明，在《星际争霸》单位的micromanagement benchmark上，我们的框架明显优于baseline方法，并允许我们在不牺牲性能的情况下切断了80％以上的通信。有关实验的视频，请访问<a href="https://sites.google.com/view/ndq" target="_blank" rel="noopener">https://sites.google.com/view/ndq</a>。</li></ul></li><li><p><strong>LEARNING TO COORDINATE MANIPULATION SKILLS VIA SKILL BEHAVIOR DIVERSIFICATION.</strong> Lee, Y., Yang, J., &amp; Lim, J. J. (2019). [<a href="https://clvrai.com/coordination" target="_blank" rel="noopener">论文链接</a>]</p><ul><li>当完成一个复杂的操纵任务时，人们经常将任务分解为身体各个部分的子技能，独立地练习这些子技能，然后一起执行这些子技能。同样，具有多个末端执行器的机器人可以通过协调每个末端执行器的子技能来执行复杂的任务。<strong>为了实现技能的时间和行为协调，我们提出了一个模块化框架，该框架首先通过skill behavior diversification分别训练每个末端执行器的子技能，然后学习使用技能的多种行为来协调末端执行器</strong>。我们证明了我们提出的框架能够有效地协调技能，以解决具有挑战性的协作控制任务，例如捡起一根长棒，在用两个机械手推动容器的同时在容器内放置一个块以及用两个蚂蚁推动容器。 视频和代码可在<a href="https://clvrai.com/coordination" target="_blank" rel="noopener">https://clvrai.com/coordination</a>上获得。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICLR2020论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading List" scheme="https://yachenkang.github.io/blog/categories/Reading-List/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives</title>
    <link href="https://yachenkang.github.io/blog/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/"/>
    <id>https://yachenkang.github.io/blog/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/</id>
    <published>2020-02-17T03:25:52.000Z</published>
    <updated>2020-03-25T07:05:39.064Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>在各种复杂环境中运行的强化学习智能体可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互。对原语进行正则化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。</p><a id="more"></a><h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-24_Mendeley_Desktop_76.png" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>作者：Goyal, A., Sodhani, S., Binas, J., Peng, X. Bin, Levine, S., &amp; Bengio, Y.</li><li>出处：ICLR2020 Poster</li><li>机构：Facebook, UCB</li><li>关键词：原语生成，信息论，去中心化原语决策</li><li><a href="http://arxiv.org/abs/1906.10667" target="_blank" rel="noopener">论文链接</a><!-- * 开源代码： --></li><li>其他资料：<ul><li><a href="https://xbpeng.github.io/projects/CICP/index.html" target="_blank" rel="noopener">主页</a></li><li><a href="https://blog.csdn.net/weixin_41697507/article/details/93900110" target="_blank" rel="noopener">翻译</a></li></ul></li></ul><h3 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h3><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/AcroRd32_CCYGAesAxx.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>尽管分层强化学习通过分解可以使得底层原语可以特异化面对不同状态，并能够被元策略加以组合或选择。然而元策略本身仍然需要处理所有状态空间，从而使得元策略在不同环境中的迁移成为瓶颈。</p><p>该方法包括三个组件：</p><ol><li>一种将特定原语限制在状态空间子集的机制</li><li>原语之间的竞争机制，从而为给定状态选择最有效的原语</li><li>规范化机制，可提高策略整体的泛化性能。</li></ol><h4 id="包含信息瓶颈的原语"><a href="#包含信息瓶颈的原语" class="headerlink" title="包含信息瓶颈的原语"></a>包含信息瓶颈的原语</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/AcroRd32_qhtDGoA5UR.png" alt title>                </div>                <div class="image-caption"></div>            </figure><script type="math/tex; mode=display">\pi_{\theta}^{k}(A | S)=\int_{z} p_{\mathrm{enc}}\left(z_{k} | S\right) p_{\mathrm{dec}}\left(A | z_{k}\right) \mathrm{d} z_{k}</script><p>这里作者对每个原语设计一个信息瓶颈，以防止其利用状态中的全部信息（这里我认为是在迫使原语提取主要信息，以增强泛化性）</p><script type="math/tex; mode=display">\mathcal{L}_{k}=\mathrm{D}_{\mathrm{KL}}\left(p_{\mathrm{enc}}\left(Z_{k} | S\right) \| \mathcal{N}(0,1)\right)</script><p>通过Z（隐变量状态空间）的分布与正态分布之间的KL散度作为罚项以期另Z中包含尽可能少的关于S的信息，文中的表述是原语需要通过对$L_k$支付“信息损失”已获得关于当前状态的更多信息。<br>这里的正态分布可被替换为预先学习的先验。</p><p>尽管信息瓶颈使得原语能获得的关于状态的信息尽可能少，然而并没有约束不同原语去关注状态空间中的不同部分，所以进一步提出了竞争机制以鼓励原语的多样性。</p><h4 id="信息约束的竞争原语"><a href="#信息约束的竞争原语" class="headerlink" title="信息约束的竞争原语"></a>信息约束的竞争原语</h4><p>由于原语面对状态s时的$L_k$体现了其对于当前状态的有效性，所以最高$L_k$的原语应当被激活。这里采用softmax计算归一化权重的方法</p><script type="math/tex; mode=display">\alpha_{k}=\exp \left(\mathcal{L}_{k}\right) / \sum_{j} \exp \left(\mathcal{L}_{j}\right)</script><p>选择$\alpha_{k}$最高的或通过构造分布然后采样的方式，决定要激活的原语。</p><p>通过$r_{k}=\alpha_{k} r,$ 其中 $r=\sum_{k} r_{k}$的方式鼓励每个原语从状态中获取更多信息，与信息瓶颈相拮抗，使得每个原语特异化。</p><h4 id="组合表示的正则化"><a href="#组合表示的正则化" class="headerlink" title="组合表示的正则化"></a>组合表示的正则化</h4><p>采用如下的额外正则化项，鼓励多样化的原语设置，以及保证模型不会坍缩到单一原语</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{reg}}=\sum_{k} \alpha_{k} \mathcal{L}_{k}</script><p>可以重写为</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{reg}}=-H(\alpha)+\operatorname{LSE}\left(\mathcal{L}_{1}, \ldots, \mathcal{L}_{K}\right)</script><p>前项使得$\alpha$熵值增加，进而使得原语选择集合具有多样性。LSE则近似于参数的最大项，$\operatorname{LSE}(x) \approx \max _{j} x_{j}$，因此惩罚了占主导地位的$L_k$项，使他们趋向一致</p><h4 id="目标与算法总结"><a href="#目标与算法总结" class="headerlink" title="目标与算法总结"></a>目标与算法总结</h4><p>总目标函数包含3项：</p><ol><li>来自标准RL目标的期望奖励，$R(\pi)$，根据参与度将其分配给不同原语</li><li>单个瓶颈项导致单个原语专注于状态空间的特定部分，$L_k$，其中$k = 1,\dots,K$</li><li>归一化项应用于组合模型，$L_{reg}$</li></ol><p>对于第k个原语的总目标函数如下：</p><script type="math/tex; mode=display">J_{k}(\theta) \equiv \mathbb{E}_{\pi_{\theta}}\left[r_{k}\right]-\beta_{\mathrm{ind}} \mathcal{L}_{k}-\beta_{\mathrm{reg}} \mathcal{L}_{\mathrm{reg}}</script><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2020-03-25_Goyal_et_al._-_2019_-_REINFORCEMENT_LEARNING_WITH__77.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/AcroRd32_5OImexFX2U.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>如图，每个面板对应于不同的训练设置，其中不同的任务表示为A，B，C，…，具有n个圆圈的矩形对应于由在相应任务上训练的n个原语组成的代理。 第一行：为受过单个任务训练的代理原语的激活。 下面一行：<strong>重新训练</strong>：在A上训练了两个原语并迁移到B。结果（成功率）表明，多原语模型比baseline（transfer A2C）实质上具有更高的样本效率。 <strong>复制和合并</strong>：随着时间的推移，更多的原语以即插即用的方式添加到模型中（在A上训练了2个原语；模型通过自己的副本扩展自身；在B上训练了所得的四原语模型。）对比比其他强baseline更有效。<strong>零样本泛化</strong>：在C上训练一组原语，并评估对A和B的零样本泛化。这些原语学习一种空间分解的形式，这使它们可以在目标任务A和B中处于活动状态。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在各种复杂环境中运行的强化学习智能体可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互。对原语进行正则化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
      <category term="Skill Learning" scheme="https://yachenkang.github.io/blog/tags/Skill-Learning/"/>
    
  </entry>
  
  <entry>
    <title>NAS Reading List</title>
    <link href="https://yachenkang.github.io/blog/2019/12/05/NAS-Reading-List/"/>
    <id>https://yachenkang.github.io/blog/2019/12/05/NAS-Reading-List/</id>
    <published>2019-12-05T07:31:04.000Z</published>
    <updated>2020-03-10T06:17:49.377Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h2><ul><li>[x] （ICLR2017, google brain）Neural architecture search with reinforcement learning</li><li>[ ] （2019.11）Meta-Learning of Neural Architectures for Few-Shot Learning，meta与NAS的结合：<a href="https://arxiv.org/abs/1911.11090v1" target="_blank" rel="noopener">https://arxiv.org/abs/1911.11090v1</a></li><li><p>[x] （2019.01）Designing neural networks through neuroevolution，NE方法综述</p></li><li><p>（2017.09）Evolution Strategies as a Scalable Alternative to Reinforcement Learning <a href="https://openai.com/blog/evolution-strategies/" target="_blank" rel="noopener">https://openai.com/blog/evolution-strategies/</a>， <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="noopener">https://arxiv.org/abs/1703.03864</a>：NES方法与DQN、A3C相匹敌（但未完全脱离梯度）</p></li><li><p>（2018.04）Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning <a href="https://arxiv.org/abs/1712.06567" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06567</a>：gradient-free的NE方法与DQN、A3C相匹敌</p></li><li><p>（2018.04）Simple random search provides a competitive approach to reinforcement learning <a href="https://arxiv.org/abs/1803.07055" target="_blank" rel="noopener">https://arxiv.org/abs/1803.07055</a>：简化NE方法（RS方法）与RPO、PPO、DDPG相匹敌</p></li></ul><h3 id="结合基于梯度的方法和神经进化"><a href="#结合基于梯度的方法和神经进化" class="headerlink" title="结合基于梯度的方法和神经进化"></a>结合基于梯度的方法和神经进化</h3><ul><li><p>（2018.05）Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients <a href="https://arxiv.org/abs/1712.06563" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06563</a>：保存状态与动作之间的关系库</p></li><li><p>（ICLR 2018.05）Policy Optimization by Genetic Distillation <a href="https://arxiv.org/abs/1711.01012" target="_blank" rel="noopener">https://arxiv.org/abs/1711.01012</a>：Genetic policy optimization</p></li><li><p>（ICLR 2018）Noisy Networks for Exploration <a href="https://arxiv.org/abs/1706.10295" target="_blank" rel="noopener">https://arxiv.org/abs/1706.10295</a></p></li><li><p>（ICLR 2018）Parameter space noise for exploration <a href="https://arxiv.org/abs/1706.01905" target="_blank" rel="noopener">https://arxiv.org/abs/1706.01905</a></p></li></ul><h3 id="新一代进化算法"><a href="#新一代进化算法" class="headerlink" title="新一代进化算法"></a>新一代进化算法</h3><ul><li><p>The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities <a href="https://arxiv.org/abs/1803.03453" target="_blank" rel="noopener">https://arxiv.org/abs/1803.03453</a> ：综述</p></li><li><p>（NIPS 2018）Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents <a href="https://arxiv.org/abs/1712.06560" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06560</a></p></li><li><p>（NIPS workshop 2018）Deep Curiosity Search: Intra-Life Exploration Can Improve Performance on Challenging Deep Reinforcement Learning Problems <a href="https://arxiv.org/abs/1806.00553" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00553</a></p></li></ul><h3 id="架构进化"><a href="#架构进化" class="headerlink" title="架构进化"></a>架构进化</h3><ul><li><p>From Nodes to Networks: Evolving Recurrent Neural Networks <a href="https://arxiv.org/abs/1803.04439" target="_blank" rel="noopener">https://arxiv.org/abs/1803.04439</a></p></li><li><p>（AAAI 2019）Regularized Evolution for Image Classifier Architecture Search <a href="https://arxiv.org/abs/1802.01548" target="_blank" rel="noopener">https://arxiv.org/abs/1802.01548</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;NAS&quot;&gt;&lt;a href=&quot;#NAS&quot; class=&quot;headerlink&quot; title=&quot;NAS&quot;&gt;&lt;/a&gt;NAS&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;[x] （ICLR2017, google brain）Neural architecture search with 
      
    
    </summary>
    
    
      <category term="Reading List" scheme="https://yachenkang.github.io/blog/categories/Reading-List/"/>
    
    
      <category term="NAS" scheme="https://yachenkang.github.io/blog/tags/NAS/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architecture Search with Reinforcement Learning</title>
    <link href="https://yachenkang.github.io/blog/2019/12/03/Neural-Architecture-Search-with-Reinforcement-Learning/"/>
    <id>https://yachenkang.github.io/blog/2019/12/03/Neural-Architecture-Search-with-Reinforcement-Learning/</id>
    <published>2019-12-03T02:26:34.000Z</published>
    <updated>2020-03-22T14:01:46.067Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>神经网络是一种功能强大、灵活的模型，在图像、语音和自然语言理解等许多困难的学习任务中起着很好的作用。尽管取得了成功，神经网络仍然很难设计。在本文中，我们使用递归网络来生成神经网络的模型描述，并利用强化学习来训练该RNN，最大化所生成的架构在验证集上的期望精度。在CIFAR-10数据集上，我们的方法可以从头开始，设计一种新的网络体系结构，在测试集精度方面可以与人类发明的最佳体系结构相媲美。我们的CIFAR-10模型的测试错误率为3.65，比以前使用类似架构方案的最新模型高0.09%，快1.05倍。在Penn Treebank数据集上，我们的模型可以组成一个新的递归单元，其性能优于广泛使用的LSTM单元和其他SOTA baseline。我们的单元在Penn Treebank上达到了测试集62.4的困惑度，这比之前的SOTA模型在困惑度上好3.6。该单元还可以转移到PTB上的字符语言建模任务，并达到SOTA的1.214困惑度。</p><a id="more"></a><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>在这一节中，我们将首先描述一种使用递归网络生成卷积结构的简单方法。我们将展示如何使用策略梯度方法来训练递归网络，以最大化采样架构的精度的期望。我们将提出几个基于我们核心方法的改进，如形成skip连接，以增加模型的复杂性，以及使用参数服务器方法来加快训练。这一节的最后，我们将着重于生成递归架构，这是本文的另一个重要贡献。</p><h4 id="Generate-Model-Descriptions-with-a-Controller-Recurrent-Neural-Network"><a href="#Generate-Model-Descriptions-with-a-Controller-Recurrent-Neural-Network" class="headerlink" title="Generate Model Descriptions with a Controller Recurrent Neural Network"></a>Generate Model Descriptions with a Controller Recurrent Neural Network</h4><p>在神经网络架构搜索（NAS）中，我们使用控制器生成神经网络的结构超参数。为了灵活，控制器被实现为RNN。假设我们想要预测只有卷积层的前馈神经网络，我们可以使用控制器将它们的超参数作为一系列标记生成：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/2019-12-03_Zoph%2C_Le_-_2016_-_Neural_Architecture_Search_with__17.png" alt="图2：我们的控制器递归神经网络如何采样一个简单的卷积网络。它预测filter高度、filter宽度、stride高度、stride宽度、一层的filters数量并重复。每个预测都由softmax分类器进行，然后作为输入输入到下一个时间步。" title>                </div>                <div class="image-caption">图2：我们的控制器递归神经网络如何采样一个简单的卷积网络。它预测filter高度、filter宽度、stride高度、stride宽度、一层的filters数量并重复。每个预测都由softmax分类器进行，然后作为输入输入到下一个时间步。</div>            </figure><p>在我们的实验中，如果层的数量超过某个值，生成架构的过程就会停止。这个值遵循一个时间表，我们在训练过程中增加它。一旦控制器RNN生成了一个架构，就用这个架构建立并训练一个神经网络。收敛时，将记录持有的验证集上网络的准确性。控制器RNN的参数，$\theta_c$，接着被优化以最大化提出的架构在验证集上预测精度的期望。在下一节中，我们将描述一种策略梯度方法，用于更新参数$\theta_c$，以便控制器RNN随着时间的推移生成更好的架构。</p><h4 id="Training-with-REINFORCE"><a href="#Training-with-REINFORCE" class="headerlink" title="Training with REINFORCE"></a>Training with REINFORCE</h4><p>控制器预测的标记列表可以看作是设计子网络架构的action的列表$a_{1:T}$。在收敛时，该子网络将在持有的数据集上达到精度$R$。我们可以用这个精度$R$作为奖励信号，用强化学习来训练控制器。更具体地说，为了找到最佳的架构，我们要求我们的控制器最大化其预期报酬，以$J(\theta_c)$表示：</p><script type="math/tex; mode=display">J\left(\theta_{c}\right)=E_{P\left(a_{1: T} ; \theta_{c}\right)}[R]</script><p>由于奖励信号$R$是不可微的，我们需要使用策略梯度方法迭代更新$\theta_c$。在这项工作中，我们使用<a href="https://link.springer.com/article/10.1007/BF00992696" target="_blank" rel="noopener">Williams（1992）</a>的REINFORCE规则：</p><script type="math/tex; mode=display">\nabla \theta_{c} J\left(\theta_{c}\right)=\sum_{t=1}^{T} E_{P\left(a_{1: T} ; \theta_{c}\right)}\left[\nabla \theta_{c} \log P\left(a_{t} | a_{(t-1): 1} ; \theta_{c}\right) R\right]</script><p><strong>Accelerate Training with Parallelism and Asynchronous Updates:</strong></p><h4 id="Increase-Architecture-Complexity-with-Skip-Connections-and-Other-Layer-Types"><a href="#Increase-Architecture-Complexity-with-Skip-Connections-and-Other-Layer-Types" class="headerlink" title="Increase Architecture Complexity with Skip Connections and Other Layer Types"></a>Increase Architecture Complexity with Skip Connections and Other Layer Types</h4><h4 id="Generate-Recurrent-Cell-Architectures"><a href="#Generate-Recurrent-Cell-Architectures" class="headerlink" title="Generate Recurrent Cell Architectures"></a>Generate Recurrent Cell Architectures</h4>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;神经网络是一种功能强大、灵活的模型，在图像、语音和自然语言理解等许多困难的学习任务中起着很好的作用。尽管取得了成功，神经网络仍然很难设计。在本文中，我们使用递归网络来生成神经网络的模型描述，并利用强化学习来训练该RNN，最大化所生成的架构在验证集上的期望精度。在CIFAR-10数据集上，我们的方法可以从头开始，设计一种新的网络体系结构，在测试集精度方面可以与人类发明的最佳体系结构相媲美。我们的CIFAR-10模型的测试错误率为3.65，比以前使用类似架构方案的最新模型高0.09%，快1.05倍。在Penn Treebank数据集上，我们的模型可以组成一个新的递归单元，其性能优于广泛使用的LSTM单元和其他SOTA baseline。我们的单元在Penn Treebank上达到了测试集62.4的困惑度，这比之前的SOTA模型在困惑度上好3.6。该单元还可以转移到PTB上的字符语言建模任务，并达到SOTA的1.214困惑度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://yachenkang.github.io/blog/tags/Reinforcement-Learning/"/>
    
      <category term="NAS" scheme="https://yachenkang.github.io/blog/tags/NAS/"/>
    
  </entry>
  
  <entry>
    <title>Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</title>
    <link href="https://yachenkang.github.io/blog/2019/11/01/Meta-World-A-Benchmark-and-Evaluation-for-Multi-Task-and-Meta-Reinforcement-Learning/"/>
    <id>https://yachenkang.github.io/blog/2019/11/01/Meta-World-A-Benchmark-and-Evaluation-for-Multi-Task-and-Meta-Reinforcement-Learning/</id>
    <published>2019-11-01T07:24:27.000Z</published>
    <updated>2020-03-19T08:20:28.664Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>元强化学习算法可以通过利用先前的经验来学习如何学习，从而使机器人更快地掌握新技能。但是，当前有关元强化学习的许多研究都集中在非常狭窄的任务分布上。例如，一个常用的元强化学习基准将模拟机器人的不同的运行速度作为不同的任务。当在这样狭窄的任务分布上进行策略的元训练时，它们可能无法泛化到更快地获取全新的任务。因此，如果这些方法的目的是能够更快地获取全新的行为，则我们必须在足够广泛的任务分布上评估它们，以使其能够推广到新的行为。在本文中，我们提出了一种用于元强化学习和多任务学习的开源模拟benchmark，该benchmark包含50个不同的机器人操纵任务。我们的目标是使开发用于加速获取全新的、可执行的任务的算法成为可能。我们针对这些任务评估了6种最新的元强化学习和多任务学习算法。令人惊讶的是，尽管每项任务及其变体（例如，不同的对象位置）都可以合理地成功学习，但是这些算法难以同时学习多个任务，即使只有十个不同的训练任务也是如此。我们的分析和开源环境为将来的多任务学习和元学习研究铺平了道路，这些研究可以实现有意义的泛化，从而释放这些方法的全部潜力。</p><p>benchmark任务的视频在项目页面上：<a href="https://meta-world.github.io" target="_blank" rel="noopener">meta-world.github.io</a>。我们的开源代码可在以下网址获得：<a href="https://github.com/rlworkgroup/metaworld" target="_blank" rel="noopener">https://github.com/rlworkgroup/metaworld</a></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;元强化学习算法可以通过利用先前的经验来学习如何学习，从而使机器人更快地掌握新技能。但是，当前有关元强化学习的许多研究都集中在非常狭窄的任务分布上。例如，一个常用的元强化学习基准将模拟机器人的不同的运行速度作为不同的任务。当在这样狭窄的任务分布上进行策略的元训练时，它们可能无法泛化到更快地获取全新的任务。因此，如果这些方法的目的是能够更快地获取全新的行为，则我们必须在足够广泛的任务分布上评估它们，以使其能够推广到新的行为。在本文中，我们提出了一种用于元强化学习和多任务学习的开源模拟benchmark，该benchmark包含50个不同的机器人操纵任务。我们的目标是使开发用于加速获取全新的、可执行的任务的算法成为可能。我们针对这些任务评估了6种最新的元强化学习和多任务学习算法。令人惊讶的是，尽管每项任务及其变体（例如，不同的对象位置）都可以合理地成功学习，但是这些算法难以同时学习多个任务，即使只有十个不同的训练任务也是如此。我们的分析和开源环境为将来的多任务学习和元学习研究铺平了道路，这些研究可以实现有意义的泛化，从而释放这些方法的全部潜力。&lt;/p&gt;
&lt;p&gt;benchmark任务的视频在项目页面上：&lt;a href=&quot;https://meta-world.github.io&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;meta-world.github.io&lt;/a&gt;。我们的开源代码可在以下网址获得：&lt;a href=&quot;https://github.com/rlworkgroup/metaworld&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/rlworkgroup/metaworld&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="meta-learning" scheme="https://yachenkang.github.io/blog/tags/meta-learning/"/>
    
      <category term="benchmark" scheme="https://yachenkang.github.io/blog/tags/benchmark/"/>
    
  </entry>
  
  <entry>
    <title>iMAML笔记（翻译）[更新中]</title>
    <link href="https://yachenkang.github.io/blog/2019/10/18/iMAML%E7%AC%94%E8%AE%B0%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89/"/>
    <id>https://yachenkang.github.io/blog/2019/10/18/iMAML笔记（翻译）/</id>
    <published>2019-10-18T07:45:27.000Z</published>
    <updated>2020-03-22T14:01:46.067Z</updated>
    
    <content type="html"><![CDATA[<p>转载并翻译iMAML的阅读笔记</p><a id="more"></a><p><strong>作者</strong>： <a href="https://www.inference.vc/" target="_blank" rel="noopener">inFERENCe</a></p><p>本周，我阅读了这份关于元学习的新文章：基于一些关于正则化优化最优结果的微分的观察，与前一版本相比，它的方法略有不同。</p><ul><li>Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine (2019) <a href="https://arxiv.org/abs/1909.04630" target="_blank" rel="noopener">Meta-Learning with Implicit Gradients</a></li></ul><p>同时发表的另一篇论文也发现了类似的技术，所以我认为我会更新该帖子并提及它，尽管我不会详细介绍它，并且该帖子主要是关于 Rajeswaran et al (2019) 的</p><ul><li>Yutian Chen, Abram L. Friesen, Feryal Behbahani, David Budden, Matthew W. Hoffman, Arnaud Doucet, Nando de Freitas (2019) <a href="https://arxiv.org/abs/1909.05557" target="_blank" rel="noopener">Modular Meta-Learning with Shrinkage</a></li></ul><h4 id="大纲："><a href="#大纲：" class="headerlink" title="大纲："></a>大纲：</h4><ul><li>我将对元学习设置进行高层概述，我们的目标是学习一种良好的SGD初始化或正则化策略，从而使SGD收敛到更好地完成一系列任务的最小值。</li><li>我将说明iMAML如何在1D玩具示例中工作，并讨论元目标的行为和属性。</li><li>然后，我将讨论iMAML的局限性：它仅考虑最小值的位置，而不考虑随机算法最终达到特定最小值的可能性。</li><li>最后，我将把iMAML与元学习的一种变体方法联系起来。</li></ul><h3 id="元学习与MAML"><a href="#元学习与MAML" class="headerlink" title="元学习与MAML"></a>元学习与MAML</h3><p>元学习有几种可能的表述方式，我将尝试按照我自己的解释和表示来解释这篇文章的设置，这与这篇文章有所不同，但使我的解释更加清楚（希望会）。</p><p>在元学习中，我们有一系列独立的任务，分别具有关联的训练和验证损失函数$f_i$和$g_i$。我们有一组在任务之间共享的模型参数$\theta$，损失函数$f_i(\theta)$和$g_i(\theta)$评估具有参数$\theta$的模型在任务$i$的训练和测试案例中的表现如何。我们有一个算法可以访问训练损失$f_i$和一些元参数$\theta_0$，并输出一些最优或学习的参数$\theta^\ast_i = Alg(f_i,\theta_0)$。元学习算法的目标是关于元参数$\theta_0$优化元目标</p><script type="math/tex; mode=display">\mathcal{M}(\theta_0)=\sum_i g_i(Alg(f_{i}, \theta_{0}))</script><p>在这项工作的早期版本MAML中，该算法被选择为随机梯度下降算法，$f_i$和$g_i$是神经网络的训练和测试损失。元参数$\theta_0$是SGD算法的初始化点，在所有任务之间共享。由于SGD更新是可微的，因此可以通过简单地穿过SGD更新步反向传播来计算相对于初始值$\theta_0$的元目标的梯度。这基本上就是MAML所做的。</p><p>但是，初始化对$\theta$最终值的影响非常微弱，并且很难进行分析表征（如果有可能的话）。如果我们允许SGD继续执行许多更新步，则可能会收敛到一个更好的参数，但是轨迹将非常长，并且相对于初始值的梯度将消失。如果我们使轨迹足够短，则关于$\theta_{0}$的梯度是有用的，但我们可能无法达到很好的最终值。</p><h3 id="iMAML"><a href="#iMAML" class="headerlink" title="iMAML"></a>iMAML</h3><p>这就是为什么Rajeswaran等人选择使轨迹的终点对元参数$\theta_0$的依赖性更强的原因：除了简单地从$\theta_0$初始化SGD之外，他们还通过在loss中添加二次正则项$|\theta−\theta_0|$来固定参数以使其停留在$\theta_0$附近。因此，发生了两件事：</p><ul><li>现在，SGD的所有步骤都取决于$\theta$，而不仅仅是起始点</li><li>现在最小化SGD最终收敛到的位置也取决于$\theta_0$</li></ul><p>iMAML正是利用了这第二个属性。让我说明一下这种依赖性是什么样的：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/20191018165008.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>在上图中，假设我们要最小化目标函数$f(\theta)$。这将是元学习算法必须解决的任务之一的训练损失。我们当前的元参数$\theta_0$标记在x轴上，橙色曲线显示了相关的二次惩罚。蓝绿色曲线显示了加上惩罚项的目标。红星表示最小值的位置，这是学习算法发现的位置。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/Notes%20on%20iMAML_%20Meta-Learning%20with%20Implicit%20Gradients.gif" alt title>                </div>                <div class="image-caption"></div>            </figure><p>现在，让此动画动起来。我将移动锚点$\theta_0$，并重现相同的图。您会看到，随着我们移动$\theta_0$和相应的惩罚项，正则化目标移动的局部（同时也是全局）最小值发生了变化：</p><p>因此很明显，锚点$\theta_0$与局部最小值$\theta^\ast$的位置之间存在非平凡非线性的关系。让我们根据锚点绘制此关系：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/20191018165057.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>我们可以看到该函数一点也不好处理，当最接近$\theta_0$的局部最小值发生变化时，它具有急剧的跳变，并且在这些跳变之间相对平坦。实际上，你可以观察到最接近$\theta_0$的局部最小值越锐利，则$\theta_0$和$\theta$之间的关系越平坦。这是因为，如果$f$在$\theta_0$附近具有尖锐的局部最小值，则正则化最小值的位置将主要由$f$确定，并且锚点$\theta_0$的位置无关紧要。如果围绕$f$的局部最小值很宽，则最优值会有很大的摆动空间，并且正则化的效果会更大。</p><h3 id="Implicit-Gradients"><a href="#Implicit-Gradients" class="headerlink" title="Implicit Gradients"></a>Implicit Gradients</h3><p>现在，我们讨论iMAML程序的全部内容。实际上，该函数$\theta^\ast(\theta_0)$的梯度可以以封闭形式计算。实际上，它与$f$的曲率或二阶导数有关，在我们找到的最小值附近：</p><script type="math/tex; mode=display">\frac{d \theta^{\ast}}{d \theta_{0}}=\frac{1}{1+f^{\prime \prime}\left(\theta^{\ast}\right)}</script><p>为了检查此公式是否有效，我对导数进行了数值计算，并将其与理论预测的结果进行了比较，它们完全匹配：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/20191018165818.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>当参数空间是高维时，我们有一个类似的公式，其中包含Hessian的逆加单位阵。在高维中，Hessian的求逆甚至计算和存储都不太实际。iMAML论文的主要贡献之一是使用共轭梯度内部优化循环来逼近梯度的实用方法。有关详细信息，请阅读论文。</p><h4 id="Optimizing-the-meta-objective"><a href="#Optimizing-the-meta-objective" class="headerlink" title="Optimizing the meta-objective"></a>Optimizing the meta-objective</h4><p>在元学习设置中优化锚点时，我们感兴趣的不是位置$\theta^\ast$，而是函数$f$在此位置处取的值。（实际上，我们现在将使用验证损失来代替用于梯度下降的训练损失，但为简单起见，我假设这两个损失是重叠的）。$f$在其局部最优处的值绘制如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/picrepo/master/img/20191018165958.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>噢，这个函数不是很漂亮。元目标$f(\theta^\ast(\theta_0))$变成分段连续函数，是相邻盆地之间的连接，边界不光滑。该函数的局部梯度包含很少的有关损失函数的整体结构的信息，它仅告诉你如何到达最接近的局部最小值的位置。所以我不会说这是最好的优化函数。</p><p>值得庆幸的是，该函数不是我们必须优化的。在元学习中，我们有优化的函数上的分布，因此实际的元目标类似于$\sum_i f_i(\theta^\ast_i(\theta_0))$。一堆丑陋的函数的总和很可能会变成平滑而优美的东西。另外，我在此博客文章中使用的一维函数不能代表我们要应用iMAML的神经网络的高维损失函数。以模式连通性的概念为例（参见例如<a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://arxiv.org/pdf/1802.10026.pdf" target="_blank" rel="noopener">Garipov et al, 2018</a>）：似乎SGD使用不同的随机种子发现的模式不仅仅只是孤立的盆地，而是通过训练和测试误差低的光滑的山谷相连。反过来，这可能会使元目标在最小值之间表现得更加平稳。</p><h3 id="What-is-missing-Stochasticity"><a href="#What-is-missing-Stochasticity" class="headerlink" title="What is missing? Stochasticity"></a>What is missing? Stochasticity</h3><p>MAML或iMAML不考虑的重要方面是我们通常使用随机优化算法的事实。SGD不会确定性地找到特定的最小值，而是采样不同的最小值：当使用不同的随机种子运行时，它将发现不同的最小值。</p><p>对元目标的更为慷慨的表述将允许使用随机算法。如果我们用$Alg(f_i,\theta_0)$表示算法发现的解的分布，则元目标将是</p><script type="math/tex; mode=display">\mathcal{M}_{s t o c h a s t i c}(\theta)=\sum_{i} \mathbb{E}_{\theta \sim \mathcal{Alg}\left(f_{i}, \theta_{0}\right)} g_{i}(\theta)</script><p>允许随机行为实际上可能对元学习而言是个很好的特性。正则化目标的全局最小值的位置会随$\theta_{0}$突然变化（如上面第三图所示），允许随机行为可能会使我们的元学习目标变得平滑。</p><p>现在假设锚定到$\theta_{0}$的SGD收敛到局部极小值的有限集合之一。那么元学习目标以两种不同方式受到$\theta_{0}$的影响：</p><ul><li>当我们更改锚点$\theta_{0}$时，最小值的位置也会发生变化，如上所述。这种变化是可微的，我们知道其导数。</li><li>当我们改变锚点$\theta_{0}$时，找到不同解的概率就会改变。有些解的发现频率更高，而有些则更少。</li></ul><p>iMAML考虑第一种影响，但它忽略了第二种机制的影响。这并不是说iMAML是错误的，而是它忽略了MAML或通过算法显式微分没有忽略的随机行为可能做出的关键贡献。</p><h3 id="Compare-with-a-Variational-Approach"><a href="#Compare-with-a-Variational-Approach" class="headerlink" title="Compare with a Variational Approach"></a>Compare with a Variational Approach</h3><p>当然，这项工作使我想起了贝叶斯方法。每当有人描述二次惩罚时，我所看到的就是高斯分布。</p><p>在iMAML的贝叶斯解释中，可以将锚点$\theta_{0}$视为神经网络权重上先验分布的均值。然后，在给定相关数据集的情况下，算法的内部循环或$Alg(f_i,\theta_{0})$会找到$\theta$上后验的最大后验（MAP）近似值。假设损失是某种形式的对数似然。问题是，如何更新元参数$\theta_{0}$？</p><p>在贝叶斯世界中，我们将寻求通过最大化边缘似然来优化$\theta_{0}$。由于这通常很棘手，因此通常需要变分近似，在这种情况下，它看起来像这样：</p><script type="math/tex; mode=display">\mathcal{M}_{\text {variational }}\left(\theta_{0}, Q_{i}\right)=\sum_{i}\left(K L\left[Q_{i} | \mathcal{N}_{\theta_{0}}\right]+\mathbb{E}_{\theta \sim Q_{i}} f_{i}(\theta)\right)</script><p>其中$Q_i$逼近任务$i$的模型参数的后验。$Q_i$的特定选择是狄拉克三角洲分布，其中心位于特定点$ Q_i(\theta)= \delta(\theta-\theta^{\ast}_i)$。如果我们慷慨地忽略某些常数会无限大地爆炸，那么高斯先验和简并点后验之间的KL散度就是一个简单的欧几里得距离，而我们的变分目标可简化为：</p><script type="math/tex; mode=display">\mathcal{M}_{\mathrm{variational}}\left(\theta_{0}, \theta_{i}\right)=\sum_{i}\left(\left\|\theta_{i}-\theta_{0}\right\|^{2}+f_{i}\left(\theta_{i}\right)\right)</script><p>现在，该目标函数非常类似于iMAML内循环试图解决的优化问题。如果我们在纯变分框架中工作，则可能是我们留下的东西，我们可以共同优化所有$\theta_i$和$\theta_{0}$。知道的人，请在下面发表评论，为我提供进行元学习的最佳参考。</p><p>使用无内环优化或黑魔法，该目标明显易于优化。它只是简单地将$\theta_{0}$拉到更接近为每个任务$i$找到的各种最优值的重心。不确定对于元学习来说这是否是一个好主意，因为通过从头开始从$\theta_{0}$进行SGD，我们可能无法通过共同优化所有目标而获得的$\theta_i$最终值。但是谁知道。鉴于上述观察，一个好主意可能是使$\theta_{0}$和$\theta_i$的变化目标共同最小化，但不时地将$\theta_{i}$重新初始化为$\theta_{0}$。但是在这一点上，我真的只是在编造东西…</p><p>无论如何，回到iMAML，它对这个变化目标做了一些有趣的事情，我认为可以将其理解为一种摊销计算：不是将$\theta_i$视为单独的辅助参数，而是指定了$\theta_i$实际上是$\theta_{0}$的确定性函数。。由于变分目标是任何$\theta_i$值的有效上限，因此，如果我们明确地使$\theta_i$取决于$\theta_{0}$，它也是有效的上限。因此，变分目标仅成为$\theta_{0}$的函数（以及算法$Alg$的超参数（如果有的话））：</p><script type="math/tex; mode=display">\mathcal{M}_{\text {variational }}\left(\theta_{0}\right)=\sum_{i}\left(\left\|Alg \left(f_{i}, \theta_{0}\right)-\theta_{0}\right\|^{2}+f_{i}\left(Alg\left(f_{i}, \theta_{0}\right)\right)\right)</script><p>我们终于得到它了。元学习$\theta_{0}$的变分目标与MAML / iMAML元目标非常相似，不同之处在于它还有$|Alg(f_i,\theta_{0})-\theta_{0}|^2$项，这是我们以前没有更新过的$\theta_{0}$的因素。还要注意，我没有使用单独的训练和验证损失$f_i$和$g_i$，但这也是一个非常合理的选择。</p><p>这样做的妙处在于，它为iMAML正在尝试做的事情提供了额外的理由和解释，并提出了可能改进iMAML的方向。另一方面，iMAML中的隐式区分技巧可能在其他情况下同样有用，即我们希望摊销后验后验。</p><p>我敢肯定我错过了很多参考资料，如果您认为我应该添加任何内容，特别是在变体位上，请在下面评论。</p><p><strong>原文链接</strong>：<a href="https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/" target="_blank" rel="noopener">Notes on iMAML: Meta-Learning with Implicit Gradients</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转载并翻译iMAML的阅读笔记&lt;/p&gt;
    
    </summary>
    
    
      <category term="blog翻译" scheme="https://yachenkang.github.io/blog/categories/blog%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="meta-learning" scheme="https://yachenkang.github.io/blog/tags/meta-learning/"/>
    
      <category term="MAML" scheme="https://yachenkang.github.io/blog/tags/MAML/"/>
    
  </entry>
  
  <entry>
    <title>On First-Order Meta-Learning Algorithms</title>
    <link href="https://yachenkang.github.io/blog/2019/10/17/On-First-Order-Meta-Learning-Algorithms/"/>
    <id>https://yachenkang.github.io/blog/2019/10/17/On-First-Order-Meta-Learning-Algorithms/</id>
    <published>2019-10-17T02:58:41.000Z</published>
    <updated>2020-03-19T08:20:28.664Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>本文考虑了存在任务分布的元学习问题，并且我们希望获得一个从该分布中采样到以前没有见过的任务时表现良好（即快速学习）的agent。我们分析了一族用于学习参数初始化的算法，可以在新任务上进行快速微调，仅使用一阶导数进行元学习更新。该族包括并推广了一阶MAML，它是通过忽略二阶导数获得的MAML的近似值。它还包括Reptile，这是我们在此处引入的新算法，该算法通过重复采样任务，对其进行训练并将初始化朝着该任务的训练权重进行工作。我们扩展了Finn等人的结果。说明一阶元学习算法在一些公认的针对少数镜头分类的基准上表现良好，并且我们提供了旨在理解这些算法为何起作用的理论分析。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;本文考虑了存在任务分布的元学习问题，并且我们希望获得一个从该分布中采样到以前没有见过的任务时表现良好（
      
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="meta-learning" scheme="https://yachenkang.github.io/blog/tags/meta-learning/"/>
    
      <category term="MAML" scheme="https://yachenkang.github.io/blog/tags/MAML/"/>
    
  </entry>
  
  <entry>
    <title>Meta-Learning: A Survey[更新中]</title>
    <link href="https://yachenkang.github.io/blog/2019/10/09/Meta-Learning-A-Survey/"/>
    <id>https://yachenkang.github.io/blog/2019/10/09/Meta-Learning-A-Survey/</id>
    <published>2019-10-09T12:37:57.000Z</published>
    <updated>2020-03-19T08:20:28.664Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>元学习或学会学习是系统地观察不同机器学习方法如何在广泛的学习任务中执行的科学，然后从这种经验或元数据中学习，以比其他方式更快地学习新任务 。<br>这不仅极大地加速和改进了机器学习流程或神经网络架构的设计，还使我们能够用数据驱动方式学习的新方法取代手工设计算法。<br>在本章中，我们将概述这个迷人且不断发展的领域的最新技术。</p><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><hr><p>当我们学习新技能时，我们很少 - 如果有的话 - 从头开始。我们从之前在相关任务中学到的技能开始，重用以前运作良好的方法，并根据经验关注可能值得尝试的内容（Lake et al。，2017）。通过学到的所有技能，学习新技能变得更容易，需要更少的示例和更少的试错。简而言之，我们跨任务学习如何学习。同样，在为特定任务构建机器学习模型时，我们通常会建立相关任务的经验，或者使用我们（通常是隐含的）对机器学习技术行为的理解来帮助做出正确的选择。</p><p>元学习的挑战是以系统的，数据驱动的方式从先前的经验中学习。首先，我们需要收集描述先前学习任务和先前学习模型的<strong>元数据</strong>。它们包括用于训练模型的精确<strong>算法配置</strong>，包括超参数设置，流程组合和/或网络架构，所得到的<strong>模型评估</strong>，例如准确性和训练时间，学到的模型参数，例如训练到的神经网络的权重，以及任务本身的可测量的适当关系，也称为<strong>元特征</strong>。接着，我们需要从这个先前的元数据开始<strong>学习</strong>，以提取和传递知识用于指导搜索新任务的最佳模型。本章简要概述了有效实现这一目标的不同元学习方法。</p><p><strong>元学习</strong>一词涵盖基于其他任务的先前经验的任何类型的学习。这些先前的任务越<strong>相似</strong>，我们可以利用的元数据类型就越多，并且定义任务相似性将是一个关键的总体挑战。不用多说，天下没有免费的午餐（Wolpert和Macready，1996； Giraud-Carrier和Provost，2005）。当一项新任务代表完全不相关的现象或随机噪音时，利用先前的经验将无效。幸运的是，在现实世界中的任务中，有很多机会可以学习以前的经验。</p><p>在本章的其余部分，我们根据元数据学习所利用的元数据类型对元学习技术进行分类，从最通用的到最特定于任务的。首先，在第2节中，我们讨论如何<strong>纯粹从模型评估中学习</strong>。这些技术可用于推荐通常有用的配置和配置搜索空间，以及从<strong>经验相似的</strong>任务中转移知识。在第3节中，我们讨论如何<strong>表征</strong>任务以更明确地表达任务相似性并建立元模型来学习数据特征与学习性能之间的关系。最后，第4节介绍了如何在固有相似的任务之间<strong>传递训练后的模型参数</strong>，例如共享相同的输入功能，从而可以进行迁移学习（Pan和Yang，2010）和少样本学习（Ravi和Larochelle，2017）。</p><p>请注意，尽管<strong>多任务学习</strong>（Caruana，1997）（同时学习多个相关任务）和<strong>集成学习</strong>（Dietterich，2000）（在同一任务上构建多个模型）通常可以与元学习系统有意义地结合，本身并不涉及在其他任务上的先前经验中学习。</p><h3 id="Learning-from-Model-Evaluations"><a href="#Learning-from-Model-Evaluations" class="headerlink" title="Learning from Model Evaluations"></a>Learning from Model Evaluations</h3><hr><p>考虑到我们可以访问先前任务$t_j \in T$，所有已知任务的集合以及一组学习算法，这些算法完全由其<strong>配置</strong>$\theta_i\in \Theta$定义； 在此，$\Theta$表示离散的，连续的或混合的配置空间，其可以包括超参数设置，流程组件和/或网络架构组件。$P$是任务$t_j$上所有配置$\theta_i$的所有先前标量评估$P_{i, j}=P(\theta_i,t_j)$的集合，根据预先定义的评估方法，例如 准确性和模型评估技术，例如 交叉验证。$P_{new}$是新任务$t_{new}$上一组已知评估$P_{i,new}$的集合。现在，我们想训练一个元学习器$L$，它预测针对新任务$t_{new}$的推荐配置$\Theta^\ast _{new}$。元学习器接受元数据$P\cup P_{new}$的训练。$P$通常是事先收集的，或者是从元数据存储库中提取的（Vanschoren等，2014，2012）。通过元学习技术本身以迭代方式学习$P_{new}$，有时以另一种方法生成的初始$P’_ {new}$进行<strong>热启动</strong>。</p><h4 id="Task-Independent-Recommendations"><a href="#Task-Independent-Recommendations" class="headerlink" title="Task-Independent Recommendations"></a>Task-Independent Recommendations</h4><p>首先，假设无法获得有关$t_{new}$的任何评估，因此$P_{new}=\emptyset$。然后，我们仍然可以学习函数$f: \Theta \times T \to \{\theta^\ast _ k\},k = 1..K$，产生了一组<strong>独立</strong>于$t_{new}$的推荐配置。然后可以重新评估这些$\theta ^ \ast _ k$，以选择最佳的$\theta  _ k$，或热启动进一步的优化方法，例如第2.3节中讨论的方法。</p><p>这种方法通常会产生排序，即<strong>有序</strong>集合$\theta ^ \ast _ k$。这通常是通过将$\theta$离散化为一组候选配置$\theta_ i$，该候选配置也称为<strong>portfolio</strong>，在大量任务$t_j$上进行评估来完成的。然后，我们可以根据<strong>成功率</strong>，<strong>AUC</strong>或<strong>significant wins</strong>来建立每个任务上的排名（Brazdil等，2003a； Demˇsar，2006； Leite等，2012）。但是，通常希望将同样好的但速度更快的算法排在更高的位置，并且已经提出了多种方法来权衡准确性和训练时间（Brazdil等人，2003a; van Rijn等人，2015）。接下来，我们可以将这些单任务排名汇总为<strong>全局排名</strong>，例如通过计算在所有任务上的平均排名（Lin，2010; Abdulrahman et al。，2018）。当没有足够的数据来建立全局排名时，可以根据每个先验任务的最佳已知配置来推荐<strong>配置子集</strong>（Todorovski和Dzeroski，1999; Kalousis，2002），或者返回<strong>准线性排名</strong>（Cook等。（1996）。</p><p>为了找到任务$t_{new}$的最佳$\theta^\ast$，这是从未见过的任务，一种随时随地的简单方法是选择前$K$个配置（Brazdil等人，2003a），从列表中查找并依次评估每个配置在$t_{new}$上的表现。在固定值$K$个，时间预算约束或找到足够准确的模型之后，可以停止此评估。在时间受限的环境中，已表明多目标排名（包括训练时间）更快地收敛到接近最优的模型（Abdulrahman等，2018； van Rijn等，2015），并提供了强有力的基线用于算法比较（Abdulrahman等，2018; Leite等，2012）。</p><p>与上述方法非常不同的方法是，首先对特定任务$t_j$的所有先前评估拟合微分函数$f_j(\theta_i)= P_{i,j}$，然后使用梯度下降找到每个先前任务的优化配置$\theta^\ast _ j$（Wistuba等人，2015a）。假设某些任务$t_j$与$t_{new}$相似，则这些$\theta ^ \ast _ j$对于热启动贝叶斯优化方法很有用。</p><h4 id="Confifiguration-Space-Design"><a href="#Confifiguration-Space-Design" class="headerlink" title="Confifiguration Space Design"></a>Confifiguration Space Design</h4><h4 id="Confifiguration-Transfer"><a href="#Confifiguration-Transfer" class="headerlink" title="Confifiguration Transfer"></a>Confifiguration Transfer</h4><h5 id="Relative-Landmarks"><a href="#Relative-Landmarks" class="headerlink" title="Relative Landmarks"></a>Relative Landmarks</h5><h5 id="Surrogate-Models"><a href="#Surrogate-Models" class="headerlink" title="Surrogate Models"></a>Surrogate Models</h5><h5 id="Warm-Started-Multi-task-Learning"><a href="#Warm-Started-Multi-task-Learning" class="headerlink" title="Warm-Started Multi-task Learning"></a>Warm-Started Multi-task Learning</h5><h5 id="Other-Techniques"><a href="#Other-Techniques" class="headerlink" title="Other Techniques"></a>Other Techniques</h5><h4 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title="Learning Curves"></a>Learning Curves</h4><h3 id="Learning-from-Task-Properties"><a href="#Learning-from-Task-Properties" class="headerlink" title="Learning from Task Properties"></a>Learning from Task Properties</h3><hr><h4 id="Task-Independent-Recommendations-1"><a href="#Task-Independent-Recommendations-1" class="headerlink" title="Task-Independent Recommendations"></a>Task-Independent Recommendations</h4><h4 id="Confifiguration-Space-Design-1"><a href="#Confifiguration-Space-Design-1" class="headerlink" title="Confifiguration Space Design"></a>Confifiguration Space Design</h4><h4 id="Confifiguration-Transfer-1"><a href="#Confifiguration-Transfer-1" class="headerlink" title="Confifiguration Transfer"></a>Confifiguration Transfer</h4><h5 id="Relative-Landmarks-1"><a href="#Relative-Landmarks-1" class="headerlink" title="Relative Landmarks"></a>Relative Landmarks</h5><h5 id="Surrogate-Models-1"><a href="#Surrogate-Models-1" class="headerlink" title="Surrogate Models"></a>Surrogate Models</h5><h5 id="Warm-Started-Multi-task-Learning-1"><a href="#Warm-Started-Multi-task-Learning-1" class="headerlink" title="Warm-Started Multi-task Learning"></a>Warm-Started Multi-task Learning</h5><h5 id="Other-Techniques-1"><a href="#Other-Techniques-1" class="headerlink" title="Other Techniques"></a>Other Techniques</h5><h4 id="Learning-Curves-1"><a href="#Learning-Curves-1" class="headerlink" title="Learning Curves"></a>Learning Curves</h4><h3 id="Learning-from-Task-Properties-1"><a href="#Learning-from-Task-Properties-1" class="headerlink" title="Learning from Task Properties"></a>Learning from Task Properties</h3><hr><h4 id="Meta-Features"><a href="#Meta-Features" class="headerlink" title="Meta-Features"></a>Meta-Features</h4><h4 id="Learning-Meta-Features"><a href="#Learning-Meta-Features" class="headerlink" title="Learning Meta-Features"></a>Learning Meta-Features</h4><h4 id="Warm-Starting-Optimization-from-Similar-Tasks"><a href="#Warm-Starting-Optimization-from-Similar-Tasks" class="headerlink" title="Warm-Starting Optimization from Similar Tasks"></a>Warm-Starting Optimization from Similar Tasks</h4><h4 id="Meta-Models"><a href="#Meta-Models" class="headerlink" title="Meta-Models"></a>Meta-Models</h4><h5 id="Ranking"><a href="#Ranking" class="headerlink" title="Ranking"></a>Ranking</h5><h5 id="Performance-Prediction"><a href="#Performance-Prediction" class="headerlink" title="Performance Prediction"></a>Performance Prediction</h5><h4 id="Pipeline-Synthesis"><a href="#Pipeline-Synthesis" class="headerlink" title="Pipeline Synthesis"></a>Pipeline Synthesis</h4><h4 id="To-Tune-or-Not-to-Tune"><a href="#To-Tune-or-Not-to-Tune" class="headerlink" title="To Tune or Not to Tune?"></a>To Tune or Not to Tune?</h4><h3 id="Learning-from-Prior-Models"><a href="#Learning-from-Prior-Models" class="headerlink" title="Learning from Prior Models"></a>Learning from Prior Models</h3><hr><h4 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h4><h4 id="Meta-Learning-in-Neural-Networks"><a href="#Meta-Learning-in-Neural-Networks" class="headerlink" title="Meta-Learning in Neural Networks"></a>Meta-Learning in Neural Networks</h4><h4 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h4><p>一项特别具有挑战性的元学习问题是，鉴于有大量可用训练集的非常相似任务的先前经验，我们仅使用几个训练示例就可以训练出准确的深度学习模型。这称为少样本学习。人类对此具有与生俱来的能力，我们希望构建可以做到这一点的机器学习agent（Lake等人，2017）。一个具体的例子是“ K-shot N-way”分类，其中给了我们某些类别（例如物体）的许多例子（例如图像），并且想要学习一个能够仅使用$K$个示例对$N$个新类别进行分类的分类器$l_{new}$。</p><p>例如，利用先前的经验，我们可以学习所有任务的通用特征表示，通过更好的模型参数初始化$W_{init}$开始训练$l_{new}$，以及获得归纳偏差以帮助指导模型参数的优化，从而使$l_{new}$能比其他方法训练快得多。</p><p>单样本学习的早期工作主要基于手工设计的特征 (Fei-Fei et al., 2006; Fei-Fei, 2006; Fink, 2005; Bart and Ullman, 2005)。但是，通过元学习，我们希望以端到端的方式学习所有任务的通用特征表示。</p><p>Vinyals et al. (2016) 指出，要从很少的数据中学习，就应该关注非参数模型（例如k近邻），该模型使用记忆组件而不是学习许多模型参数。他们的元学习器是一个匹配网络，它应用了神经网络中记忆组件的概念。它为标记的样本学习通用表示，并使用余弦相似度将每个新的测试样本与存储的样本进行匹配。该网络在小批次上进行了训练，每个批次仅包含几个特定任务的样本。</p><p>Snell et al. (2017) 提出了原型网络，将样本映射到p维向量空间，以使给定输出类的样本彼此接近。然后，它计算每个类的原型（均值向量）。新的测试样本被映射到相同的向量空间，并且距离度量用于在所有可能的类上创建softmax。Ren et al. (2018) 将这种方法扩展到半监督学习。</p><p>Ravi and Larochelle (2017) 使用基于LSTM的元学习器来学习用于训练神经网络学习器的更新规则。对于每个新样本，学习器将当前的梯度和损失返回给LSTM元学习器，然后LSTM元学习器更新学习器的模型参数$\{w_k\}$。元学习器在所有先前任务上训练。</p><p>另一方面，模型无关的元学习（MAML） (Finn et al., 2017)不尝试学习更新规则，而是学习模型参数初始化$W_{init}$，该模型可以更好地推广到类似任务。从随机$\{w_k\}$开始，迭代选择一批先前的任务，并针对每个任务对学习器进行$K$个样本的训练，以计算梯度和损失（在测试集上）。然后，它会将元梯度反向传播，以沿权重$\{w_k\}$更容易更新的方向进行更新。换句话说，在每次迭代之后，权重$\{w_k\}$成为更好的$W_{init}$，可以开始对任何任务进行微调。Finn and Levine (2017) 表明，在使用足够深的ReLU网络和某些损失的情况下，MAML能够逼近任何学习算法。他们还得出结论，与基于LSTM的元学习方法相比，MAML初始化对于小样本的过拟合更具弹性，并且泛化得更广泛。 Grant et al. (2018) 提出了MAML的新派生和扩展，说明了该算法可以理解为推理贝叶斯模型中先验分布的参数。</p><p>REPTILE (Nichol et al., 2018) 是MAML的近似值，它对给定任务执行$K$次迭代的随机梯度下降，然后朝$K$次迭代后获得的权重方向逐渐移动初始化权重。Intuition在于每个任务可能具有一组以上的最佳权重$\{w_i^\ast\}$，目标是找到一个与每个任务至少接近$\{w_i^\ast\}$的$W_{init}$。</p><p>最后，我们还可以从黑盒神经网络派生一个元学习器。 Santoro et al. (2016a)提出了记忆增强神经网络（MANNs），它可以训练神经图灵机（NTM） (Graves et al., 2014)，这是一种具有增强记忆功能的神经网络，是一种元学习器。然后，该元学习者可以记住有关先前任务的信息，并利用这些信息学习学习器$l_{new}$。 SNAIL (Mishra et al., 2018)是一种通用的元学习器架构，由交错的时间卷积和因果attention层组成。卷积网络为训练样本（图像）学习一个公共特征向量，以汇总过去的经验信息。因果关注层可从收集的经验中学习选择哪些信息，以适应新的任务。</p><p>总体而言，深度学习和元学习的交集被证明是开创性新思想的特别沃土，我们希望随着时间的推移，这一领域将变得越来越重要。</p><h4 id="Beyond-Supervised-Learning"><a href="#Beyond-Supervised-Learning" class="headerlink" title="Beyond Supervised Learning"></a>Beyond Supervised Learning</h4><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><hr><p>元学习机会以许多不同的方式展现出来，并且可以使用多种学习技术加以体现。每当我们尝试学习某个任务时，无论成功与否，我们都会获得有益的经验，可以利用这些经验来学习新任务。我们永远不必完全从头开始。相反，我们应该系统地收集我们的“学习资源”，并从中学习以构建随着时间的推移不断改进的AutoML系统，从而帮助我们更加有效地解决新的学习问题。我们遇到的新任务越多，这些新任务越相似，我们就越可以利用先前的经验，以至于大多数必需的学习已经事先完成。计算机系统能够存储几乎无限量的以前的学习经验（以元数据的形式）的能力为以全新的方式使用该经验提供了广泛的机会，而我们才刚刚开始学习如何从中学习事先有效的经验。然而，这是一个值得实现的目标：学习如何学习任何任务，不仅使我们了解如何学习特定的任务，还使我们拥有了更多的能力。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;元学习或学会学习是系统地观察不同机器学习方法如何在广泛的学习任务中执行的科学，然后从这种经验或元数据中学习，以比其他方式更快地学习新任务 。&lt;br&gt;这不仅极大地加速和改进了机器学习流程或神经网络架构的设计，还使我们能够用数据驱动方式学习的新方法取代手工设计算法。&lt;br&gt;在本章中，我们将概述这个迷人且不断发展的领域的最新技术。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="meta-learning" scheme="https://yachenkang.github.io/blog/tags/meta-learning/"/>
    
      <category term="Survey" scheme="https://yachenkang.github.io/blog/tags/Survey/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Learn via Self-Critique</title>
    <link href="https://yachenkang.github.io/blog/2019/10/09/Learning-to-Learn-via-Self-Critique/"/>
    <id>https://yachenkang.github.io/blog/2019/10/09/Learning-to-Learn-via-Self-Critique/</id>
    <published>2019-10-09T12:33:12.000Z</published>
    <updated>2020-03-19T08:19:56.030Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>在少样本学习中，机器学习系统从一小组与特定任务有关的有标签样本中学习，从而可以推广到同一任务的新示例。鉴于此类任务中有标签样本的数量有限，我们希望充分利用所有可能的信息。通常，模型从小型训练集（support-set）中学习任务特定的信息，以对无标签验证集（target-set也叫query-set）进行预测。target-set包含其他特定于任务的信息，而现有的少样本学习方法并未利用这些信息。通过transductive learning来使用target-set样本需要更先进的方法；at inference time, the target-set contains only unlabelled input data-points, and so discriminative learning cannot be used。在本文中，我们提出了一个名为“Self-Critique and Adapt”或SCA的框架，该框架可以学习无标签损失函数，该函数被参数化为神经网络。基本模型使用现有方法（例如，随机梯度下降与交叉熵损失相结合）在支持集上学习，然后使用学习到的损失函数针对传入的target-task进行更新。学习无标签损失函数，以便target-set-updated模型实现更高的泛化性能。实验表明，与仅适用于支持集的基准相比，SCA可以显着降低错误率，并可以在Mini-ImageNet和Caltech-UCSD Birds 200上获得最先进的基准性能。</p><a id="more"></a><h3 id="Self-Critique-and-Adapt"><a href="#Self-Critique-and-Adapt" class="headerlink" title="Self-Critique and Adapt"></a>Self-Critique and Adapt</h3><p>为了让模型学习和适应仅输入数据点可用的设置（例如，在给定任务的少样本target-set上），就需要一种无标签损失函数。例如，许多无监督的学习方法试图使生成概率最大化，因此使用负对数似然度（或其bound）作为损失函数。通常，大多数生成模型都与任务无关。在一组特定的任务中，针对损失函数可能会有更恰当和专业的选择。</p><p>手动设计这样的损失函数具有挑战性，通常只能产生可能在一种设置下起作用而在另一种情况下不起作用的损失函数。了解损失函数选择的全部影响并不容易。相反，我们提出了一种Self-Critique and Adapt方法，该方法元学习特定任务集的损失函数。它是通过使用set-to-set少样本学习框架并使用端到端基于梯度的可微元学习作为我们的学习框架来解决问题的。</p><p>SCA与模型无关，可以应用在<strong>任何</strong>使用内环优化过程来获取特定于任务的信息的端到端可微且基于梯度的元学习方法之上。许多这样的方法（Ravi and Larochelle, 2016; Finn et al., 2017; Li et al., 2017; Antoniou et al., 2019; Finn et al., 2018; Qiao et al., 2018; Rusu et al., 2018; Grant et al., 2018）目前正在争夺少样本学习领域中的SOTA。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570623088/wps_2019-09-28_22-57-12_eioif3.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>在上图中总结的Self-Critique and Adapt，采用一个基本模型并使用现有的基于梯度的元学习方法(e.g. MAML (Finn et al., 2017), MAML++ (Antoniou et al., 2019) or LEO (Rusu et al., 2018))，根据support-set更新基本模型，然后推断出对target-set的预测。推断出预测后，将它们与其他基本模型相关的信息（例如模型参数，任务嵌入等）串联在一起，然后传递到可学习的critic loss network，其输出应解释为给定输入的loss值。该critic network计算并返回关于target-set的损失。然后针对该critic loss，使用任何随机梯度优化方法(如SGD)更新基本模型；如有必要，可以多次进行更新。这种inner-loop优化可生成特定于support/target set信息的预测模型。</p><p>内部循环过程在推理时直接用于手头的任务。但是，与其他元学习设置一样，我们使用一系列训练任务来优化inner-loop（这些任务与第7节中所述的测试任务不同）。使用训练任务中的ground truth标签评估inner-loop学习的预测模型的质量。然后，outer loop优化初始参数和critic loss，以最大化inner loop预测的质量。与其他元学习方法一样，整个inner loop的可微性确保可以使用基于梯度的方法来学习此outer loop。</p><p>在本文中，我们使用MAML ++作为基本方法。我们用$f(\cdot,\theta)$表示参数化为神经网络的模型，参数为$\theta$，critic loss $C(\cdot,W)$也是参数为$W$的神经网络。我们想学习好的参数$\theta$和$W$，当优化模型$f$时，针对support set $S_b=\{x_S, y_S\}$上的loss $L$，需要执行$N$步优化，然后再针对critic loss $C$ 另外向target-set $T_b = \{x_T\}$优化$I$步，可以在target-set上实现良好的泛化性能。这里，$b$是一个具体任务在一批任务中的索引。完整算法在下面进行了描述。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570624552/wps_2019-10-09_20-35-27_twt5kl.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>算法框图中的等式2定义了潜在的条件特征集合$F$，这些特征概括了基本模型及其行为。这些特征可以被无监督的critic loss $C$ 用来调整target set 更新。在这些可能的特征中，$f (\theta_N,x_T)$是基本模型$f$的预测，使用参数$\theta_N$（即针对support-set loss的N步更新后的参数），而$g(x_S,x_n)$是任务嵌入，参数化为神经网络，该神经网络以support和target输入数据点为条件。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在少样本学习中，机器学习系统从一小组与特定任务有关的有标签样本中学习，从而可以推广到同一任务的新示例。鉴于此类任务中有标签样本的数量有限，我们希望充分利用所有可能的信息。通常，模型从小型训练集（support-set）中学习任务特定的信息，以对无标签验证集（target-set也叫query-set）进行预测。target-set包含其他特定于任务的信息，而现有的少样本学习方法并未利用这些信息。通过transductive learning来使用target-set样本需要更先进的方法；at inference time, the target-set contains only unlabelled input data-points, and so discriminative learning cannot be used。在本文中，我们提出了一个名为“Self-Critique and Adapt”或SCA的框架，该框架可以学习无标签损失函数，该函数被参数化为神经网络。基本模型使用现有方法（例如，随机梯度下降与交叉熵损失相结合）在支持集上学习，然后使用学习到的损失函数针对传入的target-task进行更新。学习无标签损失函数，以便target-set-updated模型实现更高的泛化性能。实验表明，与仅适用于支持集的基准相比，SCA可以显着降低错误率，并可以在Mini-ImageNet和Caltech-UCSD Birds 200上获得最先进的基准性能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper笔记" scheme="https://yachenkang.github.io/blog/categories/paper%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="meta-learning" scheme="https://yachenkang.github.io/blog/tags/meta-learning/"/>
    
  </entry>
  
</feed>
