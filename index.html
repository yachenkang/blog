<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- <link rel="stylesheet" type="text/css" href="/css/matery.css"> -->
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Hamish的科研blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content>
    <meta property="og:type" content="website">
<meta property="og:title" content="Hamish的科研blog">
<meta property="og:url" content="https://yachenkang.github.io/blog/index.html">
<meta property="og:site_name" content="Hamish的科研blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hamish的科研blog">
    
        <link rel="alternate" type="application/atom+xml" title="Hamish的科研blog" href="/blog/atom.xml">
    
    <link rel="shortcut icon" href="/blog/img/avatar.jpg">
    <link rel="stylesheet" href="/blog/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    
</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu"  >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/blog/img/brand.jpg)">
      <div class="brand">
        <a href="/blog/" class="avatar waves-effect waves-circle waves-light">
          <img src="/blog/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Kang Yachen</h5>
          <a href="mailto:kangyachen@westlake.edu.cn" title="kangyachen@westlake.edu.cn" class="mail">kangyachen@westlake.edu.cn</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect active">
              <a href="/blog/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/blog/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/blog/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/blog/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/sherlockbear" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Hamish的科研blog</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header index-header">

    <div class="container fade-scale">
        <h1 class="title">Hamish的科研blog</h1>
        <h5 class="subtitle">
            
                
            
        </h5>
    </div>

    


</header>

<div class="container body-wrap">

    <ul class="post-list">
    
        <li class="post-list-item fade">
            <article id="post-NIPS2020-Reinforcement-Learning-Reading-List"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-06-15 10:20:40" datetime="2020-06-15T02:20:40.000Z"  itemprop="datePublished">2020-06-15</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/Reading-List/">Reading List</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/06/15/NIPS2020-Reinforcement-Learning-Reading-List/">NIPS2020 Reinforcement Learning Reading List</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <p>NIPS2020论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。</p>
    

        <a href="/blog/2020/06/15/NIPS2020-Reinforcement-Learning-Reading-List/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-NIPS2019-Reinforcement-Learning-Reading-List"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-06-15 10:20:40" datetime="2020-06-15T02:20:40.000Z"  itemprop="datePublished">2020-06-15</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/Reading-List/">Reading List</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/06/15/NIPS2019-Reinforcement-Learning-Reading-List/">NIPS2019 Reinforcement Learning Reading List</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <p>NIPS2019论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。</p>
    

        <a href="/blog/2020/06/15/NIPS2019-Reinforcement-Learning-Reading-List/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-SQIL-IMITATION-LEARNING-VIA-REINFORCEMENT-LEARNING-WITH-SPARSE-REWARDS"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-06-01 05:16:39" datetime="2020-05-31T21:16:39.000Z"  itemprop="datePublished">2020-06-01</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/06/01/SQIL-IMITATION-LEARNING-VIA-REINFORCEMENT-LEARNING-WITH-SPARSE-REWARDS/">SQIL: IMITATION LEARNING VIA REINFORCEMENT LEARNING WITH SPARSE REWARDS</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>学习模仿演示中的专家行为可能是富有挑战性的，特别是在高维，观察连续以及动态未知的环境中。基于behavioral cloning（BC）的有监督学习方法存在分布偏移的问题：因为智能体贪婪地模仿演示的动作，它可能会由于误差累积而偏离演示的状态。近来基于强化学习（RL）的方法，例如逆强化学习（inverse RL）和生成对抗式模仿学习（GAIL），通过训练RL智能体去匹配长时程的演示来克服这个问题。由于该任务的真正奖励函数是未知的，因此这些方法通常通过使用复杂且脆弱的近似技术来参与对抗训练，从演示中学习奖励函数。我们提出了一个简单的替代方法，该替代方法仍然使用RL，但不需要学习奖励函数。关键思想是通过鼓励智能体在遇到新的、分布之外的状态时返回到演示状态，从而激励他们在很长的时间内匹配演示。为此，我们为智能体提供了在演示状态下匹配演示操作的$r=+1$的恒定奖励，以及对所有其他行为的$r=0$的恒定奖励。我们的方法，我们称为soft Q imitation learning（SQIL），可以通过对任何标准Q-learning或off-policy actor-critic算法进行少量的修改来实现。从理论上讲，我们表明SQIL可以解释为BC利用稀疏先验来鼓励长时程模仿的正则化变体。实验上，我们在Box2D，Atari和MuJoCo中的各种基于图像的以及低维的任务上，SQIL的性能优于BC，与GAIL相比也取得了相近的结果。本文证明了基于RL且具有固定奖励的简单模仿方法与使用学到奖励的更复杂方法一样有效。</p>
    

        <a href="/blog/2020/06/01/SQIL-IMITATION-LEARNING-VIA-REINFORCEMENT-LEARNING-WITH-SPARSE-REWARDS/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Reinforcement-Learning-and-Control-as-Probabilistic-Inference-Tutorial-and-Review"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-04-10 20:33:52" datetime="2020-04-10T12:33:52.000Z"  itemprop="datePublished">2020-04-10</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/04/10/Reinforcement-Learning-and-Control-as-Probabilistic-Inference-Tutorial-and-Review/">Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>强化学习或最优控制的框架提供了强大且广泛适用的智能决策的数学形式。尽管强化学习问题的一般形式可以对不确定性进行有效的推理，但是强化学习与概率模型中的推理之间的联系并不是很明显。但是，这种联系在算法设计方面具有相当大的价值：将问题形式化为概率推理，从原理上使我们能够使用各种各样的近似推理工具，以灵活强大的方式扩展模型，并了解组成性和部分可观察性的原因。在本文中，我们将讨论强化学习或最优控制问题的泛化（有时称为最大熵强化学习）如何等同于确定性动态情况下的精确概率推理和随机动态情况下的变分推理。我们将介绍此框架的详细信息，概述以此为基础的先前工作及相关思想，以提出新的强化学习和控制算法，并描述对未来研究的看法。<br>
    

        <a href="/blog/2020/04/10/Reinforcement-Learning-and-Control-as-Probabilistic-Inference-Tutorial-and-Review/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Option-Discovery-using-Deep-Skill-Chaining"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-20 14:17:04" datetime="2020-03-20T06:17:04.000Z"  itemprop="datePublished">2020-03-20</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/20/Option-Discovery-using-Deep-Skill-Chaining/">Option Discovery using Deep Skill Chaining</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>自动发现时间上可扩展的动作或技能是分层强化学习的长期目标。我们提出了一种新的算法，该算法将技能链与深度神经网络相结合，可以在高维、连续域中自动发现技能。最终的算法，即<em>deep skill chaining</em>，通过这样的属性来构建技能，即当执行一个时使能智能体去执行另一个。（constructs skills with the property that executing one enables the agent to execute another.） 我们证明，在挑战性的连续控制任务中，<em>deep skill chaining</em>显着优于非分层智能体和其他最新的技能发现技术。</p>
    

        <a href="/blog/2020/03/20/Option-Discovery-using-Deep-Skill-Chaining/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Skill-Learning/">Skill Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Dynamical-Distance-Learning-for-Semi-Supervised-and-Unsupervised-Skill-Discovery"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-20 13:56:49" datetime="2020-03-20T05:56:49.000Z"  itemprop="datePublished">2020-03-20</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/20/Dynamical-Distance-Learning-for-Semi-Supervised-and-Unsupervised-Skill-Discovery/">Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>强化学习需要手动指定奖励函数才能学习任务。虽然原则上该奖励函数仅需要指定任务目标，但在实践中，强化学习可能非常耗时甚至不可行，除非对奖励函数进行了调整，以便产生平滑的梯度导向成功的结果。但手动调整是很难的，尤其是从原始观察结果（例如图像）获取任务时。<strong>在本文中，我们研究了如何自动学习动态距离：一种从任何其他状态到达给定目标状态的预期时间步个数的度量</strong>。这些动态距离可用于提供well-shaped奖励函数，以实现新的目标，从而有可能有效地学习复杂的任务。我们表明动态距离可以被用于半监督，其中无监督与环境的交互用于学习动态距离，而少量的偏好监督用于确定任务目标，而无需任何人工设计的奖励函数或目标示例。我们在真实机器人和仿真中都评估了我们的方法。我们展示了我们的方法可以使用原始的9自由度机械手学习阀门的转动，使用原始图像观察结果和十个偏好标签，而无需任何其他监督。</p>
    

        <a href="/blog/2020/03/20/Dynamical-Distance-Learning-for-Semi-Supervised-and-Unsupervised-Skill-Discovery/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Skill-Learning/">Skill Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-COMPOSING-TASK-AGNOSTIC-POLICIES-WITH-DEEP-REINFORCEMENT-LEARNING"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-20 13:44:24" datetime="2020-03-20T05:44:24.000Z"  itemprop="datePublished">2020-03-20</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/20/COMPOSING-TASK-AGNOSTIC-POLICIES-WITH-DEEP-REINFORCEMENT-LEARNING/">COMPOSING TASK-AGNOSTIC POLICIES WITH DEEP REINFORCEMENT LEARNING</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>用基本行为的构成去解决迁移学习难题的是构建人工智能的关键要素之一。迄今为止，在学习task-specific的策略或技能方面已经有了大量工作，但几乎没有关注构建与任务无关的必要技能以找到新问题的解决方案。<strong>在本文中，我们提出了一种新的，基于深度强化学习的技能迁移和组合方法，该方法采用智能体的primitive策略来解决未曾见过的任务</strong>。我们在困难的环境中评估我们的方法，在这些环境中，通过标准强化学习（RL）甚至是分层RL的训练策略要么不可行，要么表现出较高的样本复杂性。我们证明了我们的方法不仅可以将技能迁移到新的问题设置中，而且还可以解决既需要任务计划又需要运动控制的挑战性环境，且数据效率很高。</p>
    

        <a href="/blog/2020/03/20/COMPOSING-TASK-AGNOSTIC-POLICIES-WITH-DEEP-REINFORCEMENT-LEARNING/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Skill-Learning/">Skill Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Discovering-Motor-Programs-By-Recomposing-Demonstrations"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-20 13:23:41" datetime="2020-03-20T05:23:41.000Z"  itemprop="datePublished">2020-03-20</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/20/Discovering-Motor-Programs-By-Recomposing-Demonstrations/">Discovering Motor Programs By Recomposing Demonstrations</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>在本文中，我们提出了一种从大规模且多样化的操作演示中来学习可重构motor primitives的方法。当前将演示分解为primitives的方法通常采用手动定义的primitives，而绕开了发现这些primitives的难度。另一方面，用于发现primitives的方法对primitive的复杂性进行了限制性假设，这使得任务的适用性限制在了狭窄的范围。<strong>我们的方法试图通过同时学习基础的motor primitives并重组这些primitives以重构原始演示来应对这些挑战</strong>。通过限制primitives分解的简约性和给定primitive的简单性，我们能够学习各种不同的motor primitives，以及它们的连贯潜在表示。我们从定性和定量两个方面证明了我们所学的primitives捕获了演示中语义上有意义的方面。这使我们能够在分层强化学习设置中组合这些primitives，以有效解决机器人操作任务，例如伸手和推手。</p>
    

        <a href="/blog/2020/03/20/Discovering-Motor-Programs-By-Recomposing-Demonstrations/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Skill-Learning/">Skill Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-LEARNING-TO-COORDINATE-MANIPULATION-SKILLS-VIA-SKILL-BEHAVIOR-DIVERSIFICATION"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-20 10:18:55" datetime="2020-03-20T02:18:55.000Z"  itemprop="datePublished">2020-03-20</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/20/LEARNING-TO-COORDINATE-MANIPULATION-SKILLS-VIA-SKILL-BEHAVIOR-DIVERSIFICATION/">LEARNING TO COORDINATE MANIPULATION SKILLS VIA SKILL BEHAVIOR DIVERSIFICATION</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>当完成一个复杂的操纵任务时，人们经常将任务分解为身体各个部分的子技能，独立地练习这些子技能，然后一起执行这些子技能。同样，具有多个末端执行器的机器人可以通过协调每个末端执行器的子技能来执行复杂的任务。<strong>为了实现技能的时间和行为协调，我们提出了一个模块化框架，该框架首先通过skill behavior diversification分别训练每个末端执行器的子技能，然后学习使用技能的多种行为来协调末端执行器</strong>。我们证明了我们提出的框架能够有效地协调技能，以解决具有挑战性的协作控制任务，例如捡起一根长棒，在用两个机械手推动容器的同时在容器内放置一个块以及用两个蚂蚁推动容器。 视频和代码可在上获得。</p>
    

        <a href="/blog/2020/03/20/LEARNING-TO-COORDINATE-MANIPULATION-SKILLS-VIA-SKILL-BEHAVIOR-DIVERSIFICATION/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Skill-Learning/">Skill Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Contrastive-Learning-of-Structured-World-Models"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-19 15:26:06" datetime="2020-03-19T07:26:06.000Z"  itemprop="datePublished">2020-03-19</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/19/Contrastive-Learning-of-Structured-World-Models/">Contrastive Learning of Structured World Models</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一个挑战。为了朝这个目标迈进，我们引入了Contrastively-trained Structured World Models（C-SWMs）。C-SWMs利用对比方法在具有合成结构的环境中进行表示学习。<strong>我们通过图神经网络建模将每个state嵌入构造为一组对象表示及其关系。这允许模型从原始像素观察中发现对象，而无需把直接监督作为学习过程的一部分</strong>。我们在包含多个交互对象的合成环境中评估C-SWMs，这些交互对象均可以被智能体相互独立操作，包括简单的Atari游戏和多对象物理模拟器。我们的实验表明，C-SWMs可以在学习到可解释的基于对象的表示形式的基础上，克服基于像素重构的模型的局限性，并在高度结构化的环境中胜过此类模型的典型代表。</p>
    

        <a href="/blog/2020/03/19/Contrastive-Learning-of-Structured-World-Models/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Fast-Task-Inference-with-Variational-Intrinsic-Successor-Features"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-19 11:21:00" datetime="2020-03-19T03:21:00.000Z"  itemprop="datePublished">2020-03-19</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/19/Fast-Task-Inference-with-Variational-Intrinsic-Successor-Features/">Fast Task Inference with Variational Intrinsic Successor Features</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>已经确定，张成马尔可夫决策过程可控子空间的多样性行为可以通过奖励与其他policy有区别的policy来训练(Gregor et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018)。但是，这种方法的一个局限性是难以推广到超出可明确学习的有限行为集的范围，而这在后续任务中可能是必需的。Successor features(Dayan, 1993; Barreto et al., 2017)提供了一个吸引人的解决方案，适用于此泛化问题，但需要在某些基础特征空间中将奖励函数定义为线性。在本文中，我们展示了可以将这两种技术结合使用，并且相互可以解决彼此的主要局限。为此，<strong>我们引入了Variational Intrinsic Successor FeatuRes（VISR），这是一种新的算法，能够学习可控特征，可通过Successor features框架利用可控特征来提供增强的泛化能力和快速的任务推断能力</strong>。我们在全部Atari套件上对VISR进行了实验验证，我们使用了新的设置，其中奖励仅是在漫长的无监督阶段之后才短暂显示。在12场比赛中达到人类水平的表现并超过所有baselines，使我们认为VISR代表了朝着能够从有限的反馈中快速学习的智能体迈出的一步。</p>
    

        <a href="/blog/2020/03/19/Fast-Task-Inference-with-Variational-Intrinsic-Successor-Features/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-ICLR2020-Reinforcement-Learning-Reading-List"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-03-06 09:44:37" datetime="2020-03-06T01:44:37.000Z"  itemprop="datePublished">2020-03-06</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/Reading-List/">Reading List</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/03/06/ICLR2020-Reinforcement-Learning-Reading-List/">ICLR2020 Reinforcement Learning Reading List</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <p>ICLR2020论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。</p>
    

        <a href="/blog/2020/03/06/ICLR2020-Reinforcement-Learning-Reading-List/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2020-02-17 11:25:52" datetime="2020-02-17T03:25:52.000Z"  itemprop="datePublished">2020-02-17</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/">Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>在各种复杂环境中运行的强化学习智能体可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互。对原语进行正则化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。</p>
    

        <a href="/blog/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Skill-Learning/">Skill Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-NAS-Reading-List"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2019-12-05 15:31:04" datetime="2019-12-05T07:31:04.000Z"  itemprop="datePublished">2019-12-05</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/Reading-List/">Reading List</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2019/12/05/NAS-Reading-List/">NAS Reading List</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h2 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h2><ul>
<li>[x] （ICLR2017, google brain）Neural architecture search with reinforcement learning</li>
<li>[ ] （2019.11）Meta-Learning of Neural Architectures for Few-Shot Learning，meta与NAS的结合：<a href="https://arxiv.org/abs/1911.11090v1" target="_blank" rel="noopener">https://arxiv.org/abs/1911.11090v1</a></li>
<li><p>[x] （2019.01）Designing neural networks through neuroevolution，NE方法综述</p>
</li>
<li><p>（2017.09）Evolution Strategies as a Scalable Alternative to Reinforcement Learning <a href="https://openai.com/blog/evolution-strategies/" target="_blank" rel="noopener">https://openai.com/blog/evolution-strategies/</a>， <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="noopener">https://arxiv.org/abs/1703.03864</a>：NES方法与DQN、A3C相匹敌（但未完全脱离梯度）</p>
</li>
<li><p>（2018.04）Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning <a href="https://arxiv.org/abs/1712.06567" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06567</a>：gradient-free的NE方法与DQN、A3C相匹敌</p>
</li>
<li><p>（2018.04）Simple random search provides a competitive approach to reinforcement learning <a href="https://arxiv.org/abs/1803.07055" target="_blank" rel="noopener">https://arxiv.org/abs/1803.07055</a>：简化NE方法（RS方法）与RPO、PPO、DDPG相匹敌</p>
</li>
</ul>
<h3 id="结合基于梯度的方法和神经进化"><a href="#结合基于梯度的方法和神经进化" class="headerlink" title="结合基于梯度的方法和神经进化"></a>结合基于梯度的方法和神经进化</h3><ul>
<li><p>（2018.05）Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients <a href="https://arxiv.org/abs/1712.06563" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06563</a>：保存状态与动作之间的关系库</p>
</li>
<li><p>（ICLR 2018.05）Policy Optimization by Genetic Distillation <a href="https://arxiv.org/abs/1711.01012" target="_blank" rel="noopener">https://arxiv.org/abs/1711.01012</a>：Genetic policy optimization</p>
</li>
<li><p>（ICLR 2018）Noisy Networks for Exploration <a href="https://arxiv.org/abs/1706.10295" target="_blank" rel="noopener">https://arxiv.org/abs/1706.10295</a></p>
</li>
<li><p>（ICLR 2018）Parameter space noise for exploration <a href="https://arxiv.org/abs/1706.01905" target="_blank" rel="noopener">https://arxiv.org/abs/1706.01905</a></p>
</li>
</ul>
<h3 id="新一代进化算法"><a href="#新一代进化算法" class="headerlink" title="新一代进化算法"></a>新一代进化算法</h3><ul>
<li><p>The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities <a href="https://arxiv.org/abs/1803.03453" target="_blank" rel="noopener">https://arxiv.org/abs/1803.03453</a> ：综述</p>
</li>
<li><p>（NIPS 2018）Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents <a href="https://arxiv.org/abs/1712.06560" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06560</a></p>
</li>
<li><p>（NIPS workshop 2018）Deep Curiosity Search: Intra-Life Exploration Can Improve Performance on Challenging Deep Reinforcement Learning Problems <a href="https://arxiv.org/abs/1806.00553" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00553</a></p>
</li>
</ul>
<h3 id="架构进化"><a href="#架构进化" class="headerlink" title="架构进化"></a>架构进化</h3><ul>
<li><p>From Nodes to Networks: Evolving Recurrent Neural Networks <a href="https://arxiv.org/abs/1803.04439" target="_blank" rel="noopener">https://arxiv.org/abs/1803.04439</a></p>
</li>
<li><p>（AAAI 2019）Regularized Evolution for Image Classifier Architecture Search <a href="https://arxiv.org/abs/1802.01548" target="_blank" rel="noopener">https://arxiv.org/abs/1802.01548</a></p>
</li>
</ul>

    

        <a href="/blog/2019/12/05/NAS-Reading-List/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/NAS/">NAS</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Neural-Architecture-Search-with-Reinforcement-Learning"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2019-12-03 10:26:34" datetime="2019-12-03T02:26:34.000Z"  itemprop="datePublished">2019-12-03</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2019/12/03/Neural-Architecture-Search-with-Reinforcement-Learning/">Neural Architecture Search with Reinforcement Learning</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>神经网络是一种功能强大、灵活的模型，在图像、语音和自然语言理解等许多困难的学习任务中起着很好的作用。尽管取得了成功，神经网络仍然很难设计。在本文中，我们使用递归网络来生成神经网络的模型描述，并利用强化学习来训练该RNN，最大化所生成的架构在验证集上的期望精度。在CIFAR-10数据集上，我们的方法可以从头开始，设计一种新的网络体系结构，在测试集精度方面可以与人类发明的最佳体系结构相媲美。我们的CIFAR-10模型的测试错误率为3.65，比以前使用类似架构方案的最新模型高0.09%，快1.05倍。在Penn Treebank数据集上，我们的模型可以组成一个新的递归单元，其性能优于广泛使用的LSTM单元和其他SOTA baseline。我们的单元在Penn Treebank上达到了测试集62.4的困惑度，这比之前的SOTA模型在困惑度上好3.6。该单元还可以转移到PTB上的字符语言建模任务，并达到SOTA的1.214困惑度。</p>
    

        <a href="/blog/2019/12/03/Neural-Architecture-Search-with-Reinforcement-Learning/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/NAS/">NAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-Meta-World-A-Benchmark-and-Evaluation-for-Multi-Task-and-Meta-Reinforcement-Learning"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2019-11-01 15:24:27" datetime="2019-11-01T07:24:27.000Z"  itemprop="datePublished">2019-11-01</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2019/11/01/Meta-World-A-Benchmark-and-Evaluation-for-Multi-Task-and-Meta-Reinforcement-Learning/">Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>元强化学习算法可以通过利用先前的经验来学习如何学习，从而使机器人更快地掌握新技能。但是，当前有关元强化学习的许多研究都集中在非常狭窄的任务分布上。例如，一个常用的元强化学习基准将模拟机器人的不同的运行速度作为不同的任务。当在这样狭窄的任务分布上进行策略的元训练时，它们可能无法泛化到更快地获取全新的任务。因此，如果这些方法的目的是能够更快地获取全新的行为，则我们必须在足够广泛的任务分布上评估它们，以使其能够推广到新的行为。在本文中，我们提出了一种用于元强化学习和多任务学习的开源模拟benchmark，该benchmark包含50个不同的机器人操纵任务。我们的目标是使开发用于加速获取全新的、可执行的任务的算法成为可能。我们针对这些任务评估了6种最新的元强化学习和多任务学习算法。令人惊讶的是，尽管每项任务及其变体（例如，不同的对象位置）都可以合理地成功学习，但是这些算法难以同时学习多个任务，即使只有十个不同的训练任务也是如此。我们的分析和开源环境为将来的多任务学习和元学习研究铺平了道路，这些研究可以实现有意义的泛化，从而释放这些方法的全部潜力。</p>
<p>benchmark任务的视频在项目页面上：<a href="https://meta-world.github.io" target="_blank" rel="noopener">meta-world.github.io</a>。我们的开源代码可在以下网址获得：<a href="https://github.com/rlworkgroup/metaworld" target="_blank" rel="noopener">https://github.com/rlworkgroup/metaworld</a></p>
    

        <a href="/blog/2019/11/01/Meta-World-A-Benchmark-and-Evaluation-for-Multi-Task-and-Meta-Reinforcement-Learning/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/benchmark/">benchmark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/meta-learning/">meta-learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-iMAML笔记（翻译）"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2019-10-18 15:45:27" datetime="2019-10-18T07:45:27.000Z"  itemprop="datePublished">2019-10-18</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/blog翻译/">blog翻译</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2019/10/18/iMAML笔记（翻译）/">iMAML笔记（翻译）[更新中]</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <p>转载并翻译iMAML的阅读笔记</p>
    

        <a href="/blog/2019/10/18/iMAML笔记（翻译）/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/MAML/">MAML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/meta-learning/">meta-learning</a></li></ul>

    </div>
    
</article>

        </li>
    
        <li class="post-list-item fade">
            <article id="post-On-First-Order-Meta-Learning-Algorithms"
  class="article-card article-type-post" itemprop="blogPost">

    <div class="post-meta">
        <time class="post-time" title="2019-10-17 10:58:41" datetime="2019-10-17T02:58:41.000Z"  itemprop="datePublished">2019-10-17</time>

        
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/paper笔记/">paper笔记</a></li></ul>



    </div>

    


  
    <h3 class="post-title" itemprop="name">
      <a class="post-title-link" href="/blog/2019/10/17/On-First-Order-Meta-Learning-Algorithms/">On First-Order Meta-Learning Algorithms</a>
    </h3>
  




    <div class="post-content" id="post-content" itemprop="postContent">

    
        <h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>本文考虑了存在任务分布的元学习问题，并且我们希望获得一个从该分布中采样到以前没有见过的任务时表现良好（即快速学习）的agent。我们分析了一族用于学习参数初始化的算法，可以在新任务上进行快速微调，仅使用一阶导数进行元学习更新。该族包括并推广了一阶MAML，它是通过忽略二阶导数获得的MAML的近似值。它还包括Reptile，这是我们在此处引入的新算法，该算法通过重复采样任务，对其进行训练并将初始化朝着该任务的训练权重进行工作。我们扩展了Finn等人的结果。说明一阶元学习算法在一些公认的针对少数镜头分类的基准上表现良好，并且我们提供了旨在理解这些算法为何起作用的理论分析。</p>

    

        <a href="/blog/2019/10/17/On-First-Order-Meta-Learning-Algorithms/" class="post-more waves-effect waves-button">
            阅读全文…
        </a>
    </div>
    
    <div class="post-footer">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/MAML/">MAML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/meta-learning/">meta-learning</a></li></ul>

    </div>
    
</article>

        </li>
    
    </ul>

    
<nav id="page-nav">
    <div class="inner">
    <span class="page-number current">1</span><a class="page-number" href="/blog/page/2/">2</a><a class="extend next" rel="next" href="/blog/page/2/">下一页</a>
    </div>
</nav>


</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/blog/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Kang Yachen &copy; 2015 - 2021</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://yachenkang.github.io/blog/&title=Hamish的科研blog&pic=https://yachenkang.github.io/blog/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://yachenkang.github.io/blog/&title=Hamish的科研blog&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://yachenkang.github.io/blog/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=Hamish的科研blog&url=https://yachenkang.github.io/blog/&via=https://yachenkang.github.io/blog" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://yachenkang.github.io/blog/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAAAAACKZ2kyAAABxElEQVR42u3aS27DMAwFwNz/0u4BUtuPpKUaxWgVJIg88oLgR59PvI6v9f392b+u9/ysWLi4uGPucbkm3G9Wvv/pzri4uBu5yWOuH9/7NTfg4uK+n3tNmdBxcXH/H/cMnRwGFxf3bdyk+Lne6IhX8goeqNVwcXEH3LxLue7zkv4uLi5ui3sUV68QOh5auLi4e7h5QEkKm94hy8/FxcVdzF3RBk0O2bvqgYuLu4dbbVb2gtFTVzFwcXH3cJPWZDVNSUJh8hZ/+YyLi7uFG8W51thjXizd1Gq4uLhbuJNEJD/2kuIHFxd3Gbc3/syJ1XAZxV1cXNxl3MnotHpFo9ouOd0BFxf3T7nJFCN5TLV8agYyXFzch7jVKxTVwDT59ab4wcXF3cLtjUur6csk3cHFxd3JnQ9OkkZqb3yLi4v7t9zqcDQnPpUe4eLiruYexZUMPKrXvAoJFi4u7hbu/EpENfWpFkXNw+Di4o65k+CVpCZ52hS9UVxc3I3cpxqd1atXieemi4OLi/sCbnJJolf8FCo2XFzcV3KradCk8fFAIMPFxW1x82FJ3jCdJD03Q1ZcXNzF3HIoaRUt1SsdD/R3cXFxO9wfCf4p+CD0TrcAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/blog/', SHARE: true, REWARD: false };


</script>

<script src="/blog/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/blog/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





</body>
</html>
